
@online{14:00-17:00ISOIEC14882,
  title = {{{ISO}}/{{IEC}} 14882:2020},
  shorttitle = {{{ISO}}/{{IEC}} 14882},
  author = {{14:00-17:00}},
  url = {https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/07/93/79358.html},
  urldate = {2021-10-28},
  abstract = {Programming languages — C++},
  langid = {english},
  organization = {{ISO}},
  file = {/Users/cperivol/Zotero/storage/UW93FCQV/79358.html}
}

@online{AdaptonComposableDemanddriven,
  title = {Adapton: Composable, Demand-Driven Incremental Computation: {{ACM SIGPLAN Notices}}: Vol 49, {{No}} 6},
  url = {https://dl.acm.org/doi/abs/10.1145/2666356.2594324},
  urldate = {2021-10-09}
}

@inproceedings{agrawalAutomatedSelectionMaterialized2000,
  title = {Automated {{Selection}} of {{Materialized Views}} and {{Indexes}} for {{SQL Databases}}.},
  author = {Agrawal, Sanjay and Chaudhuri, Surajit and Narasayya, Vivek},
  date = {2000-09-01},
  url = {https://www.microsoft.com/en-us/research/publication/automated-selection-of-materialized-views-and-indexes-for-sql-databases/},
  urldate = {2021-11-26},
  abstract = {Automatically selecting an appropriate set of materialized views and indexes for SQL databases is a non-trivial task. A judicious choice must be cost-driven and influenced by the workload experienced by the system. Although there has been work in materialized view selection in the context of multidimensional (OLAP) databases, no past work has looked at the […]},
  eventtitle = {{{VLDB}}},
  langid = {american},
  file = {/Users/cperivol/Zotero/storage/6TMT425F/automated-selection-of-materialized-views-and-indexes-for-sql-databases.html}
}

@article{ahmedAutomatedGenerationMaterialized2020,
  title = {Automated Generation of Materialized Views in Oracle},
  author = {Ahmed, Rafi and Bello, Randall and Witkowski, Andrew and Kumar, Praveen},
  date = {2020},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {13},
  number = {12},
  pages = {3046--3058},
  publisher = {{VLDB Endowment}}
}

@inproceedings{alagiannisH2OHandsfreeAdaptive2014,
  title = {{{H2O}}: A Hands-Free Adaptive Store},
  shorttitle = {{{H2O}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Alagiannis, Ioannis and Idreos, Stratos and Ailamaki, Anastasia},
  date = {2014-06-18},
  series = {{{SIGMOD}} '14},
  pages = {1103--1114},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2588555.2610502},
  url = {https://doi.org/10.1145/2588555.2610502},
  urldate = {2021-05-27},
  abstract = {Modern state-of-the-art database systems are designed around a single data storage layout. This is a fixed decision that drives the whole architectural design of a database system, i.e., row-stores, column-stores. However, none of those choices is a universally good solution; different workloads require different storage layouts and data access methods in order to achieve good performance. In this paper, we present the H2O system which introduces two novel concepts. First, it is flexible to support multiple storage layouts and data access patterns in a single engine. Second, and most importantly, it decides on-the-fly, i.e., during query processing, which design is best for classes of queries and the respective data parts. At any given point in time, parts of the data might be materialized in various patterns purely depending on the query workload; as the workload changes and with every single query, the storage and access patterns continuously adapt. In this way, H2O makes no a priori and fixed decisions on how data should be stored, allowing each single query to enjoy a storage and access pattern which is tailored to its specific properties. We present a detailed analysis of H2O using both synthetic benchmarks and realistic scientific workloads. We demonstrate that while existing systems cannot achieve maximum performance across all workloads, H2O can always match the best case performance without requiring any tuning or workload knowledge.},
  isbn = {978-1-4503-2376-5},
  keywords = {adaptive hybrids,adaptive storage,dynamic operators},
  file = {/Users/cperivol/Zotero/storage/SMF4827Q/Alagiannis et al. - 2014 - H2O a hands-free adaptive store.pdf}
}

@inproceedings{alagiannisH2OHandsfreeAdaptive2014a,
  title = {{{H2O}}: A Hands-Free Adaptive Store},
  shorttitle = {{{H2O}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Alagiannis, Ioannis and Idreos, Stratos and Ailamaki, Anastasia},
  date = {2014},
  pages = {1103--1114},
  file = {/Users/cperivol/Zotero/storage/GGPM3XJ2/Alagiannis et al. - 2014 - H2O a hands-free adaptive store.pdf;/Users/cperivol/Zotero/storage/97KICDUW/2588555.html}
}

@inproceedings{armbrustSparkSQLRelational2015,
  title = {Spark {{SQL}}: Relational {{Data Processing}} in {{Spark}}},
  shorttitle = {Spark {{SQL}}},
  booktitle = {Proceedings of the 2015 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Armbrust, Michael and Xin, Reynold S. and Lian, Cheng and Huai, Yin and Liu, Davies and Bradley, Joseph K. and Meng, Xiangrui and Kaftan, Tomer and Franklin, Michael J. and Ghodsi, Ali and Zaharia, Matei},
  date = {2015-05-27},
  series = {{{SIGMOD}} '15},
  pages = {1383--1394},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2723372.2742797},
  url = {https://doi.org/10.1145/2723372.2742797},
  urldate = {2021-11-28},
  abstract = {Spark SQL is a new module in Apache Spark that integrates relational processing with Spark's functional programming API. Built on our experience with Shark, Spark SQL lets Spark programmers leverage the benefits of relational processing (e.g. declarative queries and optimized storage), and lets SQL users call complex analytics libraries in Spark (e.g. machine learning). Compared to previous systems, Spark SQL makes two main additions. First, it offers much tighter integration between relational and procedural processing, through a declarative DataFrame API that integrates with procedural Spark code. Second, it includes a highly extensible optimizer, Catalyst, built using features of the Scala programming language, that makes it easy to add composable rules, control code generation, and define extension points. Using Catalyst, we have built a variety of features (e.g. schema inference for JSON, machine learning types, and query federation to external databases) tailored for the complex needs of modern data analysis. We see Spark SQL as an evolution of both SQL-on-Spark and of Spark itself, offering richer APIs and optimizations while keeping the benefits of the Spark programming model.},
  isbn = {978-1-4503-2758-9},
  keywords = {data warehouse,databases,hadoop,machine learning,spark},
  annotation = {01399},
  file = {/Users/cperivol/Zotero/storage/I3LXQP7G/Armbrust et al. - 2015 - Spark SQL Relational Data Processing in Spark.pdf}
}

@inproceedings{asadaArrowsAreStrong2010,
  title = {Arrows Are Strong Monads},
  booktitle = {Proceedings of the Third {{ACM SIGPLAN}} Workshop on {{Mathematically}} Structured Functional Programming - {{MSFP}} '10},
  author = {Asada, Kazuyuki},
  date = {2010},
  pages = {33},
  publisher = {{ACM Press}},
  location = {{Baltimore, Maryland, USA}},
  doi = {10.1145/1863597.1863607},
  url = {http://portal.acm.org/citation.cfm?doid=1863597.1863607},
  urldate = {2021-08-02},
  abstract = {Hughes’ arrows were shown, by Jacobs et al., to be roughly monads in the bicategory Prof of profunctors (distributors, modules). However in their work as well as others’, the categorical nature of the first operator was not pursued and its formulation remained rather ad hoc. In this paper, we identify first with strength for a monad, therefore: arrows are strong monads in Prof . Strong monads have been widely used in the semantics of functional programming after Moggi’s seminal work, therefore our observation establishes categorical canonicity of the notion of arrow.},
  eventtitle = {The Third {{ACM SIGPLAN}} Workshop},
  isbn = {978-1-4503-0255-5},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/XSBJIMVS/Asada - 2010 - Arrows are strong monads.pdf}
}

@online{AssociatedTypeSynonyms,
  title = {Associated Type Synonyms | {{ACM SIGPLAN Notices}}},
  url = {https://dl.acm.org/doi/abs/10.1145/1090189.1086397?casa_token=HLcvyM59laEAAAAA:3yH9-bJAMmzqia0ewv_MReCDVnbfydL5nsIokMy4g8KWDC12huZV0_X7kMSVaE257aTivPkkuZLO0A},
  urldate = {2021-10-10},
  file = {/Users/cperivol/Zotero/storage/TFYM4ANT/1090189.html}
}

@article{azimRecacheReactiveCaching2017,
  title = {Recache: Reactive Caching for Fast Analytics over Heterogeneous Data},
  shorttitle = {Recache},
  author = {Azim, Tahir and Karpathiotakis, Manos and Ailamaki, Anastasia},
  date = {2017},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {11},
  pages = {324--337},
  issue = {ARTICLE}
}

@online{BacktrackingInterleavingTerminating,
  title = {Backtracking, Interleaving, and Terminating Monad Transformers: (Functional Pearl): {{ACM SIGPLAN Notices}}: Vol 40, {{No}} 9},
  url = {https://dl.acm.org/doi/abs/10.1145/1090189.1086390},
  urldate = {2021-10-20},
  file = {/Users/cperivol/Zotero/storage/H6GJA5B6/1090189.html}
}

@inproceedings{barataOverviewDecisionSupport2015,
  title = {An {{Overview}} of {{Decision Support Benchmarks}}: {{TPC}}-{{DS}}, {{TPC}}-{{H}} and {{SSB}}},
  shorttitle = {An {{Overview}} of {{Decision Support Benchmarks}}},
  booktitle = {New {{Contributions}} in {{Information Systems}} and {{Technologies}}},
  author = {Barata, Melyssa and Bernardino, Jorge and Furtado, Pedro},
  editor = {Rocha, Alvaro and Correia, Ana Maria and Costanzo, Sandra and Reis, Luis Paulo},
  date = {2015},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {619--628},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-16486-1_61},
  abstract = {Database management systems form the core of all business intelligence solutions, with information being the key to success in today’s businesses. Performance evaluation of database systems is a non-trivial activity, due to different architectures and functionalities tuned for specific requirements. In this paper we provide an overview and analyze the main properties of three benchmarks: TPC-DS, TPC-H and the SSB which are online analytical processing benchmarks in order to validate the performance of a Decision Support System.},
  isbn = {978-3-319-16486-1},
  langid = {english},
  keywords = {Analysis,Benchmark,Database,Metric},
  file = {/Users/cperivol/Zotero/storage/J6FD6MJQ/Barata et al. - 2015 - An Overview of Decision Support Benchmarks TPC-DS.pdf}
}

@book{bartoszmilewskiDaoFunctionalProgramming,
  title = {The {{Dao}} of {{Functional Programming}}},
  author = {, Bartosz Milewski},
  url = {https://github.com/BartoszMilewski/Publications/blob/master/TheDaoOfFP/DaoFP.pdf}
}

@article{bayirGeneticAlgorithmMultipleQuery2007,
  title = {Genetic {{Algorithm}} for the {{Multiple}}-{{Query Optimization Problem}}},
  author = {Bayir, M. and Toroslu, I. H. and Cosar, A.},
  date = {2007},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  doi = {10.1109/TSMCC.2006.876060},
  abstract = {This correspondence is the first attempt to solve MQO using an evolutionary technique, genetic algorithms, which minimizes the total execution time by performing common tasks only once. Producing answers to a set of queries with common tasks efficiently is known as the multiple-query optimization (MQO) problem. Each query can have several alternative evaluation plans, each with a different set of tasks. Therefore, the goal of MQO is to choose the right set of plans for queries which minimizes the total execution time by performing common tasks only once. Since MQO is an NP-hard problem, several, mostly heuristics based, solutions have been proposed for solving it. To the best of our knowledge, this correspondence is the first attempt to solve MQO using an evolutionary technique, genetic algorithms},
  annotation = {00000}
}

@inproceedings{begoliApacheCalciteFoundational2018,
  title = {Apache Calcite: A Foundational Framework for Optimized Query Processing over Heterogeneous Data Sources},
  shorttitle = {Apache Calcite},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Begoli, Edmon and Camacho-Rodríguez, Jesús and Hyde, Julian and Mior, Michael J. and Lemire, Daniel},
  date = {2018},
  pages = {221--230},
  file = {/Users/cperivol/Zotero/storage/JGP4UPNB/Begoli et al. - 2018 - Apache calcite A foundational framework for optim.pdf;/Users/cperivol/Zotero/storage/IFCQGQNY/3183713.html}
}

@inproceedings{benkridFrameworkDesigningAutonomous2019,
  title = {A Framework for Designing Autonomous Parallel Data Warehouses},
  booktitle = {International {{Conference}} on {{Algorithms}} and {{Architectures}} for {{Parallel Processing}}},
  author = {Benkrid, Soumia and Bellatreche, Ladjel},
  date = {2019},
  pages = {97--104},
  publisher = {{Springer}},
  file = {/Users/cperivol/Zotero/storage/LXFE3CPW/978-3-030-38961-1_9.html}
}

@incollection{benkridGlobalParadigmDesigning2014,
  title = {A Global Paradigm for Designing Parallel Relational Data Warehouses in Distributed Environments},
  booktitle = {Transactions on {{Large}}-{{Scale Data}}-and {{Knowledge}}-{{Centered Systems XV}}},
  author = {Benkrid, Soumia and Bellatreche, Ladjel and Cuzzocrea, Alfredo},
  date = {2014},
  pages = {64--101},
  publisher = {{Springer}},
  file = {/Users/cperivol/Zotero/storage/7GF6LZIM/978-3-662-45761-0_3.html}
}

@inproceedings{bhatotiaIncoopMapReduceIncremental2011,
  title = {Incoop: {{MapReduce}} for Incremental Computations},
  shorttitle = {Incoop},
  booktitle = {Proceedings of the 2nd {{ACM Symposium}} on {{Cloud Computing}}},
  author = {Bhatotia, Pramod and Wieder, Alexander and Rodrigues, Rodrigo and Acar, Umut A. and Pasquin, Rafael},
  date = {2011-10-26},
  series = {{{SOCC}} '11},
  pages = {1--14},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2038916.2038923},
  url = {https://doi.org/10.1145/2038916.2038923},
  urldate = {2021-10-09},
  abstract = {Many online data sets evolve over time as new entries are slowly added and existing entries are deleted or modified. Taking advantage of this, systems for incremental bulk data processing, such as Google's Percolator, can achieve efficient updates. To achieve this efficiency, however, these systems lose compatibility with the simple programming models offered by non-incremental systems, e.g., MapReduce, and more importantly, requires the programmer to implement application-specific dynamic algorithms, ultimately increasing algorithm and code complexity. In this paper, we describe the architecture, implementation, and evaluation of Incoop, a generic MapReduce framework for incremental computations. Incoop detects changes to the input and automatically updates the output by employing an efficient, fine-grained result reuse mechanism. To achieve efficiency without sacrificing transparency, we adopt recent advances in the area of programming languages to identify the shortcomings of task-level memoization approaches, and to address these shortcomings by using several novel techniques: a storage system, a contraction phase for Reduce tasks, and an affinity-based scheduling algorithm. We have implemented Incoop by extending the Hadoop framework, and evaluated it by considering several applications and case studies. Our results show significant performance improvements without changing a single line of application code.},
  isbn = {978-1-4503-0976-9},
  keywords = {memoization,self-adjusting computation,stability},
  file = {/Users/cperivol/Zotero/storage/XXN8M9FM/Bhatotia et al. - 2011 - Incoop MapReduce for incremental computations.pdf}
}

@inproceedings{bianWideTableLayout2017,
  title = {Wide Table Layout Optimization Based on Column Ordering and Duplication},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}}},
  author = {Bian, Haoqiong and Yan, Ying and Tao, Wenbo and Chen, Liang Jeff and Chen, Yueguo and Du, Xiaoyong and Moscibroda, Thomas},
  date = {2017},
  pages = {299--314},
  file = {/Users/cperivol/Zotero/storage/SRZQ6985/Bian et al. - 2017 - Wide table layout optimization based on column ord.pdf;/Users/cperivol/Zotero/storage/N6PTX95Q/3035918.html}
}

@inproceedings{boukorcaHYPADHypergraphdrivenApproach2015,
  title = {{{HYPAD}}: Hyper-Graph-Driven Approach for Parallel Data Warehouse Design},
  shorttitle = {{{HYPAD}}},
  booktitle = {International {{Conference}} on {{Algorithms}} and {{Architectures}} for {{Parallel Processing}}},
  author = {Boukorca, Ahcene and Bellatreche, Ladjel and Benkrid, Soumia},
  date = {2015},
  pages = {770--783},
  publisher = {{Springer}},
  file = {/Users/cperivol/Zotero/storage/JIFVZA8T/978-3-319-27140-8_53.html}
}

@inproceedings{bransenIncrementalEvaluationHigher2015,
  title = {Incremental {{Evaluation}} of {{Higher Order Attributes}}},
  booktitle = {Proceedings of the 2015 {{Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {Bransen, Jeroen and Dijkstra, Atze and Swierstra, S. Doaitse},
  date = {2015-01-13},
  series = {{{PEPM}} '15},
  pages = {39--48},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2678015.2682541},
  url = {https://doi.org/10.1145/2678015.2682541},
  urldate = {2021-03-16},
  abstract = {Compilers, amongst other programs, often work with data that (slowly) changes over time. When the changes between subsequent runs of the compiler are small, one would hope the compiler to incrementally update its results, resulting in much lower running times. However, the manual construction of an incremental compiler is very hard and error prone and therefore usually not an option. Attribute grammars provide an attractive way of constructing compilers, as they are compositional in nature and allow for aspect oriented programming. In this work we extend previous work on the automatic generation of incremental attribute grammar evaluators, with the purpose of (semi-)automatically generating an incremental compiler from the regular attribute grammar definition, by adding support for incremental evaluation of higher order attributes, a well known extension to the classical attribute grammars that is used in many ways in compiler construction, for example to model different compiler phases.},
  isbn = {978-1-4503-3297-2},
  keywords = {attribute grammars,change propagation,incremental evaluation,program transformation,type inference},
  file = {/Users/cperivol/Zotero/storage/EAGMX3M2/Bransen et al. - 2015 - Incremental Evaluation of Higher Order Attributes.pdf}
}

@article{brunoContinuousCloudscaleQuery2013,
  title = {Continuous Cloud-Scale Query Optimization and Processing},
  author = {Bruno, Nicolas and Jain, Sapna and Zhou, Jingren},
  date = {2013},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {6},
  number = {11},
  pages = {961--972},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/5MSP2SI9/Bruno et al. - 2013 - Continuous cloud-scale query optimization and proc.pdf;/Users/cperivol/Zotero/storage/QY8N4SI9/2536222.html}
}

@thesis{camacho-rodriguezPigReuseReusebasedOptimizer2016,
  type = {phdthesis},
  title = {{{PigReuse}}: A {{Reuse}}-Based {{Optimizer}} for {{Pig Latin}}},
  shorttitle = {{{PigReuse}}},
  author = {Camacho-Rodríguez, Jesús and Colazzo, Dario and Herschel, Melanie and Manolescu, Ioana and Chowdhury, Soudip Roy},
  date = {2016},
  institution = {{Inria Saclay}}
}

@inproceedings{camacho-rodriguezReusebasedOptimizationPig2016,
  title = {Reuse-Based Optimization for Pig Latin},
  booktitle = {Proceedings of the 25th {{ACM International}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Camacho-Rodríguez, Jesús and Colazzo, Dario and Herschel, Melanie and Manolescu, Ioana and Roy Chowdhury, Soudip},
  date = {2016},
  pages = {2215--2220},
  file = {/Users/cperivol/Zotero/storage/5GR8TTDC/Camacho-Rodríguez et al. - 2016 - Reuse-based optimization for pig latin.pdf;/Users/cperivol/Zotero/storage/RKNZH5DF/2983323.html}
}

@article{chamberlinHistoryEvaluationSystem1981,
  title = {A History and Evaluation of {{System R}}},
  author = {Chamberlin, Donald D. and Astrahan, Morton M. and Blasgen, Michael W. and Gray, James N. and King, W. Frank and Lindsay, Bruce G. and Lorie, Raymond and Mehl, James W. and Price, Thomas G. and Putzolu, Franco and Selinger, Patricia Griffiths and Schkolnick, Mario and Slutz, Donald R. and Traiger, Irving L. and Wade, Bradford W. and Yost, Robert A.},
  date = {1981-10-01},
  journaltitle = {Commun. ACM},
  volume = {24},
  number = {10},
  pages = {632--646},
  issn = {0001-0782},
  doi = {10.1145/358769.358784},
  url = {https://doi.org/10.1145/358769.358784},
  urldate = {2021-11-28},
  abstract = {System R, an experimental database system, was constructed to demonstrate that the usability advantages of the relational data model can be realized in a system with the complete function and high performance required for everyday production use. This paper describes the three principal phases of the System R project and discusses some of the lessons learned from System R about the design of relational systems and database systems in general.},
  keywords = {access path selection,authorization,compilation,database management systems,locking,recovery,relational model},
  annotation = {00339},
  file = {/Users/cperivol/Zotero/storage/MTU7YTYC/Chamberlin et al. - 1981 - A history and evaluation of System R.pdf}
}

@inproceedings{chandeGeneticOptimizationJoin2011,
  title = {Genetic Optimization for the Join Ordering Problem of Database Queries},
  booktitle = {2011 {{Annual IEEE India Conference}}},
  author = {Chande, Swati V. and Sinha, Madhavi},
  date = {2011},
  pages = {1--5},
  publisher = {{IEEE}},
  file = {/Users/cperivol/Zotero/storage/7NUS5HJB/6139336.html}
}

@article{chaudhuriAutoAdminWhatifIndex1998,
  title = {{{AutoAdmin}} “What-If” Index Analysis Utility},
  author = {Chaudhuri, Surajit and Narasayya, Vivek},
  date = {1998},
  journaltitle = {ACM SIGMOD Record},
  volume = {27},
  number = {2},
  pages = {367--378},
  publisher = {{ACM New York, NY, USA}},
  file = {/Users/cperivol/Zotero/storage/5A9KKG7Q/Chaudhuri and Narasayya - 1998 - AutoAdmin “what-if” index analysis utility.pdf;/Users/cperivol/Zotero/storage/VN8BGM57/276305.html}
}

@article{chaudhuriPayasyougoFrameworkQuery2008,
  title = {A Pay-as-You-Go Framework for Query Execution Feedback},
  author = {Chaudhuri, Surajit and Narasayya, Vivek and Ramamurthy, Ravi},
  date = {2008},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {1},
  number = {1},
  pages = {1141--1152},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/666HX54E/Chaudhuri et al. - 2008 - A pay-as-you-go framework for query execution feed.pdf;/Users/cperivol/Zotero/storage/8W2AB7YU/1453856.html}
}

@inproceedings{chaudhuriSelftuningDatabaseSystems2007,
  title = {Self-Tuning Database Systems: A Decade of Progress},
  shorttitle = {Self-Tuning Database Systems},
  booktitle = {Proceedings of the 33rd International Conference on {{Very}} Large Data Bases},
  author = {Chaudhuri, Surajit and Narasayya, Vivek},
  date = {2007},
  pages = {3--14},
  file = {/Users/cperivol/Zotero/storage/W52AFDQI/Chaudhuri and Narasayya - 2007 - Self-tuning database systems a decade of progress.pdf}
}

@article{chenMemSQLQueryOptimizer2016,
  title = {The {{MemSQL}} Query Optimizer: A Modern Optimizer for Real-Time Analytics in a Distributed Database},
  shorttitle = {The {{MemSQL}} Query Optimizer},
  author = {Chen, Jack and Jindel, Samir and Walzer, Robert and Sen, Rajkumar and Jimsheleishvilli, Nika and Andrews, Michael},
  date = {2016-09-01},
  journaltitle = {Proc. VLDB Endow.},
  volume = {9},
  number = {13},
  pages = {1401--1412},
  issn = {2150-8097},
  doi = {10.14778/3007263.3007277},
  url = {https://doi.org/10.14778/3007263.3007277},
  urldate = {2021-11-28},
  abstract = {Real-time analytics on massive datasets has become a very common need in many enterprises. These applications require not only rapid data ingest, but also quick answers to analytical queries operating on the latest data. MemSQL is a distributed SQL database designed to exploit memory-optimized, scale-out architecture to enable real-time transactional and analytical workloads which are fast, highly concurrent, and extremely scalable. Many analytical queries in MemSQL's customer workloads are complex queries involving joins, aggregations, sub-queries, etc. over star and snowflake schemas, often ad-hoc or produced interactively by business intelligence tools. These queries often require latencies of seconds or less, and therefore require the optimizer to not only produce a high quality distributed execution plan, but also produce it fast enough so that optimization time does not become a bottleneck. In this paper, we describe the architecture of the MemSQL Query Optimizer and the design choices and innovations which enable it quickly produce highly efficient execution plans for complex distributed queries. We discuss how query rewrite decisions oblivious of distribution cost can lead to poor distributed execution plans, and argue that to choose high-quality plans in a distributed database, the optimizer needs to be distribution-aware in choosing join plans, applying query rewrites, and costing plans. We discuss methods to make join enumeration faster and more effective, such as a rewrite-based approach to exploit bushy joins in queries involving multiple star schemas without sacrificing optimization time. We demonstrate the effectiveness of the MemSQL optimizer over queries from the TPC-H benchmark and a real customer workload.},
  annotation = {00040},
  file = {/Users/cperivol/Zotero/storage/ZT2TM6I4/Chen et al. - 2016 - The MemSQL query optimizer a modern optimizer for.pdf}
}

@article{chirkovaMaterializedViews2011,
  title = {Materialized Views},
  author = {Chirkova, Rada and Yang, Jun},
  date = {2011},
  journaltitle = {Foundations and Trends in Databases},
  volume = {4},
  number = {4},
  pages = {295--405},
  publisher = {{Citeseer}},
  file = {/Users/cperivol/Zotero/storage/CL6RS42Q/Chirkova and Yang - 2011 - Materialized views.pdf}
}

@online{chuAxiomaticFoundationsAlgorithms2018,
  title = {Axiomatic Foundations and Algorithms for Deciding Semantic Equivalences of {{SQL}} Queries},
  author = {Chu, Shumo and Murphy, Brendan and Roesch, Jared and Cheung, Alvin and Suciu, Dan},
  date = {2018},
  eprint = {1802.02229},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/cperivol/Zotero/storage/AX692ULH/Chu et al. - 2018 - Axiomatic foundations and algorithms for deciding .pdf;/Users/cperivol/Zotero/storage/VR9A5XBP/1802.html}
}

@inproceedings{chuCosetteAutomatedSQL2017,
  title = {Cosette: An Automated {{SQL}} Prover},
  shorttitle = {Cosette},
  author = {Chu, Shumo and Wang, Chenglong and Weitz, Konstantin and Cheung, Alvin},
  date = {2017},
  publisher = {{CIDR}}
}

@inproceedings{chuDemonstrationCosetteAutomated2017,
  title = {Demonstration of the Cosette Automated Sql Prover},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}}},
  author = {Chu, Shumo and Li, Daniel and Wang, Chenglong and Cheung, Alvin and Suciu, Dan},
  date = {2017},
  pages = {1591--1594},
  file = {/Users/cperivol/Zotero/storage/H8XH3DFF/Chu et al. - 2017 - Demonstration of the cosette automated sql prover.pdf;/Users/cperivol/Zotero/storage/96LZBSAB/3035918.html}
}

@article{chuHoTTSQLProvingQuery2017,
  title = {{{HoTTSQL}}: Proving Query Rewrites with Univalent {{SQL}} Semantics},
  shorttitle = {{{HoTTSQL}}},
  author = {Chu, Shumo and Weitz, Konstantin and Cheung, Alvin and Suciu, Dan},
  date = {2017},
  journaltitle = {ACM SIGPLAN Notices},
  volume = {52},
  number = {6},
  pages = {510--524},
  publisher = {{ACM New York, NY, USA}},
  file = {/Users/cperivol/Zotero/storage/AKL2ND3M/Chu et al. - 2017 - HoTTSQL Proving query rewrites with univalent SQL.pdf;/Users/cperivol/Zotero/storage/T9XBCIY5/3140587.html}
}

@inproceedings{chungPeeringDarkOwl2019,
  title = {Peering through the Dark: An Owl's View of Inter-Job Dependencies and Jobs' Impact in Shared Clusters},
  shorttitle = {Peering through the Dark},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Chung, Andrew and Curino, Carlo and Krishnan, Subru and Karanasos, Konstantinos and Garefalakis, Panagiotis and Ganger, Gregory R.},
  date = {2019},
  pages = {1889--1892},
  file = {/Users/cperivol/Zotero/storage/JJTXBEHR/Chung et al. - 2019 - Peering through the dark An owl's view of inter-j.pdf;/Users/cperivol/Zotero/storage/65LVK6SJ/3299869.html}
}

@online{DatabaseSystemsComplete,
  title = {Database {{Systems}}: The {{Complete Book}}},
  url = {http://infolab.stanford.edu/~ullman/dscb.html},
  urldate = {2021-08-26},
  file = {/Users/cperivol/Zotero/storage/ISNHARK3/dscb.html}
}

@inproceedings{datrindadeKaskadeGraphViews2020,
  title = {Kaskade: Graph Views for Efficient Graph Analytics},
  shorttitle = {Kaskade},
  booktitle = {2020 {{IEEE}} 36th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {da Trindade, Joana MF and Karanasos, Konstantinos and Curino, Carlo and Madden, Samuel and Shun, Julian},
  options = {useprefix=true},
  date = {2020},
  pages = {193--204},
  publisher = {{IEEE}},
  file = {/Users/cperivol/Zotero/storage/CUND8RKL/da Trindade et al. - 2020 - Kaskade Graph views for efficient graph analytics.pdf;/Users/cperivol/Zotero/storage/A23KEQ67/9101351.html}
}

@article{dawsonCompoundMonadsKleisli2007,
  title = {Compound Monads and the Kleisli Category},
  author = {Dawson, Jeremy E},
  date = {2007},
  journaltitle = {Unpublished note. Available online at http://users. cecs. anu. edu. au/\textasciitilde{} jeremy/pubs/cmkc},
  publisher = {{Citeseer}}
}

@inproceedings{degiacomoDecidabilityQueryContainment1999,
  title = {On the Decidability of Query Containment under Constraints},
  author = {De Giacomo, G. and Calvanese, D. and Lenzerini, M.},
  date = {1999},
  publisher = {{PODS}}
}

@inproceedings{dietzTwoAlgorithmsMaintaining1987,
  title = {Two Algorithms for Maintaining Order in a List},
  booktitle = {Proceedings of the Nineteenth Annual {{ACM}} Conference on {{Theory}} of Computing  - {{STOC}} '87},
  author = {Dietz, P. and Sleator, D.},
  date = {1987},
  pages = {365--372},
  publisher = {{ACM Press}},
  location = {{New York, New York, United States}},
  doi = {10.1145/28395.28434},
  url = {http://portal.acm.org/citation.cfm?doid=28395.28434},
  urldate = {2021-10-10},
  abstract = {The order maintenance problem is that of maintaining a list under a sequence of Insert and Delete operations, while answering Order queries (determine which of two records comes first in the list). We give two new algorithms for this problem. The first algorithm matches the 0(1)amortized time per operation of the best previously known algorithm, and is much simpler. The second algorithm permits all operations to be performed in 0(1)worst-case time.},
  eventtitle = {The Nineteenth Annual {{ACM}} Conference},
  isbn = {978-0-89791-221-1},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/N36MNPZE/Dietz and Sleator - 1987 - Two algorithms for maintaining order in a list.pdf}
}

@article{dingPlanStitchHarnessing2018,
  title = {Plan Stitch: Harnessing the Best of Many Plans},
  shorttitle = {Plan Stitch},
  author = {Ding, Bailu and Das, Sudipto and Wu, Wentao and Chaudhuri, Surajit and Narasayya, Vivek},
  date = {2018-06},
  journaltitle = {Proc. VLDB Endow.},
  volume = {11},
  number = {10},
  pages = {1123--1136},
  issn = {2150-8097},
  doi = {10.14778/3231751.3231761},
  url = {https://dl.acm.org/doi/10.14778/3231751.3231761},
  urldate = {2021-11-25},
  abstract = {Query performance regression due to the query optimizer selecting a bad query execution plan is a major pain point in production workloads. Commercial DBMSs today can automatically detect and correct such query plan regressions by storing previouslyexecuted plans and reverting to a previous plan which is still valid and has the least execution cost. Such reversion-based plan correction has relatively low risk of plan regression since the decision is based on observed execution costs. However, this approach ignores potentially valuable information of efficient subplans collected from other previously-executed plans. In this paper, we propose a novel technique, Plan Stitch, that automatically and opportunistically combines efficient subplans of previously-executed plans into a valid new plan, which can be cheaper than any individual previously-executed plan. We implement Plan Stitch on top of Microsoft SQL Server. Our experiments on TPC-DS benchmark and three real-world customer workloads show that plans obtained via Plan Stitch can reduce execution cost significantly, with a reduction of up to two orders of magnitude in execution cost when compared to reverting to the cheapest previously-executed plan.},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/SPCVL8PD/Ding et al. - 2018 - Plan stitch harnessing the best of many plans.pdf}
}

@incollection{domdouzisInMemoryDatabases2021,
  title = {In-{{Memory Databases}}},
  booktitle = {Concise {{Guide}} to {{Databases}}: A {{Practical Introduction}}},
  author = {Domdouzis, Konstantinos and Lake, Peter and Crowther, Paul},
  editor = {Domdouzis, Konstantinos and Lake, Peter and Crowther, Paul},
  date = {2021},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  pages = {189--203},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-42224-0_8},
  url = {https://doi.org/10.1007/978-3-030-42224-0_8},
  urldate = {2021-11-29},
  abstract = {What the reader will learn:},
  isbn = {978-3-030-42224-0},
  langid = {english},
  annotation = {00000},
  file = {/Users/cperivol/Zotero/storage/6NT4VHGV/Domdouzis et al. - 2021 - In-Memory Databases.pdf}
}

@inproceedings{durandGridformationSelfdrivenOnline2018,
  title = {Gridformation: Towards Self-Driven Online Data Partitioning Using Reinforcement Learning},
  shorttitle = {Gridformation},
  booktitle = {Proceedings of the {{First International Workshop}} on {{Exploiting Artificial Intelligence Techniques}} for {{Data Management}}},
  author = {Durand, Gabriel Campero and Pinnecke, Marcus and Piriyev, Rufat and Mohsen, Mahmoud and Broneske, David and Saake, Gunter and Sekeran, Maya S. and Rodriguez, Fabián and Balami, Laxmi},
  date = {2018},
  pages = {1--7}
}

@inproceedings{dursunRevisitingReuseMain2017,
  title = {Revisiting Reuse in Main Memory Database Systems},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}}},
  author = {Dursun, Kayhan and Binnig, Carsten and Cetintemel, Ugur and Kraska, Tim},
  date = {2017},
  pages = {1275--1289},
  file = {/Users/cperivol/Zotero/storage/VEAY5N8X/Dursun et al. - 2017 - Revisiting reuse in main memory database systems.pdf;/Users/cperivol/Zotero/storage/TLFY7FU3/3035918.html}
}

@inproceedings{eichDynamicProgrammingNext2015,
  title = {Dynamic Programming: The next Step},
  shorttitle = {Dynamic Programming},
  booktitle = {2015 {{IEEE}} 31st {{International Conference}} on {{Data Engineering}}},
  author = {Eich, Marius and Moerkotte, Guido},
  date = {2015},
  pages = {903--914},
  publisher = {{IEEE}},
  file = {/Users/cperivol/Zotero/storage/52G6XXHX/7113343.html}
}

@misc{eidhofRapidPrototypingTEX2009,
  title = {Rapid {{Prototyping}} in {{TEX}} 5},
  author = {Eidhof, Chris and Elkins, Derek and Hicks, Stephen and Lempsink, Eelco and Swierstra, Wouter and Hicks, Stephen and Yorgey, Brent},
  date = {2009},
  file = {/Users/cperivol/Zotero/storage/945SCPWL/Eidhof et al. - 2009 - Rapid Prototyping in TEX 5.pdf}
}

@online{elghandourReStoreReusingResults2012,
  title = {{{ReStore}}: Reusing Results of {{MapReduce}} Jobs},
  shorttitle = {{{ReStore}}},
  author = {Elghandour, Iman and Aboulnaga, Ashraf},
  date = {2012},
  eprint = {1203.0061},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/cperivol/Zotero/storage/5738YWYN/Elghandour and Aboulnaga - 2012 - ReStore reusing results of MapReduce jobs.pdf;/Users/cperivol/Zotero/storage/JCS7Q8UZ/1203.html}
}

@online{elghandourReStoreReusingResults2012a,
  title = {{{ReStore}}: Reusing {{Results}} of {{MapReduce Jobs}}},
  shorttitle = {{{ReStore}}},
  author = {Elghandour, Iman and Aboulnaga, Ashraf},
  date = {2012-02-29},
  eprint = {1203.0061},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1203.0061},
  urldate = {2021-11-26},
  abstract = {Analyzing large scale data has emerged as an important activity for many organizations in the past few years. This large scale data analysis is facilitated by the MapReduce programming and execution model and its implementations, most notably Hadoop. Users of MapReduce often have analysis tasks that are too complex to express as individual MapReduce jobs. Instead, they use high-level query languages such as Pig, Hive, or Jaql to express their complex tasks. The compilers of these languages translate queries into workflows of MapReduce jobs. Each job in these workflows reads its input from the distributed file system used by the MapReduce system and produces output that is stored in this distributed file system and read as input by the next job in the workflow. The current practice is to delete these intermediate results from the distributed file system at the end of executing the workflow. One way to improve the performance of workflows of MapReduce jobs is to keep these intermediate results and reuse them for future workflows submitted to the system. In this paper, we present ReStore, a system that manages the storage and reuse of such intermediate results. ReStore can reuse the output of whole MapReduce jobs that are part of a workflow, and it can also create additional reuse opportunities by materializing and storing the output of query execution operators that are executed within a MapReduce job. We have implemented ReStore as an extension to the Pig dataflow system on top of Hadoop, and we experimentally demonstrate significant speedups on queries from the PigMix benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases},
  annotation = {00167},
  file = {/Users/cperivol/Zotero/storage/5YVWMEMX/Elghandour and Aboulnaga - 2012 - ReStore Reusing Results of MapReduce Jobs.pdf;/Users/cperivol/Zotero/storage/T2WVD49K/1203.html}
}

@online{EvolutionLispHistory,
  title = {The Evolution of {{Lisp}} | {{History}} of Programming Languages---{{II}}},
  url = {https://dl.acm.org/doi/abs/10.1145/234286.1057818?casa_token=aOA-COsG02UAAAAA:8iFrsUj14sq78O_oXEQYNiSv_LsA6DAXPQ4gMSddJ2OGZN4vQIktwZp6uouedMb6hRe69cwYN152-A},
  urldate = {2021-10-09}
}

@article{fernandesMemoizedZipperbasedAttribute2019,
  title = {Memoized Zipper-Based Attribute Grammars and Their Higher Order Extension},
  author = {Fernandes, João Paulo and Martins, Pedro and Pardo, Alberto and Saraiva, João and Viera, Marcos},
  date = {2019-03-15},
  journaltitle = {Science of Computer Programming},
  series = {Brazilian {{Symposium}} on {{Programming Languages}} ({{SBLP}} '15+16)},
  volume = {173},
  pages = {71--94},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2018.10.006},
  url = {https://www.sciencedirect.com/science/article/pii/S016764231830412X},
  urldate = {2021-02-25},
  abstract = {Attribute grammars are a powerfull, well-known formalism to implement and reason about programs which, by design, are conveniently modular. In this work we focus on a state of the art zipper-based embedding of classic attribute grammars and higher-order attribute grammars. We improve their execution performance through controlling attribute (re)evaluation by means of memoization techniques. We present the results of our optimizations by comparing their impact in various implementations of different, well-studied, attribute grammars and their Higher-Order extensions.},
  langid = {english},
  keywords = {Attribute grammars,Embedded domain specific languages,Functional programming,Memoization,Zipper data structure},
  file = {/Users/cperivol/Zotero/storage/REPYVTWN/S016764231830412X.html}
}

@article{fernandesMemoizedZipperbasedAttribute2019a,
  title = {Memoized Zipper-Based Attribute Grammars and Their Higher Order Extension},
  author = {Fernandes, João Paulo and Martins, Pedro and Pardo, Alberto and Saraiva, João and Viera, Marcos},
  date = {2019-03-15},
  journaltitle = {Science of Computer Programming},
  series = {Brazilian {{Symposium}} on {{Programming Languages}} ({{SBLP}} '15+16)},
  volume = {173},
  pages = {71--94},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2018.10.006},
  url = {https://www.sciencedirect.com/science/article/pii/S016764231830412X},
  urldate = {2021-07-08},
  abstract = {Attribute grammars are a powerfull, well-known formalism to implement and reason about programs which, by design, are conveniently modular. In this work we focus on a state of the art zipper-based embedding of classic attribute grammars and higher-order attribute grammars. We improve their execution performance through controlling attribute (re)evaluation by means of memoization techniques. We present the results of our optimizations by comparing their impact in various implementations of different, well-studied, attribute grammars and their Higher-Order extensions.},
  langid = {english},
  keywords = {Attribute grammars,Embedded domain specific languages,Functional programming,Memoization,Zipper data structure}
}

@inproceedings{firsovPurelyFunctionalIncremental2016,
  title = {Purely {{Functional Incremental Computing}}},
  booktitle = {Programming {{Languages}}},
  author = {Firsov, Denis and Jeltsch, Wolfgang},
  editor = {Castor, Fernando and Liu, Yu David},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {62--77},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-45279-1_5},
  abstract = {Many applications have to maintain evolving data sources as well as views on these sources. If sources change, the corresponding views have to be adapted. Complete recomputation of views is typically too expensive. An alternative is to convert source changes into view changes and apply these to the views. This is the key idea of incremental computing. In this paper, we use Haskell to develop an incremental computing framework. We illustrate the concepts behind this framework by implementing several example computations on sequences. Our framework allows the user to implement incremental computations using arbitrary monad families that encapsulate mutable state. This makes it possible to use highly efficient algorithms for core computations.},
  isbn = {978-3-319-45279-1},
  langid = {english},
  keywords = {Change Propagation,Mutable State,Mutable Variable,Recursion Scheme,View Change},
  file = {/Users/cperivol/Zotero/storage/6BXXMIV9/Firsov and Jeltsch - 2016 - Purely Functional Incremental Computing.pdf}
}

@book{fischerReinventingHaskellBacktracking2009,
  title = {Reinventing Haskell Backtracking},
  author = {Fischer, Sebastian},
  date = {2009},
  publisher = {{Gesellschaft für Informatik e. V.}},
  issn = {1617-5468},
  url = {http://dl.gi.de/handle/20.500.12116/31292},
  urldate = {2021-10-21},
  abstract = {Almost ten years ago, Ralf Hinze has written a functional pearl on how to derive backtracking functionality for the purely functional programming language Haskell. In these notes, we show how to arrive at the efficient, two-continuation based backtracking monad derived by Hinze starting from an intuitive inefficient implementation that we subsequently refine using well known program transformations. It turns out that the technique can be used to build monads for non-determinism from modular, independent parts which gives rise to various new implementations. Specifically, we show how the presented approach can be applied to obtain new im- plementations of breadth-first search and iterative deepening depth-first search.},
  isbn = {978-3-88579-248-2},
  langid = {english},
  annotation = {Accepted: 2020-01-28T13:26:32Z},
  file = {/Users/cperivol/Zotero/storage/A98VE7MN/Fischer - 2009 - Reinventing haskell backtracking.pdf;/Users/cperivol/Zotero/storage/RTG2KG4E/31292.html}
}

@inproceedings{friedmanApproachFairApplicative1979,
  title = {An Approach to Fair Applicative Multiprogramming},
  booktitle = {Semantics of {{Concurrent Computation}}},
  author = {Friedman, Daniel P. and Wise, David S.},
  editor = {Kahn, Gilles},
  date = {1979},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {203--225},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0022471},
  abstract = {This paper presents a brief formal semantics of constructors for ordered sequences (cons) and for unordered multisets (frons) followed by a detailed operational semantics for both. A multiset is a generalization of a list structure which lacks order a priori; its order is determined by the a posteriori migration of computationally convergent elements to the front. The introductory material includes an example which demonstrates that a multiset of yet-unconverged values and a timing primitive may be used to implement the scheduler for an operating system in an applicative style. The operational semantics, given in PASCAL-like code, is described in two detailed steps: first a uniprocessor implementation of the cons/frons constructors and the first/rest probes, followed by an extension to a multiprocessor implementation. The center of either implementation is the EUREKA structure transformation, which brings convergent elements to the fore while preserving order of shared structures. The multiprocessor version is designed to run on an arbitrary number of processors with only one semaphore but makes heavy use of the sting memory store primitive. Stinging is a conditional store operation which is carried out independently of its dispatching processor so that shared nodes may be somewhat altered without interfering with other processors. An appendix presents the extension of this code to a ‚fair“ implementation of multisets.},
  isbn = {978-3-540-35163-4},
  langid = {english},
  keywords = {Accessible Node,Formal Semantic,Memory Operation,Operational Semantic,Unordered Structure},
  file = {/Users/cperivol/Zotero/storage/VV23Q6RV/Friedman and Wise - 1979 - An approach to fair applicative multiprogramming.pdf}
}

@book{friedmanEssentialsProgrammingLanguages2008,
  title = {Essentials of Programming Languages},
  author = {Friedman, Daniel P. and Wand, Mitchell},
  date = {2008},
  edition = {3rd ed},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA}},
  isbn = {978-0-262-06279-4},
  langid = {english},
  pagetotal = {410},
  keywords = {Programming languages (Electronic computers)},
  file = {/Users/cperivol/Zotero/storage/XDHAA5M2/Friedman and Wand - 2008 - Essentials of programming languages.pdf}
}

@article{funkeHyPerSoftwareExploiting2014,
  title = {{{HyPer Beyond Software}}: Exploiting {{Modern Hardware}} for {{Main}}-{{Memory Database Systems}}},
  shorttitle = {{{HyPer Beyond Software}}},
  author = {Funke, Florian and Kemper, Alfons and Mühlbauer, Tobias and Neumann, Thomas and Leis, Viktor},
  date = {2014-11-01},
  journaltitle = {Datenbank Spektrum},
  volume = {14},
  number = {3},
  pages = {173--181},
  issn = {1610-1995},
  doi = {10.1007/s13222-014-0165-y},
  url = {https://doi.org/10.1007/s13222-014-0165-y},
  urldate = {2021-11-25},
  abstract = {In this paper, we survey the use of advanced hardware features for optimizing main-memory database systems in the context of our HyPer project. We exploit the virtual memory management for snapshotting the transactional data in order to separate OLAP queries from parallel OLTP transactions. The access behavior of database objects from simultaneous OLTP transactions is monitored using the virtual memory management component in order to compact the database into hot and cold partitions. Utilizing many-core NUMA-organized database servers is facilitated by the morsel-driven adaptive parallelization and partitioning that guarantees data locality w.r.t.~the processing core. The most recent Hardware Transactional Memory support of, e.g., Intel’s Haswell processor, can be used as the basis for a lock-free concurrency control scheme for OLTP transactions. Finally, we show how heterogeneous processors of “wimpy” devices such as tablets can be utilized for high-performance and energy-efficient query processing.},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/DSXZEIUW/Funke et al. - 2014 - HyPer Beyond Software Exploiting Modern Hardware .pdf}
}

@inproceedings{ganzTrampolinedStyle1999,
  title = {Trampolined Style},
  booktitle = {Proceedings of the Fourth {{ACM SIGPLAN}} International Conference on {{Functional}} Programming},
  author = {Ganz, Steven E. and Friedman, Daniel P. and Wand, Mitchell},
  date = {1999-09-01},
  series = {{{ICFP}} '99},
  pages = {18--27},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/317636.317779},
  url = {https://doi.org/10.1145/317636.317779},
  urldate = {2021-10-20},
  abstract = {A trampolined program is organized as a single loop in which computations are scheduled and their execution allowed to proceed in discrete steps. Writing programs in trampolined style supports primitives for multithreading without language support for continuations. Various forms of trampolining allow for different degrees of interaction between threads. We present two architectures based on an only mildly intrusive trampolined style. Concurrency can be supported at multiple levels of granularity by performing the trampolining transformation multiple times.},
  isbn = {978-1-58113-111-6}
}

@article{giannikisSharedWorkloadOptimization2014,
  title = {Shared Workload Optimization},
  author = {Giannikis, Georgios and Makreshanski, Darko and Alonso, Gustavo and Kossmann, Donald},
  date = {2014},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {7},
  number = {6},
  pages = {429--440},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/PAN7KQQJ/Giannikis et al. - 2014 - Shared workload optimization.pdf;/Users/cperivol/Zotero/storage/EQHJFDJL/2732279.html}
}

@article{giannikisSharedWorkloadOptimization2014a,
  title = {Shared Workload Optimization},
  author = {Giannikis, Georgios and Makreshanski, Darko and Alonso, Gustavo and Kossmann, Donald},
  date = {2014-02-01},
  journaltitle = {Proc. VLDB Endow.},
  volume = {7},
  number = {6},
  pages = {429--440},
  issn = {2150-8097},
  doi = {10.14778/2732279.2732280},
  url = {https://doi.org/10.14778/2732279.2732280},
  urldate = {2021-11-26},
  abstract = {As a result of increases in both the query load and the data managed, as well as changes in hardware architecture (multicore), the last years have seen a shift from query-at-a-time approaches towards shared work (SW) systems where queries are executed in groups. Such groups share operators like scans and joins, leading to systems that process hundreds to thousands of queries in one go. SW systems range from storage engines that use in-memory co-operative scans to more complex query processing engines that share joins over analytical and star schema queries. In all cases, they rely on either single query optimizers, predicate sharing, or on manually generated plans. In this paper we explore the problem of shared workload optimization (SWO) for SW systems. The challenge in doing so is that the optimization has to be done for the entire workload and that results in a class of stochastic knapsack with uncertain weights optimization, which can only be addressed with heuristics to achieve a reasonable runtime. In this paper we focus on hash joins and shared scans and present a first algorithm capable of optimizing the execution of entire workloads by deriving a global executing plan for all the queries in the system. We evaluate the optimizer over the TPC-W and the TPC-H benchmarks. The results prove the feasibility of this approach and demonstrate the performance gains that can be obtained from SW systems.},
  annotation = {00055},
  file = {/Users/cperivol/Zotero/storage/ZYURRZ5X/Giannikis et al. - 2014 - Shared workload optimization.pdf}
}

@article{gibbonsAppreciatedUnfold1998,
  title = {The {{Under}}\&\#x2212;{{Appreciated Unfold}}},
  author = {Gibbons, Jeremy},
  date = {1998},
  url = {http://www.cs.ox.ac.uk/publications/publication2351-abstract.html},
  urldate = {2021-10-21},
  abstract = {{$<$}em{$>$}Folds{$<$}/em{$>$} are appreciated by functional programmers. Their dual, {$<$}em{$>$}unfolds{$<$}/em{$>$}, are not new, but they are not nearly as well appreciated. We believe they deserve better. To illustrate, we present (indeed, we calculate) a number of algorithms for computing the {$<$}em{$>$}breadth-first traversal{$<$}/em{$>$} of a tree. We specify breadth-first traversal in terms of {$<$}em{$>$}level-order traversal{$<$}/em{$>$}, which we characterize first as a fold. The presentation as a fold is simple, but it is inefficient, and removing the inefficiency makes it no longer a fold. We calculate a characterization as an unfold from the characterization as a fold; this unfold is equally clear, but more efficient. We also calculate a characterization of breadth-first traversal directly as an unfold; this turns out to be the `standard' queue-based algorithm.},
  langid = {british},
  file = {/Users/cperivol/Zotero/storage/R9YDJVX2/publication2351-abstract.html}
}

@article{gjengsetNoriaDynamicPartiallystateful,
  title = {Noria: Dynamic, Partially-Stateful Data-Flow for High-Performance Web Applications},
  author = {Gjengset, Jon and Schwarzkopf, Malte and Ek, Martin and Kohler, Eddie and Kaashoek, M Frans and Morris, Robert},
  pages = {20},
  abstract = {We introduce partially-stateful data-flow, a new streaming data-flow model that supports eviction and reconstruction of data-flow state on demand. By avoiding state explosion and supporting live changes to the data-flow graph, this model makes data-flow viable for building long-lived, low-latency applications, such as web applications. Our implementation, Noria, simplifies the backend infrastructure for read-heavy web applications while improving their performance.},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/NPAJVQ5N/Gjengset et al. - Noria dynamic, partially-stateful data-ﬂow for hi.pdf}
}

@inproceedings{gjengsetNoriaDynamicPartiallystateful2018,
  title = {Noria: Dynamic, Partially-Stateful Data-Flow for High-Performance Web Applications},
  shorttitle = {Noria},
  booktitle = {13th \$\{\$\vphantom\}{{USENIX}}\$\vphantom\{\}\$ {{Symposium}} on {{Operating Systems Design}} and {{Implementation}} (\$\{\$\vphantom\}{{OSDI}}\$\vphantom\{\}\$ 18)},
  author = {Gjengset, Jon and Schwarzkopf, Malte and Behrens, Jonathan and Araújo, Lara Timbó and Ek, Martin and Kohler, Eddie and Kaashoek, M. Frans and Morris, Robert},
  date = {2018},
  pages = {213--231},
  file = {/Users/cperivol/Zotero/storage/DIDGDN8E/Gjengset et al. - 2018 - Noria dynamic, partially-stateful data-flow for h.pdf;/Users/cperivol/Zotero/storage/JSPXPHBH/gjengset.html}
}

@inproceedings{gluchePhysicalDesignOODBMS1996,
  title = {Physical Design in {{OODBMS}}},
  booktitle = {Grundlagen von {{Datenbanken}}},
  author = {Gluche, Dieter and Scholl, Marc H.},
  date = {1996},
  pages = {21--25},
  file = {/Users/cperivol/Zotero/storage/XQMMBC3V/Gluche and Scholl - 1996 - Physical design in OODBMS.pdf}
}

@inproceedings{gosainSystematicReviewMaterialized2017,
  title = {A {{Systematic Review}} on {{Materialized View Selection}}},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Frontiers}} in {{Intelligent Computing}}: Theory and {{Applications}}},
  author = {Gosain, Anjana and Sachdeva, Kavita},
  editor = {Satapathy, Suresh Chandra and Bhateja, Vikrant and Udgata, Siba K. and Pattnaik, Prasant Kumar},
  date = {2017},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {663--671},
  publisher = {{Springer}},
  location = {{Singapore}},
  doi = {10.1007/978-981-10-3153-3_66},
  abstract = {The purpose of materialized view selection is to minimize the cost of answering queries and fast query response time for timely access to information and decision support. Besides various research issues related to data warehouse evolution, materialized view selection is one of the most challenging ones. Various authors have given different methodologies, strategies and followed algorithms to solve this problem in an efficient manner. The main motivation behind this systematic review is to provide a path for future research scope in materialized view selection. Various techniques presented in the papers are identified, evaluated, and compared in terms of memory storage space, cost, and query processing time to find if any particular approach is superior to others. By means of a review of the available literature, the authors have drawn several conclusions about the status quo of materialized view selection and a future outlook is predicted on bridging the large gaps that were found in the existing methods.},
  isbn = {978-981-10-3153-3},
  langid = {english},
  keywords = {Business intelligence,Caching,Data warehouse,Data warehouse evolution,Query optimization,View materialization,View selection},
  file = {/Users/cperivol/Zotero/storage/6P85AGIE/Gosain and Sachdeva - 2017 - A Systematic Review on Materialized View Selection.pdf}
}

@article{gouSupSearchEfficient2006,
  title = {A/Sup */ Search: An Efficient and Flexible Approach to Materialized View Selection},
  shorttitle = {A/Sup */ Search},
  author = {Gou, Gang and Yu, J. X. and Lu, Hongjun},
  date = {2006},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {3},
  number = {36},
  pages = {411--425},
  issn = {1094-6977, 1558-2442},
  doi = {10.1109/TSMCC.2004.843248},
  url = {https://www.infona.pl//resource/bwmeta1.element.ieee-art-000001629205},
  urldate = {2021-10-16},
  abstract = {Decision support systems issue a large number of online analytical processing (OLAP) queries to access very large databases. A data warehouse needs to precompute or materialize some of such OLAP queries in order to improve the system throughput, since many coming queries can benefit greatly from these materialized views. Materialized view selection with resource constraint is one of the most important issues in the management of data warehouses. It addresses how to fully utilize the limited resource, disk space, or maintenance time to minimize the total query processing cost. This paper revisits the problem of materialized view selection under a disk-space constraint S. Many efficient greedy algorithms have been developed to address this problem. The quality of greedy solutions is guaranteed by a lower bound. However, it is observed that, when S is small, this lower bound can be very small and even be negative. In such cases, their solution quality will not be guaranteed well. In order to improve further the solution quality in such cases, a new competitive A\&lt;sup\&gt;*\&lt;/sup\&gt; algorithm is proposed. It is shown that it is just the distinctive topological structure of the dependent lattice that makes the A\&lt;sup\&gt;*\&lt;/sup\&gt; search a very competitive strategy for this problem. Both theoretical and experimental results show that the proposed algorithm is a powerful, efficient, and flexible approach to this problem},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/NZP6SDLY/bwmeta1.element.html}
}

@article{graefeCascadesFrameworkQuery1995,
  title = {The Cascades Framework for Query Optimization},
  author = {Graefe, Goetz},
  date = {1995},
  journaltitle = {IEEE Data Eng. Bull.},
  volume = {18},
  number = {3},
  pages = {19--29},
  file = {/Users/cperivol/Zotero/storage/9CYSSET2/Graefe - 1995 - The cascades framework for query optimization.pdf}
}

@article{graefeDynamicQueryEvaluation1989,
  title = {Dynamic Query Evaluation Plans},
  author = {Graefe, G. and Ward, K.},
  date = {1989-06-01},
  journaltitle = {SIGMOD Rec.},
  volume = {18},
  number = {2},
  pages = {358--366},
  issn = {0163-5808},
  doi = {10.1145/66926.66960},
  url = {https://doi.org/10.1145/66926.66960},
  urldate = {2021-11-25},
  abstract = {In most database systems, a query embedded in a program written in a conventional programming language is optimized when the program is compiled. The query optimizer must make assumptions about the values of the program variables that appear as constants in the query, the resources that can be committed to query evaluation, and the data in the database. The optimality of the resulting query evaluation plan depends on the validity of these assumptions. If a query evaluation plan is used repeatedly over an extended period of time, it is important to determine when reoptimization is necessary. Our work aims at developing criteria when reoptimization is required, how these criteria can be implemented efficiently, and how reoptimization can be avoided by using a new technique called dynamic query evaluation plans. We experimentally demonstrate the need for dynamic plans and outline modifications to the EXODUS optimizer generator required for creating dynamic query evaluation plans.},
  file = {/Users/cperivol/Zotero/storage/IU2CTTN9/Graefe and Ward - 1989 - Dynamic query evaluation plans.pdf}
}

@inproceedings{graefeVolcanoOptimizerGenerator1993,
  title = {The {{Volcano}} Optimizer Generator: Extensibility and Efficient Search},
  shorttitle = {The {{Volcano}} Optimizer Generator},
  booktitle = {Proceedings of {{IEEE}} 9th {{International Conference}} on {{Data Engineering}}},
  author = {Graefe, G. and McKenna, W.J.},
  date = {1993-04},
  pages = {209--218},
  doi = {10.1109/ICDE.1993.344061},
  abstract = {The Volcano project, which provides efficient, extensible tools for query and request processing, particularly for object-oriented and scientific database systems, is reviewed. In particular, one of its tools, the optimizer generator, is discussed. The data model, logical algebra, physical algebra, and optimization rules are translated by the optimizer generator into optimizer source code. It is shown that, compared with the EXODUS optimizer generator prototype, the search engine of the Volcano optimizer generator is more extensible and powerful. It provides effective support for non-trivial cost models and for physical properties such as sorting order. At the same time, it is much more efficient, as it combines dynamic programming with goal-directed searching and branch-and-bound pruning. Compared with other rule-based optimization systems, it provides complete data model independence and more natural extensibility.{$<>$}},
  eventtitle = {Proceedings of {{IEEE}} 9th {{International Conference}} on {{Data Engineering}}},
  keywords = {Algebra,Costs,Data models,Database systems,Object oriented modeling,Power generation,Power system modeling,Prototypes,Search engines,Volcanoes},
  file = {/Users/cperivol/Zotero/storage/SVQ8ZI39/Graefe and McKenna - 1993 - The Volcano optimizer generator extensibility and.pdf;/Users/cperivol/Zotero/storage/ZXEAKGWE/344061.html}
}

@inproceedings{graefeVolcanoOptimizerGenerator1993a,
  title = {The {{Volcano}} Optimizer Generator: Extensibility and Efficient Search},
  shorttitle = {The {{Volcano}} Optimizer Generator},
  booktitle = {Proceedings of {{IEEE}} 9th {{International Conference}} on {{Data Engineering}}},
  author = {Graefe, G. and McKenna, W.J.},
  date = {1993-04},
  pages = {209--218},
  doi = {10.1109/ICDE.1993.344061},
  abstract = {The Volcano project, which provides efficient, extensible tools for query and request processing, particularly for object-oriented and scientific database systems, is reviewed. In particular, one of its tools, the optimizer generator, is discussed. The data model, logical algebra, physical algebra, and optimization rules are translated by the optimizer generator into optimizer source code. It is shown that, compared with the EXODUS optimizer generator prototype, the search engine of the Volcano optimizer generator is more extensible and powerful. It provides effective support for non-trivial cost models and for physical properties such as sorting order. At the same time, it is much more efficient, as it combines dynamic programming with goal-directed searching and branch-and-bound pruning. Compared with other rule-based optimization systems, it provides complete data model independence and more natural extensibility.{$<>$}},
  eventtitle = {Proceedings of {{IEEE}} 9th {{International Conference}} on {{Data Engineering}}},
  keywords = {Algebra,Costs,Data models,Database systems,Object oriented modeling,Power generation,Power system modeling,Prototypes,Search engines,Volcanoes},
  file = {/Users/cperivol/Zotero/storage/A4IZL732/Graefe and McKenna - 1993 - The Volcano optimizer generator extensibility and.pdf;/Users/cperivol/Zotero/storage/7KX24HDF/344061.html}
}

@inproceedings{grossmanVerifyingEquivalenceSpark2017,
  title = {Verifying Equivalence of Spark Programs},
  booktitle = {International {{Conference}} on {{Computer Aided Verification}}},
  author = {Grossman, Shelly and Cohen, Sara and Itzhaky, Shachar and Rinetzky, Noam and Sagiv, Mooly},
  date = {2017},
  pages = {282--300},
  publisher = {{Springer}},
  file = {/Users/cperivol/Zotero/storage/PURYSF4K/978-3-319-63390-9_15.html}
}

@inproceedings{gundaNectarAutomaticManagement2010,
  title = {Nectar: Automatic {{Management}} of {{Data}} and {{Computation}} in {{Datacenters}}.},
  shorttitle = {Nectar},
  booktitle = {{{OSDI}}},
  author = {Gunda, Pradeep Kumar and Ravindranath, Lenin and Thekkath, Chandramohan A. and Yu, Yuan and Zhuang, Li},
  date = {2010},
  volume = {10},
  pages = {1--8},
  file = {/Users/cperivol/Zotero/storage/TKJVNHS3/Gunda et al. - 2010 - Nectar Automatic Management of Data and Computati.pdf}
}

@inproceedings{guptaSelectionViewsMaterialize1997,
  title = {Selection of Views to Materialize in a Data Warehouse},
  booktitle = {Database {{Theory}} — {{ICDT}} '97},
  author = {Gupta, Himanshu},
  editor = {Afrati, Foto and Kolaitis, Phokion},
  date = {1997},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {98--112},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-62222-5_39},
  abstract = {A data warehouse stores materialized views of data from one or more sources, with the purpose of efficiently implementing decision-support or OLAP queries. One of the most important decisions in designing a data warehouse is the selection of materialized views to be maintained at the warehouse. The goal is to select an appropriate set of views that minimizes total query response time and the cost of maintaining the selected views, given a limited amount of resource, e.g., materialization time, storage space etc.In this article, we develop a theoretical framework for the general problem of selection of views in a data warehouse. We present competitive polynomial-time heuristics for selection of views to optimize total query response time, for some important special cases of the general data warehouse scenario, viz.: (i) an AND view graph, where each query/view has a unique evaluation, and (ii) an OR view graph, in which any view can be computed from any one of its related views, e.g., data cubes. We extend the algorithms to the case when there is a set of indexes associated with each view. Finally, we extend our heuristic to the most general case of AND-OR view graphs.},
  isbn = {978-3-540-49682-3},
  langid = {english},
  keywords = {Data Cube,Greedy Algorithm,Greedy Heuristic,Monotonicity Property,Outgoing Edge},
  annotation = {00000},
  file = {/Users/cperivol/Zotero/storage/5BUG8DZV/Gupta - 1997 - Selection of views to materialize in a data wareho.pdf}
}

@article{halevyAnsweringQueriesUsing2001,
  title = {Answering Queries Using Views: A Survey},
  shorttitle = {Answering Queries Using Views},
  author = {Halevy, Alon Y.},
  date = {2001-12-01},
  journaltitle = {The VLDB Journal},
  volume = {10},
  number = {4},
  pages = {270--294},
  issn = {0949-877X},
  doi = {10.1007/s007780100054},
  url = {https://doi.org/10.1007/s007780100054},
  urldate = {2021-11-26},
  abstract = {The problem of answering queries using views is to find efficient methods of answering a query using a set of previously defined materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems. In query optimization, finding a rewriting of a query using a set of materialized views can yield a more efficient query execution plan. To support the separation of the logical and physical views of data, a storage schema can be described using views over the logical schema. As a result, finding a query execution plan that accesses the storage amounts to solving the problem of answering queries using views. Finally, the problem arises in data integration systems, where data sources can be described as precomputed views over a mediated schema. This article surveys the state of the art on the problem of answering queries using views, and synthesizes the disparate works into a coherent framework. We describe the different applications of the problem, the algorithms proposed to solve it and the relevant theoretical results.},
  langid = {english},
  annotation = {00000},
  file = {/Users/cperivol/Zotero/storage/DWTBEWQV/Halevy - 2001 - Answering queries using views A survey.pdf}
}

@inproceedings{hammerAdaptonComposableDemanddriven2014,
  title = {Adapton: Composable, Demand-Driven Incremental Computation},
  shorttitle = {Adapton},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Hammer, Matthew A. and Phang, Khoo Yit and Hicks, Michael and Foster, Jeffrey S.},
  date = {2014-06-09},
  pages = {156--166},
  publisher = {{ACM}},
  location = {{Edinburgh United Kingdom}},
  doi = {10.1145/2594291.2594324},
  url = {https://dl.acm.org/doi/10.1145/2594291.2594324},
  urldate = {2021-10-09},
  abstract = {Many researchers have proposed programming languages that support incremental computation (IC), which allows programs to be efficiently re-executed after a small change to the input. However, existing implementations of such languages have two important drawbacks. First, recomputation is oblivious to specific demands on the program output; that is, if a program input changes, all dependencies will be recomputed, even if an observer no longer requires certain outputs. Second, programs are made incremental as a unit, with little or no support for reusing results outside of their original context, e.g., when reordered.},
  eventtitle = {{{PLDI}} '14: {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  isbn = {978-1-4503-2784-8},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/Z77JRC8B/Hammer et al. - 2014 - Adapton composable, demand-driven incremental com.pdf}
}

@inproceedings{hammerAdaptonComposableDemanddriven2014a,
  title = {Adapton: Composable, Demand-Driven Incremental Computation},
  shorttitle = {Adapton},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Hammer, Matthew A. and Phang, Khoo Yit and Hicks, Michael and Foster, Jeffrey S.},
  date = {2014-06-09},
  pages = {156--166},
  publisher = {{ACM}},
  location = {{Edinburgh United Kingdom}},
  doi = {10.1145/2594291.2594324},
  url = {https://dl.acm.org/doi/10.1145/2594291.2594324},
  urldate = {2021-10-09},
  abstract = {Many researchers have proposed programming languages that support incremental computation (IC), which allows programs to be efficiently re-executed after a small change to the input. However, existing implementations of such languages have two important drawbacks. First, recomputation is oblivious to specific demands on the program output; that is, if a program input changes, all dependencies will be recomputed, even if an observer no longer requires certain outputs. Second, programs are made incremental as a unit, with little or no support for reusing results outside of their original context, e.g., when reordered.},
  eventtitle = {{{PLDI}} '14: {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  isbn = {978-1-4503-2784-8},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/QCII9H5R/Hammer et al. - 2014 - Adapton composable, demand-driven incremental com.pdf}
}

@book{hansonSoftwareDesignFlexibility2021,
  title = {Software Design for Flexibility: How to Avoid Programming Yourself into a Corner},
  shorttitle = {Software Design for Flexibility},
  author = {Hanson, Chris and Sussman, Gerald Jay},
  date = {2021},
  publisher = {{The MIT Press}},
  location = {{Cambridge}},
  abstract = {"An advanced book on programming techniques to build flexible, robust, symbolic systems"--},
  isbn = {978-0-262-04549-0},
  langid = {english},
  pagetotal = {424},
  keywords = {Software architecture,Software patterns},
  file = {/Users/cperivol/Zotero/storage/5KC3TFXS/Hanson and Sussman - 2021 - Software design for flexibility how to avoid prog.pdf}
}

@book{hansonSoftwareDesignFlexibility2021a,
  title = {Software {{Design}} for {{Flexibility}}: How to {{Avoid Programming Yourself}} into a {{Corner}}},
  shorttitle = {Software {{Design}} for {{Flexibility}}},
  author = {Hanson, Chris and Sussman, Gerald Jay},
  date = {2021-03-09},
  eprint = {iM_tDwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{MIT Press}},
  abstract = {Strategies for building large systems that can be easily adapted for new situations with only minor programming modifications.Time pressures encourage programmers to write code that works well for a narrow purpose, with no room to grow. But the best systems are evolvable; they can be adapted for new situations by adding code, rather than changing the existing code. The authors describe techniques they have found effective--over their combined 100-plus years of programming experience--that will help programmers avoid programming themselves into corners.The authors explore ways to enhance flexibility by: • Organizing systems using combinators to compose mix-and-match parts, ranging from small functions to whole arithmetics, with standardized interfaces • Augmenting data with independent annotation layers, such as units of measurement or provenance • Combining independent pieces of partial information using unification or propagation • Separating control structure from problem domain with domain models, rule systems and pattern matching, propagation, and dependency-directed backtracking • Extending the programming language, using dynamically extensible evaluators},
  isbn = {978-0-262-36247-4},
  langid = {english},
  pagetotal = {449},
  keywords = {Computers / Computer Science,Computers / Programming / General,Computers / Software Development & Engineering / Systems Analysis & Design}
}

@incollection{harizopoulosOLTPLookingGlass2018,
  title = {{{OLTP}} through the Looking Glass, and What We Found There},
  booktitle = {Making {{Databases Work}}: The {{Pragmatic Wisdom}} of {{Michael Stonebraker}}},
  author = {Harizopoulos, Stavros and Abadi, Daniel J. and Madden, Samuel and Stonebraker, Michael},
  date = {2018},
  pages = {409--439},
  file = {/Users/cperivol/Zotero/storage/P7484JHD/3226595.html}
}

@article{haynesLogicContinuations1987,
  title = {Logic Continuations},
  author = {Haynes, Christopher T.},
  date = {1987-06-01},
  journaltitle = {The Journal of Logic Programming},
  volume = {4},
  number = {2},
  pages = {157--176},
  issn = {0743-1066},
  doi = {10.1016/0743-1066(87)90016-1},
  url = {https://www.sciencedirect.com/science/article/pii/0743106687900161},
  urldate = {2021-10-21},
  abstract = {We develop a “complete” embedding of logic programming into scheme—a lexically scoped lisp dialect with first-class continuations. Logic variables are bound in the scheme environment, and the success and failure continuations are represented as scheme continuations. To account for the semantics of logic variables and failure continuations, the state-space model of control is modified in a novel way that generalizes the trail mechanism. This ensures that logic variable bindings are properly restored when continuations are invoked to perform “lateral” control transfers that are not possible in a traditional logic programming context. It is thereby possible to obtain greater control flexibility while allowing much of a program to be expressed with logic programming.},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/VQ79N9F8/Haynes - 1987 - Logic continuations.pdf;/Users/cperivol/Zotero/storage/PUTGQZC9/0743106687900161.html}
}

@incollection{hermanTheoryHygienicMacros2008,
  title = {A {{Theory}} of {{Hygienic Macros}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Herman, David and Wand, Mitchell},
  editor = {Drossopoulou, Sophia},
  date = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {4960},
  pages = {48--62},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-78739-6_4},
  url = {http://link.springer.com/10.1007/978-3-540-78739-6_4},
  urldate = {2021-05-25},
  abstract = {Hygienic macro systems, such as Scheme’s, automatically rename variables to prevent unintentional variable capture—in short, they “just work.” Yet hygiene has never been formally presented as a specification rather than an algorithm. According to folklore, the definition of hygienic macro expansion hinges on the preservation of alpha-equivalence. But the only known notion of alpha-equivalence for programs with macros depends on the results of macro expansion! We break this circularity by introducing explicit binding specifications into the syntax of macro definitions, permitting a definition of alpha-equivalence independent of expansion. We define a semantics for a first-order subset of Scheme-like macros and prove hygiene as a consequence of confluence.},
  isbn = {978-3-540-78738-9 978-3-540-78739-6},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/JGWDGXGV/Herman and Wand - 2008 - A Theory of Hygienic Macros.pdf}
}

@article{herodotouXplusSqltuningawareQuery2010,
  title = {Xplus: A Sql-Tuning-Aware Query Optimizer},
  shorttitle = {Xplus},
  author = {Herodotou, Herodotos and Babu, Shivnath},
  date = {2010},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {3},
  number = {1-2},
  pages = {1149--1160},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/LK3FE6HZ/Herodotou and Babu - 2010 - Xplus a sql-tuning-aware query optimizer.pdf;/Users/cperivol/Zotero/storage/D67SZFUP/1920841.html}
}

@article{hinzeDerivingBacktrackingMonad2000,
  title = {Deriving Backtracking Monad Transformers},
  author = {Hinze, Ralf},
  date = {2000-09-01},
  journaltitle = {SIGPLAN Not.},
  volume = {35},
  number = {9},
  pages = {186--197},
  issn = {0362-1340},
  doi = {10.1145/357766.351258},
  url = {https://doi.org/10.1145/357766.351258},
  urldate = {2021-10-21},
  abstract = {In a paper about pretty printing J. Hughes introduced two fundamental techniques for deriving programs from their specification, where a specification consists of a signature and properties that the operations of the signature are required to satisfy. Briefly, the first technique, the term implementation, represents the operations by terms and works by defining a mapping from operations to observations --- this mapping can be seen as defining a simple interpreter. The second, the context-passing implementation, represents operations as functions from their calling context to observations. We apply both techniques to derive a backtracking monad transformer that adds backtracking to an arbitrary monad. In addition to the usual backtracking operations --- failure and nondeterministic choice --- the prolog cut and an operation for delimiting the effect of a cut are supported.},
  keywords = {backtracking,continuations,cut,Haskell,monad transformers,monads,program derivation,Prolog}
}

@article{hinzeDerivingBacktrackingMonad2000a,
  title = {Deriving {{Backtracking Monad Transformers}}},
  author = {Hinze, Ralf},
  date = {2000-08-15},
  journaltitle = {ACM SIGPLAN Notices},
  volume = {35},
  doi = {10.1145/357766.351258},
  abstract = {In a paper about pretty printing J. Hughes introduced two fundamental techniques for deriving programs from their specication, where a specication consists of a signature and properties that the operations of the signature are required to satisfy. Briey, the rst technique, the term implementation, represents the operations by terms and works by dening a mapping from operations to observations | this mapping can be seen as dening a simple interpreter. The second, the context-passing implementation, represents operations as functions from their calling context to observations. We apply both techniques to derive a backtracking monad transformer that adds backtracking to an arbitrary monad. In addition to the usual backtracking operations | failure and nondeterministic choice | the prolog cut and an operation for delimiting the eect of a cut are supported. Categories and Subject Descriptors D.1.1 [Programming Techniques]: Applicative (Functional) Programming; D.3.2 [Programming La...},
  file = {/Users/cperivol/Zotero/storage/U2W9SEBV/Hinze - 2000 - Deriving Backtracking Monad Transformers.pdf}
}

@article{hinzeEfficientMonadicstyleBacktracking,
  title = {Efficient Monadic-Style Backtracking},
  author = {Hinze, Ralf},
  url = {https://www.academia.edu/2957758/Efficient_monadic_style_backtracking},
  urldate = {2021-10-21},
  abstract = {Abstract Lists are ubiquitous in functional programming. The list constructor forms an instance of a monad capturing non-deterministic computations. Despite its popularity the list monad suffers from serious drawbacks: It relies in an essential way},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/AGE5NM3N/Efficient_monadic_style_backtracking.html}
}

@article{hornIncrementalRelationalLenses2018,
  title = {Incremental {{Relational Lenses}}},
  author = {Horn, Rudi and Perera, Roly and Cheney, James},
  date = {2018-09},
  journaltitle = {P. ACM Program. Lang.},
  volume = {2},
  pages = {74},
  publisher = {{Assoc Computing Machinery}},
  location = {{New York}},
  doi = {10.1145/3236769},
  url = {https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcAuth=DOISource&SrcApp=WOS&KeyAID=10.1145%2F3236769&DestApp=DOI&SrcAppSID=C5OwjYx6BlMIGF1fuH9&SrcJTitle=PROCEEDINGS+OF+THE+ACM+ON+PROGRAMMING+LANGUAGES-PACMPL&DestDOIRegistrantName=Association+for+Computing+Machinery},
  urldate = {2021-10-09},
  abstract = {Lenses are a popular approach to bidirectional transformations, a generalisation of the view update problem in databases, in which we wish to make changes to source tables to effect a desired change on a view. However, perhaps surprisingly, lenses have seldom actually been used to implement updatable views in databases. Bohannon, Pierce and Vaughan proposed an approach to updatable views called relational lenses, but to the best of our knowledge this proposal has not been implemented or evaluated to date. We propose Incremental relational lenses, that equip relational lenses with change-propagating semantics that map small changes to the view to (potentially) small changes to the source tables. We also present a language-integrated implementation of relational lenses and a detailed experimental evaluation, showing orders of magnitude improvement over the non-incremental approach. Our work shows that relational lenses can be used to support expressive and efficient view updates at the language level, without relying on updatable view support from the underlying database.},
  langid = {english},
  keywords = {bidirectional transformations,incremental   computation,lenses,model,relational calculus},
  annotation = {WOS:000688018100008},
  file = {/Users/cperivol/Zotero/storage/T7YWJJ5V/Horn et al. - 2018 - Incremental Relational Lenses.pdf}
}

@inproceedings{houshmandTCEExtensionTce2017,
  title = {{{TCE}}+: An Extension of the Tce Method for Detecting Equivalent Mutants in Java Programs},
  shorttitle = {{{TCE}}+},
  booktitle = {International {{Conference}} on {{Fundamentals}} of {{Software Engineering}}},
  author = {Houshmand, Mahdi and Paydar, Samad},
  date = {2017},
  pages = {164--179},
  publisher = {{Springer}},
  file = {/Users/cperivol/Zotero/storage/XBY9P98L/Houshmand and Paydar - 2017 - TCE+ An extension of the tce method for detecting.pdf;/Users/cperivol/Zotero/storage/Q8WVVXJK/978-3-319-68972-2_11.html}
}

@article{howardFunctionalInterpretationBar,
  title = {Functional Interpretation of Bar Induction by Bar Recursion},
  author = {Howard, W A},
  pages = {19},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/9XHJ7FJ8/Howard - Functional interpretation of bar induction by bar .pdf}
}

@misc{huaiDeepDiveSpark,
  title = {A {{Deep Dive}} into {{Spark SQL}}’s {{Catalyst Optimizer}}},
  author = {Huai, Y.}
}

@article{huetZipper1997,
  title = {The {{Zipper}}},
  author = {Huet, Gérard},
  date = {1997-09},
  journaltitle = {Journal of Functional Programming},
  volume = {7},
  number = {5},
  pages = {549--554},
  publisher = {{Cambridge University Press}},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796897002864},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/abs/zipper/0C058890B8A9B588F26E6D68CF0CE204},
  urldate = {2021-10-10},
  abstract = {Almost every programmer has faced the problem of  representing a tree together with a  subtree that is the focus of attention, where that focus may  move left, right, up or down the tree. The Zipper is Huet's nifty name for a nifty data structure which fulfills this need. I wish I had known of it when I faced this task, because the solution  I came up with was not quite so efficient or elegant as the Zipper.},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/EXXQERIV/0C058890B8A9B588F26E6D68CF0CE204.html}
}

@inproceedings{hughesProgrammingArrows2005,
  title = {Programming with {{Arrows}}},
  booktitle = {Advanced {{Functional Programming}}},
  author = {Hughes, John},
  editor = {Vene, Varmo and Uustalu, Tarmo},
  date = {2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {73--129},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11546382_2},
  abstract = {Consider this simple Haskell definition, of a function which counts the number of occurrences of a given word w in a string: count w = length . filter (==w) . words This is an example of “point-free” programming style, where we build a function by composing others, and make heavy use of higher-order functions such as filter. Point-free programming is rightly popular: used appropriately, it makes for concise and readable definitions, which are well suited to equational reasoning in the style of Bird and Meertens [2]. It’s also a natural way to assemble programs from components, and closely related to connecting programs via pipes in the UNIX shell.},
  isbn = {978-3-540-31872-9},
  langid = {english},
  keywords = {Full Adder,Input Event,Loop Body,Output Event,Stream Function},
  file = {/Users/cperivol/Zotero/storage/R2MNJ3DS/Hughes - 2005 - Programming with Arrows.pdf}
}

@inproceedings{idreosDatabaseCracking2007,
  title = {Database {{Cracking}}.},
  booktitle = {{{CIDR}}},
  author = {Idreos, Stratos and Kersten, Martin L. and Manegold, Stefan},
  date = {2007},
  volume = {7},
  pages = {68--78},
  file = {/Users/cperivol/Zotero/storage/7NXADTI8/Idreos et al. - 2007 - Database Cracking..pdf}
}

@online{IEEEXploreFullText,
  title = {{{IEEE Xplore Full}}-{{Text PDF}}:},
  url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5447892},
  urldate = {2021-05-25},
  file = {/Users/cperivol/Zotero/storage/M3HEVT68/stamp.html}
}

@online{ImplementingDataCubes,
  title = {Implementing Data Cubes Efficiently | {{Proceedings}} of the 1996 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  url = {https://dlnext.acm.org/doi/10.1145/233269.233333},
  urldate = {2021-11-26},
  annotation = {00000},
  file = {/Users/cperivol/Zotero/storage/8M5WIM9D/233269.html}
}

@online{IncoopProceedings2nd,
  title = {Incoop | {{Proceedings}} of the 2nd {{ACM Symposium}} on {{Cloud Computing}}},
  url = {https://dl.acm.org/doi/abs/10.1145/2038916.2038923?casa_token=7pHAhbVH-ecAAAAA:VjrHz7ye0hAXgyshbE0gjeSJFGvJDgjbyHn1XOoINOWgagzAgltU_gNxj3EdEZMpRH_DuZMFXQx-XA},
  urldate = {2021-10-09},
  file = {/Users/cperivol/Zotero/storage/XXJJ54HT/2038916.html}
}

@online{IncrementalComputationNames,
  title = {Incremental Computation with Names | {{Proceedings}} of the 2015 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  url = {https://dl.acm.org/doi/abs/10.1145/2814270.2814305},
  urldate = {2021-10-09}
}

@misc{IntelItaniumProcessor2001,
  title = {Intel ® {{Itanium}} ™ {{Processor}}- Specific {{Application Binary Interface}} ({{ABI}})},
  date = {2001},
  abstract = {Document Number: 245370-003 Information in this document is provided in connection with Intel ® products. No license, express or implied, by estoppel or otherwise, to any intellectual property rights is granted by this document. Except as provided in Intel's Terms and Conditions of Sale for such products, Intel assumes no liability whatsoever, and Intel disclaims any express or implied warranty, relating to sale and/or use of Intel products including liability or warranties relating to fitness for a particular purpose, merchantability, or infringement of any patent, copyright or other intellectual property right. Intel products are not intended for use in medical, life saving, or life sustaining applications. Intel may make changes to specifications and product descriptions at any time, without notice. Designers must not rely on the absence or characteristics of any features or instructions marked “reserved ” or “undefined. ” Intel reserves these for future definition and shall have no responsibility whatsoever for conflicts or incompatibilities arising from future changes to them. The Itanium processor may contain design defects or errors known as errata which may cause the product to deviate from published specifications. Current characterized errata are available on request. Contact your local Intel sales office or your distributor to obtain the latest specifications and before placing your product order. Copies of documents which have an order number and are referenced in this document, or other Intel literature, may be obtained by calling 1-800-548-4725, or by visiting Intel’s website at},
  file = {/Users/cperivol/Zotero/storage/PPKVQE6V/2001 - Intel ® Itanium ™ Processor- specific Application .pdf;/Users/cperivol/Zotero/storage/SDYPSXMW/summary.html}
}

@online{itzhakyAutomatedVerificationWeb2016,
  title = {On the Automated Verification of Web Applications with Embedded {{SQL}}},
  author = {Itzhaky, Shachar and Kotek, Tomer and Rinetzky, Noam and Sagiv, Mooly and Tamir, Orr and Veith, Helmut and Zuleger, Florian},
  date = {2016},
  eprint = {1610.02101},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{ivanovaArchitectureRecyclingIntermediates2010,
  title = {An Architecture for Recycling Intermediates in a Column-Store},
  author = {Ivanova, Milena G. and Kersten, Martin L. and Nes, Niels J. and Gonçalves, Romulo AP},
  date = {2010},
  journaltitle = {ACM Transactions on Database Systems (TODS)},
  volume = {35},
  number = {4},
  pages = {1--43},
  publisher = {{ACM New York, NY, USA}},
  file = {/Users/cperivol/Zotero/storage/YWTD745I/Ivanova et al. - 2010 - An architecture for recycling intermediates in a c.pdf;/Users/cperivol/Zotero/storage/A3LILBAY/1862919.html}
}

@inproceedings{jindalComputationReuseAnalytics2018,
  title = {Computation Reuse in Analytics Job Service at Microsoft},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Jindal, Alekh and Qiao, Shi and Patel, Hiren and Yin, Zhicheng and Di, Jieming and Bag, Malay and Friedman, Marc and Lin, Yifung and Karanasos, Konstantinos and Rao, Sriram},
  date = {2018},
  pages = {191--203},
  file = {/Users/cperivol/Zotero/storage/L423862P/Jindal et al. - 2018 - Computation reuse in analytics job service at micr.pdf;/Users/cperivol/Zotero/storage/27Z5IDM8/3183713.html}
}

@inproceedings{jindalPeregrineWorkloadOptimization2019,
  title = {Peregrine: Workload Optimization for Cloud Query Engines},
  shorttitle = {Peregrine},
  booktitle = {Proceedings of the {{ACM Symposium}} on {{Cloud Computing}}},
  author = {Jindal, Alekh and Patel, Hiren and Roy, Abhishek and Qiao, Shi and Yin, Zhicheng and Sen, Rathijit and Krishnan, Subru},
  date = {2019},
  pages = {416--427},
  file = {/Users/cperivol/Zotero/storage/E5M4FXLY/Jindal et al. - 2019 - Peregrine Workload optimization for cloud query e.pdf;/Users/cperivol/Zotero/storage/MW9B69YZ/3357223.html}
}

@article{jindalSelectingSubexpressionsMaterialize2018,
  title = {Selecting {{Subexpressions}} to {{Materialize}} at {{Datacenter Scale}}},
  author = {Jindal, Alekh and Karanasos, Konstantinos and Rao, Sriram and Patel, Hiren},
  date = {2018},
  journaltitle = {undefined},
  url = {https://www.semanticscholar.org/paper/Selecting-Subexpressions-to-Materialize-at-Scale-Jindal-Karanasos/bce89c6fb5cbafc17d88aef584a8d3c4696a1ca5},
  urldate = {2021-11-26},
  abstract = {The problem of subexpression selection for large workloads, i.e., selecting common parts of job plans and materializing them to speed-up the evaluation of subsequent jobs is focused on and BigSubs, a vertex-centric graph algorithm is introduced to iteratively choose in parallel which subexpressions to materialize and which sub expressions to use for evaluating each job. We observe significant overlaps in the computations performed by user jobs in modern shared analytics clusters. Naively computing the same subexpressions multiple times results in wasting cluster resources and longer execution times. Given that these shared cluster workloads consist of tens of thousands of jobs, identifying overlapping computations across jobs is of great interest to both cluster operators and users. Nevertheless, existing approaches support orders of magnitude smaller workloads or employ heuristics with limited effectiveness.    In this paper, we focus on the problem of subexpression selection for large workloads, i.e., selecting common parts of job plans and materializing them to speed-up the evaluation of subsequent jobs. We provide an ILP-based formulation of our problem and map it to a bipartite graph labeling problem. Then, we introduce BigSubs, a vertex-centric graph algorithm to iteratively choose in parallel which subexpressions to materialize and which subexpressions to use for evaluating each job. We provide a distributed implementation of our approach using our internal SQL-like execution framework, SCOPE, and assess its effectiveness over production workloads. BigSubs supports workloads with tens of thousands of jobs, yielding savings of up to 40\% in machine-hours. We are currently integrating our techniques with the SCOPE runtime in our production clusters.},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/HQTJSMZD/bce89c6fb5cbafc17d88aef584a8d3c4696a1ca5.html}
}

@article{jindalSelectingSubexpressionsMaterialize2018a,
  title = {Selecting Subexpressions to Materialize at Datacenter Scale},
  author = {Jindal, Alekh and Karanasos, Konstantinos and Rao, Sriram and Patel, Hiren},
  date = {2018},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {11},
  number = {7},
  pages = {800--812},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/NMLFN82C/Jindal et al. - 2018 - Selecting subexpressions to materialize at datacen.pdf;/Users/cperivol/Zotero/storage/X9Y4LB76/3192965.html}
}

@inproceedings{jindalWWHowFreeingData2013,
  title = {{{WWHow}}! {{Freeing Data Storage}} from {{Cages}}.},
  booktitle = {{{CIDR}}},
  author = {Jindal, Alekh and Quiané-Ruiz, Jorge-Arnulfo and Dittrich, Jens},
  date = {2013},
  publisher = {{Citeseer}},
  file = {/Users/cperivol/Zotero/storage/PQ4E4I6J/Jindal et al. - 2013 - WWHow! Freeing Data Storage from Cages..pdf}
}

@article{jovanovicIncrementalConsolidationDataintensive2016,
  title = {Incremental Consolidation of Data-Intensive Multi-Flows},
  author = {Jovanovic, Petar and Romero, Oscar and Simitsis, Alkis and Abelló, Alberto},
  date = {2016},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {28},
  number = {5},
  pages = {1203--1216},
  publisher = {{IEEE}},
  file = {/Users/cperivol/Zotero/storage/GGKHK5HC/Jovanovic et al. - 2016 - Incremental consolidation of data-intensive multi-.pdf;/Users/cperivol/Zotero/storage/UR5P3763/7374708.html}
}

@article{jrConnectionMachineeLisv,
  title = {Connection {{Machinee Lisv}}: Pine-{{Grained Parallel Symbolic Processing}}},
  author = {Jr, Guy L Steele and Hillis, W Daniel},
  pages = {19},
  abstract = {Connection Machine Lisp is a dialect of Lisp extended to allow a fine.grained, data-oriented style of parallel execution. We introduce a new data structure, the xapping, that is like a sparse array whose elements can be processed in parallel. This kind of processing is suitable for implementation by such fine.grained parallel computers as the Connection Machine System and NONVON. Additional program notation is introduced to indicate various parallel operations. The symbols st and • are used, in a manner syntactically reminiscent of the backquote notation used in Common Lisp, to indicate what parts of an expression are to be executed in parallel. Ths symbol fl is used to indicate permutation and reduction of sets of data.},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/6TIFGCH9/Jr and Hillis - Connection Machinee Lisv Pine-Grained Parallel Sy.pdf}
}

@article{jrEVOLUTIONLISP,
  title = {{{THE EVOLUTION OF LISP}}},
  author = {Jr, Guy L Steele and Gabriel, Richard R},
  pages = {98},
  abstract = {Lisp is the world's greatest programming language---or so its proponents think. The structure of Lisp makes it easy to extend the language or even to implement entirely new dialects without starting from scratch. Overall, the evolution of Lisp has been guided more by institutional rivalry, one-upsmanship, and the glee born of technical cleverness that is characteristic of the "hacker culture" than by sober assessments of technical requirements. Nevertheless, this process has eventually produced both an industrial-strength programming language, messy but powerful, and a technically pure dialect, small but powerful, that is suitable for use by programming-language theoreticians. We pick up where McCarthy's paper in the first HOPL conference left off: We trace the development chronologically from the era of the PDP-6, through the heyday of lnterlisp and MacLisp, past the ascension and decline of special purpose Lisp machines, to the present era of standardization activities. We then examine the technical evolution of a few representative language features, including some notable successes and failures that illuminate design issues that distinguish Lisp from other programming languages. We also discuss the use of Lisp as a laboratory for designing other programming languages. We conclude with some reflections on the forces that have driven the evolution of Lisp.},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/77J46T7E/Jr and Gabriel - THE EVOLUTION OF LISP.pdf}
}

@inproceedings{kalavriM2r2FrameworkResults2013,
  title = {M2r2: A Framework for Results Materialization and Reuse in High-Level Dataflow Systems for Big Data},
  shorttitle = {M2r2},
  booktitle = {2013 {{IEEE}} 16th {{International Conference}} on {{Computational Science}} and {{Engineering}}},
  author = {Kalavri, Vasiliki and Shang, Hui and Vlassov, Vladimir},
  date = {2013},
  pages = {894--901},
  publisher = {{IEEE}},
  file = {/Users/cperivol/Zotero/storage/DIPYEQAT/6755314.html}
}

@thesis{karanasosDeltaScalableData2013,
  type = {report},
  title = {Delta: Scalable {{Data Dissemination}} under {{Capacity Constraints}}},
  shorttitle = {Delta},
  author = {Karanasos, Konstantinos and Katsifodimos, Asterios and Manolescu, Ioana},
  date = {2013-10-29},
  pages = {37},
  institution = {{INRIA}},
  url = {https://hal.inria.fr/hal-00877758},
  urldate = {2021-11-26},
  abstract = {In content-based publish-subscribe (pub/sub) systems, users express their interests as queries over a stream of publications. Scaling up content-based pub/sub to very large numbers of subscriptions is challenging: users are interested in low \{\textbackslash em latency\}, that is, getting subscription results fast, while the pub/sub system provider is mostly interested in \{\textbackslash em scaling\}, \textbackslash ie being able to serve large numbers of subscribers, with low \{\textbackslash em computational resources utilization\}. We present a novel approach for scalable content-based pub/sub in the presence of constraints on the available CPU and network resources, implemented within our pub/sub system Delta. We achieve scalability by off-loading some subscriptions from the pub/sub \textbackslash linebreak server, and leveraging view-based query rewriting to feed these subscriptions from the data accumulated in others. Our main contribution is a novel algorithm for organizing views in a multi-level dissemination network, exploiting view-based rewriting and powerful linear programming capabilities to scale to many views, respect capacity constraints, and minimize latency. The efficiency and effectiveness of our algorithm are confirmed through extensive experiments and a large deployment in a WAN.},
  langid = {english},
  annotation = {00017},
  file = {/Users/cperivol/Zotero/storage/MVGN4PEW/Karanasos et al. - 2013 - Delta Scalable Data Dissemination under Capacity .pdf;/Users/cperivol/Zotero/storage/GP9NFCD7/hal-00877758.html}
}

@inproceedings{kathuriaEfficientProvableMultiquery2017,
  title = {Efficient and Provable Multi-Query Optimization},
  booktitle = {Proceedings of the 36th {{ACM SIGMOD}}-{{SIGACT}}-{{SIGAI Symposium}} on {{Principles}} of {{Database Systems}}},
  author = {Kathuria, Tarun and Sudarshan, S.},
  date = {2017},
  pages = {53--67},
  file = {/Users/cperivol/Zotero/storage/ZT7XG9BB/Kathuria and Sudarshan - 2017 - Efficient and provable multi-query optimization.pdf;/Users/cperivol/Zotero/storage/B8W3FDUW/3034786.html}
}

@article{keidelSoundReusableComponents2019,
  title = {Sound and Reusable Components for Abstract Interpretation},
  author = {Keidel, Sven and Erdweg, Sebastian},
  date = {2019-10-10},
  journaltitle = {Proc. ACM Program. Lang.},
  volume = {3},
  pages = {1--28},
  issn = {2475-1421},
  doi = {10.1145/3360602},
  url = {https://dl.acm.org/doi/10.1145/3360602},
  urldate = {2021-10-10},
  abstract = {Abstract interpretation is a methodology for defining sound static analysis. Yet, building sound static analyses for modern programming languages is difficult, because these static analyses need to combine sophisticated abstractions for values, environments, stores, etc. However, static analyses often tightly couple these abstractions in the implementation, which not only complicates the implementation, but also makes it hard to decide which parts of the analyses can be proven sound independently from each other. Furthermore, this coupling makes it hard to combine soundness lemmas for parts of the analysis to a soundness proof of the complete analysis.                            To solve this problem, we propose to construct static analyses modularly from               reusable analysis components               . Each analysis component encapsulates a single analysis concern and can be proven sound independently from the analysis where it is used. We base the design of our analysis components on               arrow transformers               , which allows us to compose analysis components. This composition preserves soundness, which guarantees that a static analysis is sound, if all its analysis components are sound. This means that analysis developers do not have to worry about soundness as long as they reuse sound analysis components. To evaluate our approach, we developed a library of 13 reusable analysis components in Haskell. We use these components to define a               k               -CFA analysis for PCF and an interval and reaching definition analysis for a While language.},
  issue = {OOPSLA},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/RZUPBVFL/Keidel and Erdweg - 2019 - Sound and reusable components for abstract interpr.pdf}
}

@article{keidelSoundReusableComponents2019a,
  title = {Sound and Reusable Components for Abstract Interpretation},
  author = {Keidel, Sven and Erdweg, Sebastian},
  date = {2019-10-10},
  journaltitle = {Proc. ACM Program. Lang.},
  volume = {3},
  pages = {176:1--176:28},
  doi = {10.1145/3360602},
  url = {https://doi.org/10.1145/3360602},
  urldate = {2021-10-10},
  abstract = {Abstract interpretation is a methodology for defining sound static analysis. Yet, building sound static analyses for modern programming languages is difficult, because these static analyses need to combine sophisticated abstractions for values, environments, stores, etc. However, static analyses often tightly couple these abstractions in the implementation, which not only complicates the implementation, but also makes it hard to decide which parts of the analyses can be proven sound independently from each other. Furthermore, this coupling makes it hard to combine soundness lemmas for parts of the analysis to a soundness proof of the complete analysis. To solve this problem, we propose to construct static analyses modularly from reusable analysis components. Each analysis component encapsulates a single analysis concern and can be proven sound independently from the analysis where it is used. We base the design of our analysis components on arrow transformers, which allows us to compose analysis components. This composition preserves soundness, which guarantees that a static analysis is sound, if all its analysis components are sound. This means that analysis developers do not have to worry about soundness as long as they reuse sound analysis components. To evaluate our approach, we developed a library of 13 reusable analysis components in Haskell. We use these components to define a k-CFA analysis for PCF and an interval and reaching definition analysis for a While language.},
  issue = {OOPSLA},
  keywords = {Abstract Interpretation,Soundness,Static Analysis},
  file = {/Users/cperivol/Zotero/storage/UGNDRM7C/Keidel and Erdweg - 2019 - Sound and reusable components for abstract interpr.pdf}
}

@inproceedings{kemperHyPerHybridOLTP2011,
  title = {{{HyPer}}: A Hybrid {{OLTP}}\&{{OLAP}} Main Memory Database System Based on Virtual Memory Snapshots},
  shorttitle = {{{HyPer}}},
  booktitle = {2011 {{IEEE}} 27th {{International Conference}} on {{Data Engineering}}},
  author = {Kemper, Alfons and Neumann, Thomas},
  date = {2011},
  pages = {195--206},
  publisher = {{IEEE}},
  file = {/Users/cperivol/Zotero/storage/HY2WYJ3F/5767867.html}
}

@article{kidneyAlgebrasWeightedSearch2021,
  title = {Algebras for Weighted Search},
  author = {Kidney, Donnacha Oisín and Wu, Nicolas},
  date = {2021-08-18},
  journaltitle = {Proc. ACM Program. Lang.},
  volume = {5},
  pages = {72:1--72:30},
  doi = {10.1145/3473577},
  url = {https://doi.org/10.1145/3473577},
  urldate = {2021-10-20},
  abstract = {Weighted search is an essential component of many fundamental and useful algorithms. Despite this, it is relatively under explored as a computational effect, receiving not nearly as much attention as either depth- or breadth-first search. This paper explores the algebraic underpinning of weighted search, and demonstrates how to implement it as a monad transformer. The development first explores breadth-first search, which can be expressed as a polynomial over semirings. These polynomials are generalised to the free semimodule monad to capture a wide range of applications, including probability monads, polynomial monads, and monads for weighted search. Finally, a monad transformer based on the free semimodule monad is introduced. Applying optimisations to this type yields an implementation of pairing heaps, which is then used to implement Dijkstra's algorithm and efficient probabilistic sampling. The construction is formalised in Cubical Agda and implemented in Haskell.},
  issue = {ICFP},
  keywords = {Agda,graph search,Haskell,monad},
  file = {/Users/cperivol/Zotero/storage/DCJWVUV2/Kidney and Wu - 2021 - Algebras for weighted search.pdf}
}

@article{kiselyovBacktrackingInterleavingTerminating,
  title = {Backtracking, {{Interleaving}}, and {{Terminating Monad Transformers}}},
  author = {Kiselyov, Oleg and Shan, Chung-chieh and Friedman, Daniel P and Sabry, Amr},
  pages = {12},
  abstract = {We design and implement a library for adding backtracking computations to any Haskell monad. Inspired by logic programming, our library provides, in addition to the operations required by the MonadPlus interface, constructs for fair disjunctions, fair conjunctions, conditionals, pruning, and an expressive top-level interface. Implementing these additional constructs is easy in models of backtracking based on streams, but not known to be possible in continuation-based models. We show that all these additional constructs can be generically and monadically realized using a single primitive msplit. We present two implementations of the library: one using success and failure continuations; and the other using control operators for manipulating delimited continuations.},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/RKWTARQF/Kiselyov et al. - Backtracking, Interleaving, and Terminating Monad .pdf}
}

@software{kmettPropagators2021,
  title = {Propagators},
  author = {Kmett, Edward},
  date = {2021-08-25T17:28:28Z},
  origdate = {2015-09-20T02:08:45Z},
  url = {https://github.com/ekmett/propagators},
  urldate = {2021-10-19},
  abstract = {The Art of the Propagator. See also:}
}

@inproceedings{kohnAdaptiveExecutionCompiled2018,
  title = {Adaptive Execution of Compiled Queries},
  booktitle = {2018 {{IEEE}} 34th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Kohn, André and Leis, Viktor and Neumann, Thomas},
  date = {2018},
  pages = {197--208},
  publisher = {{IEEE}},
  file = {/Users/cperivol/Zotero/storage/VLTIGIIK/8509248.html}
}

@article{kohnMakingCompilingQuery2021,
  title = {Making {{Compiling Query Engines Practical}}},
  author = {Kohn, André and Leis, Viktor and Neumann, Thomas},
  date = {2021-02},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {33},
  number = {2},
  pages = {597--612},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2019.2905235},
  abstract = {Compiling queries to machine code is a very efficient way for executing queries. One often overlooked problem with compilation is the time it takes to generate machine code. Even with fast compilation frameworks like LLVM, generating machine code for complex queries often takes hundreds of milliseconds. Such durations can be a major disadvantage for workloads that execute many complex, but quick queries. To solve this problem, we propose an adaptive execution framework, which dynamically switches from interpretation to compilation. We also propose a fast bytecode interpreter for LLVM, which can execute queries without costly translation to machine code and dramatically reduces the query latency. Adaptive execution is fine-grained, and can execute code paths of the same query using different execution modes. Our evaluation shows that this approach achieves optimal performance in a wide variety of settings-low latency for small data sets and maximum throughput for large data sizes. Besides compilation time, we also focus on debugging, which is another important challenge of compilation-based query engines. To address this problem, we present a novel, database-specific debugger for compiling query engines.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  keywords = {bytecode interpreter,Computer bugs,debugging,Debugging,Engines,Optimization,query compilation,Query processing,register allocation,Semantics,Throughput,Tools},
  annotation = {00000}
}

@article{kraskaSageDBLearnedDatabase,
  title = {{{SageDB}}: A {{Learned Database System}}},
  author = {Kraska, Tim and Alizadeh, Mohammad and Beutel, Alex and Chi, Ed H and Ding, Jialin and Kristo, Ani and Leclerc, Guillaume and Madden, Samuel and Mao, Hongzi and Nathan, Vikram},
  pages = {10},
  abstract = {Modern data processing systems are designed to be general purpose, in that they can handle a wide variety of different schemas, data types, and data distributions, and aim to provide efficient access to that data via the use of optimizers and cost models. This general purpose nature results in systems that do not take advantage of the characteristics of the particular application and data of the user. With SageDB we present a vision towards a new type of a data processing system, one which highly specializes to an application through code synthesis and machine learning. By modeling the data distribution, workload, and hardware, SageDB learns the structure of the data and optimal access methods and query plans. These learned models are deeply embedded, through code synthesis, in essentially every component of the database. As such, SageDB presents radical departure from the way database systems are currently developed, raising a host of new problems in databases, machine learning and programming systems.},
  langid = {english},
  annotation = {00111},
  file = {/Users/cperivol/Zotero/storage/C7RBLGI7/Kraska et al. - SageDB A Learned Database System.pdf}
}

@inproceedings{kraskaSagedbLearnedDatabase2019,
  title = {Sagedb: A Learned Database System},
  shorttitle = {Sagedb},
  booktitle = {{{CIDR}}},
  author = {Kraska, Tim and Alizadeh, Mohammad and Beutel, Alex and Chi, H. and Kristo, Ani and Leclerc, Guillaume and Madden, Samuel and Mao, Hongzi and Nathan, Vikram},
  date = {2019},
  annotation = {00111},
  file = {/Users/cperivol/Zotero/storage/V8C48YN4/Kraska et al. - 2019 - Sagedb A learned database system.pdf}
}

@inproceedings{krikellasGeneratingCodeHolistic2010,
  title = {Generating Code for Holistic Query Evaluation},
  booktitle = {2010 {{IEEE}} 26th {{International Conference}} on {{Data Engineering}} ({{ICDE}} 2010)},
  author = {Krikellas, Konstantinos and Viglas, Stratis D. and Cintra, Marcelo},
  date = {2010-03},
  pages = {613--624},
  issn = {2375-026X},
  doi = {10.1109/ICDE.2010.5447892},
  abstract = {We present the application of customized code generation to database query evaluation. The idea is to use a collection of highly efficient code templates and dynamically instantiate them to create query- and hardware-specific source code. The source code is compiled and dynamically linked to the database server for processing. Code generation diminishes the bloat of higher-level programming abstractions necessary for implementing generic, interpreted, SQL query engines. At the same time, the generated code is customized for the hardware it will run on. We term this approach holistic query evaluation. We present the design and development of a prototype system called HIQUE, the Holistic Integrated Query Engine, which incorporates our proposals. We undertake a detailed experimental study of the system's performance. The results show that HIQUE satisfies its design objectives, while its efficiency surpasses that of both well-established and currently-emerging query processing techniques.},
  eventtitle = {2010 {{IEEE}} 26th {{International Conference}} on {{Data Engineering}} ({{ICDE}} 2010)},
  keywords = {Database systems,Engines,Hardware,Heuristic algorithms,Informatics,Optimizing compilers,Proposals,Prototypes,Query processing,System performance},
  file = {/Users/cperivol/Zotero/storage/H9S9QXDT/Krikellas et al. - 2010 - Generating code for holistic query evaluation.pdf}
}

@inproceedings{kunjirRobusFairCache2017,
  title = {Robus: Fair Cache Allocation for Data-Parallel Workloads},
  shorttitle = {Robus},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}}},
  author = {Kunjir, Mayuresh and Fain, Brandon and Munagala, Kamesh and Babu, Shivnath},
  date = {2017},
  pages = {219--234},
  file = {/Users/cperivol/Zotero/storage/6N2Y2YXF/Kunjir et al. - 2017 - Robus Fair cache allocation for data-parallel wor.pdf;/Users/cperivol/Zotero/storage/L4M9CS3K/3035918.html}
}

@inproceedings{kuperLVarsLatticebasedData2013,
  title = {{{LVars}}: Lattice-Based Data Structures for Deterministic Parallelism},
  shorttitle = {{{LVars}}},
  booktitle = {Proceedings of the 2nd {{ACM SIGPLAN}} Workshop on {{Functional}} High-Performance Computing},
  author = {Kuper, Lindsey and Newton, Ryan R.},
  date = {2013-09-23},
  series = {{{FHPC}} '13},
  pages = {71--84},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2502323.2502326},
  url = {https://doi.org/10.1145/2502323.2502326},
  urldate = {2021-10-19},
  abstract = {Programs written using a deterministic-by-construction model of parallel computation are guaranteed to always produce the same observable results, offering programmers freedom from subtle, hard-to-reproduce nondeterministic bugs that are the scourge of parallel software. We present LVars, a new model for deterministic-by-construction parallel programming that generalizes existing single-assignment models to allow multiple assignments that are monotonically increasing with respect to a user-specified lattice. LVars ensure determinism by allowing only monotonic writes and "threshold" reads that block until a lower bound is reached. We give a proof of determinism and a prototype implementation for a language with LVars and describe how to extend the LVars model to support a limited form of nondeterminism that admits failures but never wrong answers.},
  isbn = {978-1-4503-2381-9},
  keywords = {deterministic parallelism,lattices},
  file = {/Users/cperivol/Zotero/storage/E6KDGYMI/Kuper and Newton - 2013 - LVars lattice-based data structures for determini.pdf}
}

@article{launchburyCoroutiningFoldsHyperfunctions2013,
  title = {Coroutining {{Folds}} with {{Hyperfunctions}}},
  author = {Launchbury, J. and Krstic, S. and Sauerwein, T. E.},
  date = {2013-09-19},
  journaltitle = {Electron. Proc. Theor. Comput. Sci.},
  volume = {129},
  eprint = {1309.5135},
  eprinttype = {arxiv},
  pages = {121--135},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.129.9},
  url = {http://arxiv.org/abs/1309.5135},
  urldate = {2021-05-18},
  abstract = {Fold functions are a general mechanism for computing over recursive data structures. First-order folds compute results bottom-up. With higher-order folds, computations that inherit attributes from above can also be expressed. In this paper, we explore folds over a form of recursive higher-order function, called hyperfunctions, and show that hyperfunctions allow fold computations to coroutine across data structures, as well as compute bottom up and top down. We use the compiler technique of foldr-build as an exemplar to show how hyperfunctions can be used.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages,D.1.1},
  file = {/Users/cperivol/Zotero/storage/9INTH2B8/Launchbury et al. - 2013 - Coroutining Folds with Hyperfunctions.pdf;/Users/cperivol/Zotero/storage/GF6N77YM/1309.html}
}

@article{launchburyZipFusionHyperfunctions2000,
  title = {Zip {{Fusion}} with {{Hyperfunctions}}},
  author = {Launchbury, John and Sauerwein, Timothy},
  date = {2000-04-17},
  abstract = {Type of Hyperfunctions Now that we have three models that support programming with coroutining continuations, it is natural to ask about the core functionality provided by all of them. So we embark on a study of an abstract type. We give the axiomatics satised by all three models. We establish some connections between the models and obtain some precise results about linear" models which seem to be the most relevant in this context. The upshot of this research is our thesis that all models of the abstract type are potentially usable in deforestation algorithms, and that the implementors should feel free to search among various models and choose those that do the best job. 9.1 Axiomatics Let us use the notation K a b for the abstract type of hyperfunctions. We begin by requiring that K a b be functorial in its two arguments (contravariant in the rst, covariant in the second) and that the domains K a b can be regarded as the arrow sets of a category that contains our base category o...}
}

@report{launchburyZipFusionHyperfunctions2000a,
  title = {Zip {{Fusion}} with {{Hyperfunctions}}},
  author = {Launchbury, John and Krstic, Sava and Sauerwein, Timothy E.},
  date = {2000},
  abstract = {Automatic removal of intermediate structures has been...},
  file = {/Users/cperivol/Zotero/storage/2VVNLFFD/Launchbury et al. - 2000 - Zip Fusion with Hyperfunctions.pdf;/Users/cperivol/Zotero/storage/N2D9RSSL/summary.html}
}

@inproceedings{leeJITCompilationBasedUnified2016,
  title = {A {{JIT Compilation}}-{{Based Unified SQL Query Optimization System}}},
  booktitle = {2016 6th {{International Conference}} on {{IT Convergence}} and {{Security}} ({{ICITCS}})},
  author = {Lee, Myungcheol and Lee, Miyoung and Kim, ChangSoo},
  date = {2016-09},
  pages = {1--2},
  doi = {10.1109/ICITCS.2016.7740304},
  abstract = {In-memory databases are gaining attention as a solution to efficiently support SQL queries on large volume of data, as main memories are becoming cheaper and grow in size. However, their query performance is not well improved on modern hardware with faster CPUs, registers and caches due to the limitation of the classical iterator style query processing model. We propose a unified SQL query optimization system using JIT compilation of OLTP, OLAP, and Stored Procedure workloads for enhanced performance on modern hardware.},
  eventtitle = {2016 6th {{International Conference}} on {{IT Convergence}} and {{Security}} ({{ICITCS}})},
  keywords = {Graphics processing units,Hardware,Optimization,Performance evaluation,Query processing,Runtime},
  file = {/Users/cperivol/Zotero/storage/P7NWRNJG/Lee et al. - 2016 - A JIT Compilation-Based Unified SQL Query Optimiza.pdf;/Users/cperivol/Zotero/storage/347HRPAD/7740304.html}
}

@inproceedings{lefevreOpportunisticPhysicalDesign2014,
  title = {Opportunistic Physical Design for Big Data Analytics},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {LeFevre, Jeff and Sankaranarayanan, Jagan and Hacigumus, Hakan and Tatemura, Junichi and Polyzotis, Neoklis and Carey, Michael J.},
  date = {2014},
  pages = {851--862}
}

@article{leijenParsecDirectStyle,
  title = {Parsec: Direct {{Style Monadic Parser Combinators For The Real World}}},
  author = {Leijen, Daan},
  pages = {22},
  abstract = {Despite the long list of publications on parser combinators, there does not yet exist a monadic parser combinator library that is applicable in real world situations. In particular naive implementations of parser combinators are likely to suffer from space leaks and are often unable to report precise error messages in case of parse errors. The Parsec parser combinator library described in this paper, utilizes a novel implementation technique for space and time efficient parser combinators that in case of a parse error, report both the position of the error as well as all grammar productions that would have been legal at that point in the input.},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/K7KGCXSI/Leijen - Parsec Direct Style Monadic Parser Combinators Fo.pdf}
}

@online{leijenParsecDirectStyle2001,
  type = {Preprint},
  title = {Parsec: Direct Style Monadic Parser Combinators for the Real World},
  shorttitle = {Parsec},
  author = {Leijen, D. and Meijer, E.},
  date = {2001-01-01},
  url = {http://localhost/handle/1874/2535},
  urldate = {2021-10-17},
  abstract = {Despite the long list of publications on parser combinators, there does  not yet exist a monadic parser combinator library that is applicable in real  world situations. In particular naive implementations of parser combinators  are likely to suffer from space leaks and are often unable to report  precise error messages in case of parse errors. The Parsec parser combinator  library described in this paper, utilizes a novel implementation  technique for space and time e±cient parser combinators that in case of  a parse error, report both the position of the error as well as all grammar  productions that would have been legal at that point in the input.},
  langid = {english},
  annotation = {Accepted: 2002-03-08T10:29:35Z},
  file = {/Users/cperivol/Zotero/storage/G8QQEI2P/Leijen and Meijer - 2001 - Parsec direct style monadic parser combinators fo.pdf;/Users/cperivol/Zotero/storage/6B3YMAHF/2535.html}
}

@article{leisHowGoodAre2015,
  title = {How Good Are Query Optimizers, Really?},
  author = {Leis, Viktor and Gubichev, Andrey and Mirchev, Atanas and Boncz, Peter and Kemper, Alfons and Neumann, Thomas},
  date = {2015-11},
  journaltitle = {Proc. VLDB Endow.},
  volume = {9},
  number = {3},
  pages = {204--215},
  issn = {2150-8097},
  doi = {10.14778/2850583.2850594},
  url = {https://dl.acm.org/doi/10.14778/2850583.2850594},
  urldate = {2021-11-25},
  abstract = {Finding a good join order is crucial for query performance. In this paper, we introduce the Join Order Benchmark (JOB) and experimentally revisit the main components in the classic query optimizer architecture using a complex, real-world data set and realistic multi-join queries. We investigate the quality of industrial-strength cardinality estimators and find that all estimators routinely produce large errors. We further show that while estimates are essential for finding a good join order, query performance is unsatisfactory if the query engine relies too heavily on these estimates. Using another set of experiments that measure the impact of the cost model, we find that it has much less influence on query performance than the cardinality estimates. Finally, we investigate plan enumeration techniques comparing exhaustive dynamic programming with heuristic algorithms and find that exhaustive enumeration improves performance despite the sub-optimal cardinality estimates.},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/SAVJ39W4/Leis et al. - 2015 - How good are query optimizers, really.pdf}
}

@inproceedings{leisMorseldrivenParallelismNUMAaware2014,
  title = {Morsel-Driven Parallelism: A {{NUMA}}-Aware Query Evaluation Framework for the Many-Core Age},
  shorttitle = {Morsel-Driven Parallelism},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Leis, Viktor and Boncz, Peter and Kemper, Alfons and Neumann, Thomas},
  date = {2014},
  pages = {743--754},
  file = {/Users/cperivol/Zotero/storage/QQJLKU8D/Leis et al. - 2014 - Morsel-driven parallelism A NUMA-aware query eval.pdf;/Users/cperivol/Zotero/storage/AUDCSMKJ/2588555.html}
}

@article{leisQueryOptimizationLooking2018,
  title = {Query Optimization through the Looking Glass, and What We Found Running the {{Join Order Benchmark}}},
  author = {Leis, Viktor and Radke, Bernhard and Gubichev, Andrey and Mirchev, Atanas and Boncz, Peter and Kemper, Alfons and Neumann, Thomas},
  date = {2018-10-01},
  journaltitle = {The VLDB Journal},
  volume = {27},
  doi = {10.1007/s00778-017-0480-7},
  abstract = {Finding a good join order is crucial for query performance. In this paper, we introduce the Join Order Benchmark that works on real-life data riddled with correlations and introduces 113 complex join queries. We experimentally revisit the main components in the classic query optimizer architecture using a complex, real-world data set and realistic multi-join queries. For this purpose, we describe cardinality-estimate injection and extraction techniques that allow us to compare the cardinality estimators of multiple industrial SQL implementations on equal footing, and to characterize the value of having perfect cardinality estimates. Our investigation shows that all industrial-strength cardinality estimators routinely produce large errors: though cardinality estimation using table samples solves the problem for single-table queries, there are still no techniques in industrial systems that can deal accurately with join-crossing correlated query predicates. We further show that while estimates are essential for finding a good join order, query performance is unsatisfactory if the query engine relies too heavily on these estimates. Using another set of experiments that measure the impact of the cost model, we find that it has much less influence on query performance than the cardinality estimates. We investigate plan enumeration techniques comparing exhaustive dynamic programming with heuristic algorithms and find that exhaustive enumeration improves performance despite the suboptimal cardinality estimates. Finally, we extend our investigation from main-memory only, to also include disk-based query processing. Here, we find that though accurate cardinality estimation should be the first priority, other aspects such as modeling random versus sequential I/O are also important to predict query runtime.}
}

@article{leisQueryOptimizationLooking2018a,
  title = {Query Optimization through the Looking Glass, and What We Found Running the Join Order Benchmark},
  author = {Leis, Viktor and Radke, Bernhard and Gubichev, Andrey and Mirchev, Atanas and Boncz, Peter and Kemper, Alfons and Neumann, Thomas},
  date = {2018},
  journaltitle = {The VLDB Journal},
  volume = {27},
  number = {5},
  pages = {643--668},
  publisher = {{Springer}},
  file = {/Users/cperivol/Zotero/storage/6CC59YIQ/s00778-017-0480-7.html}
}

@inproceedings{leScalableMultiqueryOptimization2012,
  title = {Scalable Multi-Query Optimization for {{SPARQL}}},
  booktitle = {2012 {{IEEE}} 28th {{International Conference}} on {{Data Engineering}}},
  author = {Le, Wangchao and Kementsietsidis, Anastasios and Duan, Songyun and Li, Feifei},
  date = {2012},
  pages = {666--677},
  publisher = {{IEEE}},
  file = {/Users/cperivol/Zotero/storage/A6X79FM8/6228123.html}
}

@inproceedings{liangMonadTransformersModular1995,
  title = {Monad Transformers and Modular Interpreters},
  booktitle = {Proceedings of the 22nd {{ACM SIGPLAN}}-{{SIGACT}} Symposium on {{Principles}} of Programming Languages},
  author = {Liang, Sheng and Hudak, Paul and Jones, Mark},
  date = {1995-01-25},
  series = {{{POPL}} '95},
  pages = {333--343},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/199448.199528},
  url = {https://doi.org/10.1145/199448.199528},
  urldate = {2021-10-10},
  abstract = {We show how a set of building blocks can be used to construct programming language interpreters, and present implementations of such building blocks capable of supporting many commonly known features, including simple expressions, three different function call mechanisms (call-by-name, call-by-value and lazy evaluation), references and assignment, nondeterminism, first-class continuations, and program tracing. The underlying mechanism of our system is monad transformers, a simple form of abstraction for introducing a wide range of computational behaviors, such as state, I/O, continuations, and exceptions. Our work is significant in the following respects. First, we have succeeded in designing a fully modular interpreter based on monad transformers that incudes features missing from Steele's, Espinosa's, and Wadler's earlier efforts. Second, we have found new ways to lift monad operations through monad transformers, in particular difficult cases not achieved in Moggi's original work. Third, we have demonstrated that interactions between features are reflected in liftings and that semantics can be changed by reordering monad transformers. Finally, we have implemented our interpreter in Gofer, whose constructor classes provide just the added power over Haskell's type classes to allow precise and convenient expression of our ideas. This implementation includes a method for constructing extensible unions and a form of subtyping that is interesting in its own right.},
  isbn = {978-0-89791-692-9},
  file = {/Users/cperivol/Zotero/storage/52QQFZJ9/Liang et al. - 1995 - Monad transformers and modular interpreters.pdf}
}

@inproceedings{liMachineLearningDatabases2021,
  title = {Machine {{Learning}} for {{Databases}}},
  booktitle = {The {{First International Conference}} on {{AI}}-{{ML}}-{{Systems}}},
  author = {Li, Guoliang and Zhou, Xuanhe and Cao, Lei},
  date = {2021-10-21},
  series = {{{AIMLSystems}} 2021},
  pages = {1--2},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3486001.3486248},
  url = {https://doi.org/10.1145/3486001.3486248},
  urldate = {2021-11-25},
  abstract = {Machine learning techniques have been proposed to optimize the databases. For example, traditional empirical database optimization techniques (e.g., cost estimation, join order selection, knob tuning) cannot meet the high-performance requirement for large-scale database instances, various applications and diversified users, especially on the cloud. Fortunately, machine learning based techniques can alleviate this problem by judiciously learning the optimization strategy from historical data or explorations. In this tutorial, we categorize database tasks into three typical problems that can be optimized by different machine learning models, including (i) NP-hard problems (e.g., knob space exploration, index/view selection, partition-key recommendation for offline optimization; query rewrite, join order selection for online optimization), (ii) regression problems (e.g., cost/cardinality estimation, index/view benefit estimation, query latency prediction), and (iii) prediction problems (e.g., transaction scheduling, trend prediction). We review existing machine learning based techniques to address these problems and provide research challenges.},
  isbn = {978-1-4503-8594-7},
  file = {/Users/cperivol/Zotero/storage/GTYQS4BR/Li et al. - 2021 - Machine Learning for Databases.pdf}
}

@article{liuKodiakLeveragingMaterialized2016,
  title = {Kodiak: Leveraging Materialized Views for Very Low-Latency Analytics over High-Dimensional Web-Scale Data},
  shorttitle = {Kodiak},
  author = {Liu, Shaosu and Song, Bin and Gangam, Sriharsha and Lo, Lawrence and Elmeleegy, Khaled},
  date = {2016},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {9},
  number = {13},
  pages = {1269--1280},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/QKE8D3NZ/Liu et al. - 2016 - Kodiak Leveraging materialized views for very low.pdf;/Users/cperivol/Zotero/storage/T4F29ZSW/3007263.html}
}

@report{makreshanskiMqjoinEfficientShared2016,
  title = {Mqjoin: Efficient Shared Execution of Main-Memory Joins},
  shorttitle = {Mqjoin},
  author = {Makreshanski, Darko and Giannikis, Georgios and Alonso, Gustavo and Kossmann, Donald},
  date = {2016},
  institution = {{ETH Zurich}},
  file = {/Users/cperivol/Zotero/storage/3E8UAMYX/Makreshanski et al. - 2016 - Mqjoin Efficient shared execution of main-memory .pdf}
}

@article{mamiSurveyViewSelection2012,
  title = {A Survey of View Selection Methods},
  author = {Mami, Imene and Bellahsene, Zohra},
  date = {2012},
  journaltitle = {Acm Sigmod Record},
  volume = {41},
  number = {1},
  pages = {20--29},
  publisher = {{ACM New York, NY, USA}},
  file = {/Users/cperivol/Zotero/storage/CDJQET29/Mami and Bellahsene - 2012 - A survey of view selection methods.pdf;/Users/cperivol/Zotero/storage/PBETV5AK/2206869.html}
}

@article{marcusNeoLearnedQuery2019,
  title = {Neo: A Learned Query Optimizer},
  shorttitle = {Neo},
  author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Zhang, Chi and Alizadeh, Mohammad and Kraska, Tim and Papaemmanouil, Olga and Tatbul, Nesime},
  date = {2019-07},
  journaltitle = {Proc. VLDB Endow.},
  volume = {12},
  number = {11},
  pages = {1705--1718},
  issn = {2150-8097},
  doi = {10.14778/3342263.3342644},
  url = {https://dl.acm.org/doi/10.14778/3342263.3342644},
  urldate = {2021-11-26},
  abstract = {Query optimization is one of the most challenging problems in database systems. Despite the progress made over the past decades, query optimizers remain extremely complex components that require a great deal of hand-tuning for specific workloads and datasets. Motivated by this shortcoming and inspired by recent advances in applying machine learning to data management challenges, we introduce Neo (Neural Optimizer), a novel learning-based query optimizer that relies on deep neural networks to generate query executions plans. Neo bootstraps its query optimization model from existing optimizers and continues to learn from incoming queries, building upon its successes and learning from its failures. Furthermore, Neo naturally adapts to underlying data patterns and is robust to estimation errors. Experimental results demonstrate that Neo, even when bootstrapped from a simple optimizer like PostgreSQL, can learn a model that offers similar performance to state-of-the-art commercial optimizers, and in some cases even surpass them.},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/F3GLFWWS/Marcus et al. - 2019 - Neo a learned query optimizer.pdf}
}

@online{marcusNeoLearnedQuery2019a,
  title = {Neo: A Learned Query Optimizer},
  shorttitle = {Neo},
  author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Zhang, Chi and Alizadeh, Mohammad and Kraska, Tim and Papaemmanouil, Olga and Tatbul, Nesime},
  date = {2019},
  eprint = {1904.03711},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/cperivol/Zotero/storage/P39FTCIA/Marcus et al. - 2019 - Neo A learned query optimizer.pdf;/Users/cperivol/Zotero/storage/5X48J89P/1904.html}
}

@article{marklLEOAutonomicQuery2003,
  title = {{{LEO}}: An Autonomic Query Optimizer for {{DB2}}},
  shorttitle = {{{LEO}}},
  author = {Markl, V. and Lohman, G. M. and Raman, V.},
  date = {2003},
  journaltitle = {IBM Systems Journal},
  volume = {42},
  number = {1},
  pages = {98--106},
  issn = {0018-8670},
  doi = {10.1147/sj.421.0098},
  abstract = {SQL has emerged as an industry standard for querying relational database management systems, largely because a user need only specify what data is wanted, not the details of how to access that data. A query optimizer uses a mathematical model of query execution to determine automatically the best way to access and process any given SQL query. This model is heavily dependent upon the optimizer's estimates for the number of rows that will result at each step of the query execution plan (QEP), especially for complex queries involving many predicates and/or operations. These estimates rely upon statistics on the database and modeling assumptions that may or may not be true for a given database. In this paper we discuss an autonomic query optimizer that automatically self-validates its model without requiring any user interaction to repair incorrect statistics or cardinality estimates. By monitoring queries as they execute, the autonomic optimizer compares the optimizer's estimates with actuals at each step in a QEP, and computes adjustments to its estimates that may be used during future optimizations of similar queries. Moreover, estimation errors can also trigger re-optimization of a query in mid-execution. The autonomic refinement of the optimizer's model can result in a reduction of query execution time by orders of magnitude at negligible additional run-time cost. We discuss various research issues and practical considerations that we needed to address during our implementation of a first prototype of LEO, a LEarning Optimizer for DB2® (Database 2™)that learns table access cardinalities and for future queries corrects the estimation error for simple predicates by adjusting the database statistics of DB2.},
  eventtitle = {{{IBM Systems Journal}}},
  annotation = {00153},
  file = {/Users/cperivol/Zotero/storage/4L22DRV9/5386840.html}
}

@article{marlowHaxlProjectFacebook2013,
  title = {The {{Haxl Project}} at {{Facebook}}},
  author = {Marlow, Simon and Coens, Jon and Brandy, Louis and Purdy, Jon},
  date = {2013},
  journaltitle = {Proceedings of the Code Mesh London}
}

@misc{martinSQLGImplementationApache2020,
  title = {{{SQLG}}: An Implementation of {{Apache TinkerPop}} on a {{RDBMS}}},
  shorttitle = {{{SQLG}}},
  author = {Martin, Pieter},
  date = {2020}
}

@article{MaterializedViewsTechniques1999,
  title = {Materialized {{Views}}: Techniques, {{Implementations}}, and {{Applications}}},
  shorttitle = {Materialized {{Views}}},
  date = {1999-05-24},
  doi = {10.7551/mitpress/4472.001.0001},
  url = {https://direct.mit.edu/books/book/2853/Materialized-ViewsTechniques-Implementations-and},
  urldate = {2021-11-26},
  abstract = {When an application is built, an underlying data model is chosen to make that application effective. Frequently, other applications need the same data, only mod},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/5TQGWSB7/Materialized-ViewsTechniques-Implementations-and.html}
}

@inproceedings{mavlyutovDependencyDrivenAnalyticsCompass2017,
  title = {Dependency-{{Driven Analytics}}: A {{Compass}} for {{Uncharted Data Oceans}}.},
  shorttitle = {Dependency-{{Driven Analytics}}},
  booktitle = {{{CIDR}}},
  author = {Mavlyutov, Ruslan and Curino, Carlo and Asipov, Boris and Cudre-Mauroux, Philippe},
  date = {2017},
  file = {/Users/cperivol/Zotero/storage/E5XFBPLT/Mavlyutov et al. - 2017 - Dependency-Driven Analytics A Compass for Unchart.pdf}
}

@article{mcbrideApplicativeProgrammingEffects2008,
  title = {Applicative Programming with Effects},
  author = {McBride, Conor and Paterson, Ross},
  date = {2008},
  journaltitle = {Journal of functional programming},
  volume = {18},
  number = {1},
  pages = {1--13},
  publisher = {{Cambridge University Press}},
  issn = {1469-7653}
}

@article{menonRelaxedOperatorFusion2017,
  title = {Relaxed Operator Fusion for In-Memory Databases: Making Compilation, Vectorization, and Prefetching Work Together at Last},
  shorttitle = {Relaxed Operator Fusion for In-Memory Databases},
  author = {Menon, Prashanth and Mowry, Todd C. and Pavlo, Andrew},
  date = {2017-09-01},
  journaltitle = {Proc. VLDB Endow.},
  volume = {11},
  number = {1},
  pages = {1--13},
  issn = {2150-8097},
  doi = {10.14778/3151113.3151114},
  url = {https://doi.org/10.14778/3151113.3151114},
  urldate = {2021-11-27},
  abstract = {In-memory database management systems (DBMSs) are a key component of modern on-line analytic processing (OLAP) applications, since they provide low-latency access to large volumes of data. Because disk accesses are no longer the principle bottleneck in such systems, the focus in designing query execution engines has shifted to optimizing CPU performance. Recent systems have revived an older technique of using just-in-time (JIT) compilation to execute queries as native code instead of interpreting a plan. The state-of-the-art in query compilation is to fuse operators together in a query plan to minimize materialization overhead by passing tuples efficiently between operators. Our empirical analysis shows, however, that more tactful materialization yields better performance. We present a query processing model called "relaxed operator fusion" that allows the DBMS to introduce staging points in the query plan where intermediate results are temporarily materialized. This allows the DBMS to take advantage of inter-tuple parallelism inherent in the plan using a combination of prefetching and SIMD vectorization to support faster query execution on data sets that exceed the size of CPU-level caches. Our evaluation shows that our approach reduces the execution time of OLAP queries by up to 2.2× and achieves up to 1.8× better performance compared to other in-memory DBMSs.},
  annotation = {00000}
}

@article{michiardiCachebasedMultiqueryOptimization2021,
  title = {Cache-Based Multi-Query Optimization for Data-Intensive Scalable Computing Frameworks},
  author = {Michiardi, Pietro and Carra, Damiano and Migliorini, Sara},
  date = {2021},
  journaltitle = {Information Systems Frontiers},
  volume = {23},
  number = {1},
  pages = {35--51},
  publisher = {{Springer}},
  file = {/Users/cperivol/Zotero/storage/IHJ7LWUW/Michiardi et al. - 2021 - Cache-based multi-query optimization for data-inte.pdf;/Users/cperivol/Zotero/storage/LLXK3X7F/s10796-020-09995-2.html}
}

@article{michiardiCachebasedMultiqueryOptimization2021a,
  title = {Cache-Based Multi-Query Optimization for Data-Intensive Scalable Computing Frameworks},
  author = {Michiardi, Pietro and Carra, Damiano and Migliorini, Sara},
  date = {2021},
  journaltitle = {Information Systems Frontiers},
  volume = {23},
  number = {1},
  pages = {35--51},
  publisher = {{Springer}},
  file = {/Users/cperivol/Zotero/storage/LLQXCM4X/Michiardi et al. - 2021 - Cache-based multi-query optimization for data-inte.pdf;/Users/cperivol/Zotero/storage/JWKE7KRQ/s10796-020-09995-2.html}
}

@inproceedings{michiardiInmemoryCachingMultiquery2019,
  title = {In-Memory {{Caching}} for {{Multi}}-Query {{Optimization}} of {{Data}}-Intensive {{Scalable Computing Workloads}}.},
  booktitle = {{{EDBT}}/{{ICDT Workshops}}},
  author = {Michiardi, Pietro and Carra, Damiano and Migliorini, Sara},
  date = {2019}
}

@inproceedings{mistryMaterializedViewSelection2001,
  title = {Materialized View Selection and Maintenance Using Multi-Query Optimization},
  booktitle = {Proceedings of the 2001 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Mistry, Hoshi and Roy, Prasan and Sudarshan, S. and Ramamritham, Krithi},
  date = {2001-05-01},
  series = {{{SIGMOD}} '01},
  pages = {307--318},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/375663.375703},
  url = {https://doi.org/10.1145/375663.375703},
  urldate = {2021-06-06},
  abstract = {Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views. In this paper, we show how to find an efficient plan for the maintenance of a set of materialized views, by exploiting common subexpressions between different view maintenance expressions. In particular, we show how to efficiently select (a) expressions and indices that can be effectively shared, by transient materialization; (b) additional expressions and indices for permanent materialization; and (c) the best maintenance plan — incremental or recomputation — for each view. These three decisions are highly interdependent, and the choice of one affects the choice of the others. We develop a framework that cleanly integrates the various choices in a systematic and efficient manner. Our evaluations show that many-fold improvement in view maintenance time can be achieved using our techniques. Our algorithms can also be used to efficiently select materialized views to speed up workloads containing queries and updates.},
  isbn = {978-1-58113-332-5},
  file = {/Users/cperivol/Zotero/storage/G4MXZM32/Mistry et al. - 2001 - Materialized view selection and maintenance using .pdf}
}

@inproceedings{moerkotteDynamicProgrammingStrikes2008,
  title = {Dynamic Programming Strikes Back},
  booktitle = {Proceedings of the 2008 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Moerkotte, Guido and Neumann, Thomas},
  date = {2008},
  pages = {539--552},
  file = {/Users/cperivol/Zotero/storage/2NPKYTB6/Moerkotte and Neumann - 2008 - Dynamic programming strikes back.pdf;/Users/cperivol/Zotero/storage/KN566UIF/1376616.html}
}

@article{morihataIncrementalComputingData2018,
  title = {Incremental Computing with Data Structures},
  author = {Morihata, Akimasa},
  date = {2018-10-15},
  journaltitle = {Science of Computer Programming},
  series = {Special Issue of Selected Papers from {{FLOPS}} 2016},
  volume = {164},
  pages = {18--36},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2017.04.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0167642317300692},
  urldate = {2021-02-25},
  abstract = {Incremental computing is a method of maintaining consistency between an input and output. If only a small portion of the input is modified, it is natural to expect that the corresponding output can be obtained more efficiently than full re-computation. However, for nontrivial data structures, such as self-balancing binary search trees, even the most primitive modifications may lead to drastic change of the underlying structure. In this paper, we develop an method of incremental computing on data structures that may consist of complex modifications. The key idea is to use shortcut fusion in order to decompose a complex modification to a series of simple ones. Based on this idea, we extend Jeuring's incremental computing method on algebraic data structures to one on more complex data structures. The method is purely functional and does not rely on any run-time support. Its correctness is straightforward from parametricity. Moreover, its cost is often proportional to that of the corresponding modification.},
  langid = {english},
  keywords = {Data structures,Datatype-generic programming,Incremental computing,Shortcut fusion},
  file = {/Users/cperivol/Zotero/storage/V9SZEY4T/S0167642317300692.html}
}

@article{morihataIncrementalComputingData2018a,
  title = {Incremental Computing with Data Structures},
  author = {Morihata, Akimasa},
  date = {2018-10-15},
  journaltitle = {Science of Computer Programming},
  series = {Special Issue of Selected Papers from {{FLOPS}} 2016},
  volume = {164},
  pages = {18--36},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2017.04.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0167642317300692},
  urldate = {2021-07-08},
  abstract = {Incremental computing is a method of maintaining consistency between an input and output. If only a small portion of the input is modified, it is natural to expect that the corresponding output can be obtained more efficiently than full re-computation. However, for nontrivial data structures, such as self-balancing binary search trees, even the most primitive modifications may lead to drastic change of the underlying structure. In this paper, we develop an method of incremental computing on data structures that may consist of complex modifications. The key idea is to use shortcut fusion in order to decompose a complex modification to a series of simple ones. Based on this idea, we extend Jeuring's incremental computing method on algebraic data structures to one on more complex data structures. The method is purely functional and does not rely on any run-time support. Its correctness is straightforward from parametricity. Moreover, its cost is often proportional to that of the corresponding modification.},
  langid = {english},
  keywords = {Data structures,Datatype-generic programming,Incremental computing,Shortcut fusion},
  file = {/Users/cperivol/Zotero/storage/F4EG3CU9/S0167642317300692.html}
}

@inproceedings{mullerMemoryEfficientKeyForeignKey2021,
  title = {Memory-{{Efficient Key}}/{{Foreign}}-{{Key Join Size Estimation}} via {{Multiplicity}} and {{Intersection Size}}},
  booktitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Müller, Magnus and Flachs, Daniel and Moerkotte, Guido},
  date = {2021-04},
  pages = {984--995},
  issn = {2375-026X},
  doi = {10.1109/ICDE51399.2021.00090},
  abstract = {Join size estimation plays a crucial role in query optimization. In this paper, we present a technique to estimate the size of a key/foreign-key join of two filtered relations. We build on a model by Allen Van Gelder, in which there is no notion of join selectivity. Instead, the size of a join is estimated as a multiple of the intersection size of the join attributes. We present both a data structure to approximate the number of distinct values in a join attribute after a filter operation, and formulas to estimate the factor by which a join size exceeds the intersection size. In addition, we evaluate three existing intersection size estimation methods that are based on HyperLogLog sketches, to which our approach is closely linked. For both real-world and generated data sets, our estimator competes well, in terms of accuracy and memory footprint, against several industry-strength and state-of-the-art join size estimation methods. In particular, our experiments indicate that our approach is less prone to heavy underestimates.},
  eventtitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  keywords = {Cardinality Estimation,Conferences,Data engineering,Data structures,Estimation,Join Size Estimation,Query processing},
  file = {/Users/cperivol/Zotero/storage/WKZCPLXF/9458862.html}
}

@article{munirIntermediateResultsMaterialization2018,
  title = {Intermediate Results Materialization Selection and Format for Data-Intensive Flows},
  author = {Munir, Rana Faisal and Nadal, Sergi and Romero, Oscar and Abelló, Alberto and Jovanovic, Petar and Thiele, Maik and Lehner, Wolfgang},
  date = {2018},
  journaltitle = {Fundamenta Informaticae},
  volume = {163},
  number = {2},
  pages = {111--138},
  publisher = {{IOS Press}}
}

@inproceedings{munirResilientstoreHeuristicbasedData2016,
  title = {Resilientstore: A Heuristic-Based Data Format Selector for Intermediate Results},
  shorttitle = {Resilientstore},
  booktitle = {International {{Conference}} on {{Model}} and {{Data Engineering}}},
  author = {Munir, Rana Faisal and Romero, Oscar and Abelló, Alberto and Bilalli, Besim and Thiele, Maik and Lehner, Wolfgang},
  date = {2016},
  pages = {42--56},
  publisher = {{Springer}},
  file = {/Users/cperivol/Zotero/storage/5XB2K7G6/Munir et al. - 2016 - Resilientstore A heuristic-based data format sele.pdf;/Users/cperivol/Zotero/storage/EEGP8CH5/978-3-319-45547-1_4.html}
}

@article{munirStorageFormatSelection2019,
  title = {Storage Format Selection and Optimization for Materialized Intermediate Results in Data-Intensive Flows},
  author = {Munir, Rana Faisal},
  date = {2019},
  publisher = {{Universitat Politècnica de Catalunya}},
  file = {/Users/cperivol/Zotero/storage/HBC4MMCS/Munir - 2019 - Storage format selection and optimization for mate.pdf;/Users/cperivol/Zotero/storage/F6JUIY84/177247.html}
}

@article{nagelEfficientQueryProcessing2015,
  title = {Efficient Query Processing in Managed Runtimes},
  author = {Nagel, Fabian Oliver},
  date = {2015-11-26},
  publisher = {{The University of Edinburgh}},
  url = {https://era.ed.ac.uk/handle/1842/15869},
  urldate = {2021-11-24},
  abstract = {This thesis presents strategies to improve the query evaluation performance over  huge volumes of relational-like data that is stored in the memory space of managed  applications. Storing and processing application data in the memory space of managed  applications is motivated by the convergence of two recent trends in data management.  First, dropping DRAM prices have led to memory capacities that allow the entire working  set of an application to fit into main memory and to the emergence of in-memory  database systems (IMDBs). Second, language-integrated query transparently integrates  query processing syntax into programming languages and, therefore, allows complex  queries to be composed in the application. IMDBs typically serve as data stores to applications  written in an object-oriented language running on a managed runtime. In  this thesis, we propose a deeper integration of the two by storing all application data in  the memory space of the application and using language-integrated query, combined  with query compilation techniques, to provide fast query processing.  As a starting point, we look into storing data as runtime-managed objects in collection  types provided by the programming language. Queries are formulated using  language-integrated query and dynamically compiled to specialized functions that produce  the result of the query in a more efficient way by leveraging query compilation  techniques similar to those used in modern database systems. We show that the generated  query functions significantly improve query processing performance compared to  the default execution model for language-integrated query. However, we also identify  additional inefficiencies that can only be addressed by processing queries using low-level  techniques which cannot be applied to runtime-managed objects. To address this,  we introduce a staging phase in the generated code that makes query-relevant managed  data accessible to low-level query code. Our experiments in .NET show an improvement  in query evaluation performance of up to an order of magnitude over the default  language-integrated query implementation.  Motivated by additional inefficiencies caused by automatic garbage collection, we  introduce a new collection type, the black-box collection. Black-box collections integrate  the in-memory storage layer of a relational database system to store data and hide  the internal storage layout from the application by employing existing object-relational  mapping techniques (hence, the name black-box). Our experiments show that black-box  collections provide better query performance than runtime-managed collections  by allowing the generated query code to directly access the underlying relational in-memory  data store using low-level techniques. Black-box collections also outperform  a modern commercial database system. By removing huge volumes of collection data  from the managed heap, black-box collections further improve the overall performance  and response time of the application and improve the application’s scalability when  facing huge volumes of collection data.  To enable a deeper integration of the data store with the application, we introduce  self-managed collections. Self-managed collections are a new type of collection for  managed applications that, in contrast to black-box collections, store objects. As the  data elements stored in the collection are objects, they are directly accessible from the  application using references which allows for better integration of the data store with  the application. Self-managed collections manually manage the memory of objects  stored within them in a private heap that is excluded from garbage collection. We introduce  a special collection syntax and a novel type-safe manual memory management  system for this purpose. As was the case for black-box collections, self-managed collections  improve query performance by utilizing a database-inspired data layout and  allowing the use of low-level techniques. By also supporting references between collection  objects, they outperform black-box collections.},
  langid = {english},
  annotation = {Accepted: 2016-06-15T08:50:03Z},
  file = {/Users/cperivol/Zotero/storage/4E8IFYNU/Nagel - 2015 - Efficient query processing in managed runtimes.pdf;/Users/cperivol/Zotero/storage/GYUDPCV3/15869.html}
}

@inproceedings{nagelRecyclingPipelinedQuery2013,
  title = {Recycling in Pipelined Query Evaluation},
  booktitle = {2013 {{IEEE}} 29th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Nagel, Fabian and Boncz, Peter and Viglas, Stratis D.},
  date = {2013},
  pages = {338--349},
  publisher = {{IEEE}},
  file = {/Users/cperivol/Zotero/storage/D5YEVTFX/6544837.html}
}

@inproceedings{nagelRecyclingPipelinedQuery2013a,
  title = {Recycling in Pipelined Query Evaluation},
  booktitle = {2013 {{IEEE}} 29th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Nagel, Fabian and Boncz, Peter and Viglas, Stratis D.},
  date = {2013},
  pages = {338--349},
  publisher = {{IEEE}},
  file = {/Users/cperivol/Zotero/storage/49H9PPRU/6544837.html}
}

@inproceedings{nagelRecyclingPipelinedQuery2013b,
  title = {Recycling in Pipelined Query Evaluation},
  booktitle = {2013 {{IEEE}} 29th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Nagel, Fabian and Boncz, Peter and Viglas, Stratis D.},
  date = {2013},
  pages = {338--349},
  publisher = {{IEEE}}
}

@report{naishPruningLogicProgramming1995,
  title = {Pruning in {{Logic Programming}}},
  author = {Naish, Lee},
  date = {1995},
  institution = {{University of Melbourne}},
  abstract = {The logic programming community has a love--hate relationship with operators for pruning the search space of logic programs such as cut, commit, once, conditionals and variations on these. Pruning operators typically are not declarative, result in incompleteness and/or unsoundness, decrease readability and flexibility of code and make program analysis and transformation more difficult. Despite this, nearly all non-trivial Prolog programs contain cuts, nearly all more recent logic programming languages have similar pruning operators and many languages insist on pruning operators in every clause. In practice, logic programming is less logical than functional programming. Why it this so? Do we really need pruning operators? Can we have sufficiently powerful pruning operators which do not destroy the declarative semantics of programs? How are pruning operators related to logic, modes, functions and lazy evaluation? This paper attempts to answer some of these questions. Keywords: cut, soft ...},
  file = {/Users/cperivol/Zotero/storage/9SWFF8HL/Naish - 1995 - Pruning in Logic Programming.pdf;/Users/cperivol/Zotero/storage/EP55KPSN/summary.html}
}

@report{naishPruningLogicProgramming1995a,
  title = {Pruning in {{Logic Programming}}},
  author = {Naish, Lee},
  date = {1995},
  institution = {{University of Melbourne}},
  abstract = {The logic programming community has a love--hate relationship with operators for pruning the search space of logic programs such as cut, commit, once, conditionals and variations on these. Pruning operators typically are not declarative, result in incompleteness and/or unsoundness, decrease readability and flexibility of code and make program analysis and transformation more difficult. Despite this, nearly all non-trivial Prolog programs contain cuts, nearly all more recent logic programming languages have similar pruning operators and many languages insist on pruning operators in every clause. In practice, logic programming is less logical than functional programming. Why it this so? Do we really need pruning operators? Can we have sufficiently powerful pruning operators which do not destroy the declarative semantics of programs? How are pruning operators related to logic, modes, functions and lazy evaluation? This paper attempts to answer some of these questions. Keywords: cut, soft ...},
  file = {/Users/cperivol/Zotero/storage/DVR3W5Z4/Naish - 1995 - Pruning in Logic Programming.pdf;/Users/cperivol/Zotero/storage/BGUISWCL/summary.html}
}

@article{neumannEfficientlyCompilingEfficient2011,
  title = {Efficiently Compiling Efficient Query Plans for Modern Hardware},
  author = {Neumann, Thomas},
  date = {2011},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {4},
  number = {9},
  pages = {539--550},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/A4XZVNDX/Neumann - 2011 - Efficiently compiling efficient query plans for mo.pdf;/Users/cperivol/Zotero/storage/T8KCLIZC/2002938.html}
}

@article{neumannEvolutionCompilingQuery2021,
  title = {Evolution of a Compiling Query Engine},
  author = {Neumann, Thomas},
  date = {2021-07-01},
  journaltitle = {Proc. VLDB Endow.},
  volume = {14},
  number = {12},
  pages = {3207--3210},
  issn = {2150-8097},
  doi = {10.14778/3476311.3476410},
  url = {https://doi.org/10.14778/3476311.3476410},
  urldate = {2021-11-28},
  abstract = {In 2011 we showed how to use dynamic code generation to process queries in a data-centric manner. This execution model can produce compact and efficient code and was successfully used by both our own systems and systems of other groups. As the systems become used in practice, additional techniques were developed for shortcomings that did arrive, including low-latency compilation, multi-threading support, and others. This paper gives an overview of the evolution of our query engine within in the last ten years, and points out which problem have to be tackled to bring a compiling system into production usage.},
  annotation = {00000},
  file = {/Users/cperivol/Zotero/storage/ETH65GEJ/Neumann - 2021 - Evolution of a compiling query engine.pdf}
}

@online{neumannEvolutionCompilingQueryEngine,
  title = {Evolution of a {{CompilingQuery Engine}}},
  author = {Neumann, Thomas},
  url = {https://www.semanticscholar.org/paper/Evolution-of-a-CompilingQuery-Engine-Neumann/2cd16bb2d3680d04948f4536270c36d98fd4e768},
  urldate = {2021-11-28},
  abstract = {In 2011 we showed how to use dynamic code generation to process queries in a data-centric manner. This execution model can produce compact and efficient code and was successfully used by both our own systems and systems of other groups. As the systems become used in practice, additional techniques were developed for shortcomings that did arrive, including low-latency compilation, multi-threading support, and others. This paper gives an overview of the evolution of our query engine within in the last ten years, and points out which problem have to be tackled to bring a compiling system into production usage. PVLDB Reference Format: Thomas Neumann. Evolution of a Compiling Query Engine. PVLDB, 14(12):},
  langid = {english},
  annotation = {00000},
  file = {/Users/cperivol/Zotero/storage/4NJWQCBZ/2cd16bb2d3680d04948f4536270c36d98fd4e768.html}
}

@incollection{nunesalonsoBuildingPolyglotData2020,
  title = {Building a {{Polyglot Data Access Layer}} for a {{Low}}-{{Code Application Development Platform}}: ({{Experience Report}})},
  shorttitle = {Building a {{Polyglot Data Access Layer}} for a {{Low}}-{{Code Application Development Platform}}},
  booktitle = {Distributed {{Applications}} and {{Interoperable Systems}}},
  author = {Nunes Alonso, Ana and Abreu, João and Nunes, David and Vieira, André and Santos, Luiz and Soares, Tércio and Pereira, José},
  editor = {Remke, Anne and Schiavoni, Valerio},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12135},
  pages = {95--103},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-50323-9_6},
  url = {http://link.springer.com/10.1007/978-3-030-50323-9_6},
  urldate = {2021-11-28},
  abstract = {Low-code application development as proposed by the OutSystems Platform enables fast mobile and desktop application development and deployment. It hinges on visual development of the interface and business logic but also on easy integration with data stores and services while delivering robust applications that scale.},
  isbn = {978-3-030-50322-2 978-3-030-50323-9},
  langid = {english},
  annotation = {00000},
  file = {/Users/cperivol/Zotero/storage/3LVU8ZGH/Nunes Alonso et al. - 2020 - Building a Polyglot Data Access Layer for a Low-Co.pdf}
}

@article{nykielMRShareSharingMultiple2010,
  title = {{{MRShare}}: Sharing across Multiple Queries in {{MapReduce}}},
  shorttitle = {{{MRShare}}},
  author = {Nykiel, Tomasz and Potamias, Michalis and Mishra, Chaitanya and Kollios, George and Koudas, Nick},
  date = {2010-09-01},
  journaltitle = {Proc. VLDB Endow.},
  volume = {3},
  number = {1-2},
  pages = {494--505},
  issn = {2150-8097},
  doi = {10.14778/1920841.1920906},
  url = {https://doi.org/10.14778/1920841.1920906},
  urldate = {2021-11-26},
  abstract = {Large-scale data analysis lies in the core of modern enterprises and scientific research. With the emergence of cloud computing, the use of an analytical query processing infrastructure (e.g., Amazon EC2) can be directly mapped to monetary value. MapReduce has been a popular framework in the context of cloud computing, designed to serve long running queries (jobs) which can be processed in batch mode. Taking into account that different jobs often perform similar work, there are many opportunities for sharing. In principle, sharing similar work reduces the overall amount of work, which can lead to reducing monetary charges incurred while utilizing the processing infrastructure. In this paper we propose a sharing framework tailored to MapReduce. Our framework, MRShare, transforms a batch of queries into a new batch that will be executed more efficiently, by merging jobs into groups and evaluating each group as a single query. Based on our cost model for MapReduce, we define an optimization problem and we provide a solution that derives the optimal grouping of queries. Experiments in our prototype, built on top of Hadoop, demonstrate the overall effectiveness of our approach and substantial savings.},
  annotation = {00350}
}

@article{nykielSharingMultipleMapReduce2014,
  title = {Sharing across Multiple {{MapReduce}} Jobs},
  author = {Nykiel, Tomasz and Potamias, Michalis and Mishra, Chaitanya and Kollios, George and Koudas, Nick},
  date = {2014},
  journaltitle = {ACM Transactions on Database Systems (TODS)},
  volume = {39},
  number = {2},
  pages = {1--46},
  publisher = {{ACM New York, NY, USA}},
  file = {/Users/cperivol/Zotero/storage/3IQ2ZN5L/2560796.html}
}

@article{oneilStarSchemaBased,
  title = {1. {{Star Schema Based}} on {{TPC}}-{{H}}},
  author = {O'Neil, Pat and O'Neil, Betty and Chen, Xuedong},
  pages = {10},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/9QVKQ5XJ/O'Neil et al. - 1. Star Schema Based on TPC-H.pdf}
}

@online{ortizEmpiricalAnalysisDeep2019,
  title = {An {{Empirical Analysis}} of {{Deep Learning}} for {{Cardinality Estimation}}},
  author = {Ortiz, Jennifer and Balazinska, Magdalena and Gehrke, Johannes and Keerthi, S. Sathiya},
  date = {2019-09-11},
  eprint = {1905.06425},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1905.06425},
  urldate = {2021-11-28},
  abstract = {We implement and evaluate deep learning for cardinality estimation by studying the accuracy, space and time trade-offs across several architectures. We find that simple deep learning models can learn cardinality estimations across a variety of datasets (reducing the error by 72\% - 98\% on average compared to PostgreSQL). In addition, we empirically evaluate the impact of injecting cardinality estimates produced by deep learning models into the PostgreSQL optimizer. In many cases, the estimates from these models lead to better query plans across all datasets, reducing the runtimes by up to 49\% on select-project-join workloads. As promising as these models are, we also discuss and address some of the challenges of using them in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases},
  annotation = {00038},
  file = {/Users/cperivol/Zotero/storage/EDEVTS3V/Ortiz et al. - 2019 - An Empirical Analysis of Deep Learning for Cardina.pdf;/Users/cperivol/Zotero/storage/EW3GMXSE/1905.html}
}

@book{perezHistoryawareQueryOptimization,
  title = {History-Aware Query Optimization with Materialized Intermediate Views},
  author = {Perez, Luis L. and Jermaine, Christopher M.},
  journaltitle = {2014 IEEE 30th International Conference on Data Engineering},
  doi = {10.1109/ICDE.2014.6816678},
  url = {https://www.infona.pl//resource/bwmeta1.element.ieee-art-000006816678},
  urldate = {2021-10-16},
  abstract = {The use of materialized views derived from the intermediate results of frequently executed queries is a popular strategy for improving performance in query workloads. Optimizers capable of matching such views with inbound queries can generate alternative execution plans that read the materialized contents directly instead of re-computing the corresponding subqueries, which tends to result in reduced query execution times. In this paper, we introduce an architecture called Hawc that extends a cost-based logical optimizer with the capability to use history information to identify query plans that, if executed, produce intermediate result sets that can be used to create materialized views with the potential to reduce the execution time of future queries. We present techniques for using knowledge of past queries to assist the query optimizer and match, generate and select useful materialized views. Experimental results indicate that these techniques provide substantial improvements in workload execution time.},
  isbn = {978-1-4799-2555-1},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/YW3JQ46G/bwmeta1.element.html}
}

@inproceedings{perezHistoryawareQueryOptimization2014,
  title = {History-Aware Query Optimization with Materialized Intermediate Views},
  booktitle = {2014 {{IEEE}} 30th {{International Conference}} on {{Data Engineering}}},
  author = {Perez, Luis L. and Jermaine, Christopher M.},
  date = {2014-03},
  pages = {520--531},
  issn = {2375-026X},
  doi = {10.1109/ICDE.2014.6816678},
  abstract = {The use of materialized views derived from the intermediate results of frequently executed queries is a popular strategy for improving performance in query workloads. Optimizers capable of matching such views with inbound queries can generate alternative execution plans that read the materialized contents directly instead of re-computing the corresponding subqueries, which tends to result in reduced query execution times. In this paper, we introduce an architecture called Hawc that extends a cost-based logical optimizer with the capability to use history information to identify query plans that, if executed, produce intermediate result sets that can be used to create materialized views with the potential to reduce the execution time of future queries. We present techniques for using knowledge of past queries to assist the query optimizer and match, generate and select useful materialized views. Experimental results indicate that these techniques provide substantial improvements in workload execution time.},
  eventtitle = {2014 {{IEEE}} 30th {{International Conference}} on {{Data Engineering}}},
  keywords = {Costing,History,Optimization,Query processing,Vectors},
  file = {/Users/cperivol/Zotero/storage/BRGX4L7M/Perez and Jermaine - 2014 - History-aware query optimization with materialized.pdf}
}

@inproceedings{perezHistoryawareQueryOptimization2014a,
  title = {History-Aware Query Optimization with Materialized Intermediate Views},
  booktitle = {2014 {{IEEE}} 30th {{International Conference}} on {{Data Engineering}}},
  author = {Perez, Luis L. and Jermaine, Christopher M.},
  date = {2014},
  pages = {520--531},
  publisher = {{IEEE}},
  file = {/Users/cperivol/Zotero/storage/GUL6LAQN/6816678.html}
}

@software{perivolaropoulosFakedrakeSsbdbgen2021,
  title = {Fakedrake/Ssb-Dbgen},
  author = {Perivolaropoulos, Chris},
  date = {2021-09-28T16:22:05Z},
  origdate = {2021-06-22T14:16:13Z},
  url = {https://github.com/fakedrake/ssb-dbgen},
  urldate = {2021-10-28},
  abstract = {Star Schema Benchmark dbgen}
}

@software{perivolaropoulosFakedrakeSsbdbgen2021a,
  title = {Fakedrake/Ssb-Dbgen},
  author = {Perivolaropoulos, Chris},
  date = {2021-09-28T16:22:05Z},
  origdate = {2021-06-22T14:16:13Z},
  url = {https://github.com/fakedrake/ssb-dbgen},
  urldate = {2021-11-15},
  abstract = {Star Schema Benchmark dbgen}
}

@online{PropagatorsEdwardKmett,
  title = {Propagators | {{Edward Kmett}} | {{YOW}}! {{Lambda Jam}} 2016 | {{YOW}}! {{Conferences}}},
  url = {https://yowconference.com/},
  urldate = {2021-10-19},
  abstract = {{$<$}p{$>$}There are a lot of algorithms that revolve around iterating a form of information propagation until it attains a deterministic fixed point. CRDTs, Datalog, SAT solving, functional reactive programming, and constraint programming all fit into this mold.{$<$}/p{$>$}  {$<$}p{$>$}One framework for these sorts of algorithms is the notion of a \&ldquo;propagator\&rdquo; due to Sussman and Radul, but until now little rigor has applied to know how such algorithms terminate with consistent results. Another framework is Lindsey Kuper\&rsquo;s work on the notion of \&ldquo;lattice variables\&rdquo; (LVars), which addresses termination, parallelism and eventual consistency well, but not iteration.{$<$}/p{$>$}  {$<$}p{$>$}By blending these frameworks, I\&rsquo;ll build up a series of sufficient conditions for propagators to terminate with consistent results and proceed to show how we can use this common framework to steal insights and quirks from each individual domain to try to optimize the rest.{$<$}/p{$>$}  {$<$}p{$>$} {$<$}/p{$>$}},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/7Y9SS2W4/propagators-12531.html}
}

@inproceedings{pughIncrementalComputationFunction1989,
  title = {Incremental Computation via Function Caching},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN}}-{{SIGACT}} Symposium on {{Principles}} of Programming Languages  - {{POPL}} '89},
  author = {Pugh, W. and Teitelbaum, T.},
  date = {1989},
  pages = {315--328},
  publisher = {{ACM Press}},
  location = {{Austin, Texas, United States}},
  doi = {10.1145/75277.75305},
  url = {http://portal.acm.org/citation.cfm?doid=75277.75305},
  urldate = {2021-10-10},
  eventtitle = {The 16th {{ACM SIGPLAN}}-{{SIGACT}} Symposium},
  isbn = {978-0-89791-294-5},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/9WRC7N9X/Pugh and Teitelbaum - 1989 - Incremental computation via function caching.pdf}
}

@online{QueryOptimizationOverview,
  title = {Query {{Optimization}} - an Overview | {{ScienceDirect Topics}}},
  url = {https://www.sciencedirect.com/topics/computer-science/query-optimization},
  urldate = {2021-11-26},
  file = {/Users/cperivol/Zotero/storage/99WQ5532/query-optimization.html}
}

@inproceedings{raasveldtDuckDBEmbeddableAnalytical2019,
  title = {{{DuckDB}}: An {{Embeddable Analytical Database}}},
  shorttitle = {{{DuckDB}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Raasveldt, Mark and Mühleisen, Hannes},
  date = {2019-06-25},
  pages = {1981--1984},
  publisher = {{ACM}},
  location = {{Amsterdam Netherlands}},
  doi = {10.1145/3299869.3320212},
  url = {https://dl.acm.org/doi/10.1145/3299869.3320212},
  urldate = {2021-11-25},
  abstract = {The immense popularity of SQLite shows that there is a need for unobtrusive in-process data management solutions. However, there is no such system yet geared towards analytical workloads. We demonstrate DuckDB, a novel data management system designed to execute analytical SQL queries while embedded in another process. In our demonstration, we pit DuckDB against other data management solutions to showcase its performance in the embedded analytics scenario. DuckDB is available as Open Source software under a permissive license.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '19: International {{Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-5643-5},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/JY53N55K/Raasveldt and Mühleisen - 2019 - DuckDB an Embeddable Analytical Database.pdf}
}

@inproceedings{raasveldtDuckdbEmbeddableAnalytical2019,
  title = {Duckdb: An Embeddable Analytical Database},
  shorttitle = {Duckdb},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Raasveldt, Mark and Mühleisen, Hannes},
  date = {2019},
  pages = {1981--1984},
  file = {/Users/cperivol/Zotero/storage/TQJTUQVK/Raasveldt and Mühleisen - 2019 - Duckdb an embeddable analytical database.pdf;/Users/cperivol/Zotero/storage/9DF6XYQR/3299869.html}
}

@article{radulPropagationNetworksFlexible2009,
  title = {Propagation {{Networks}}: A {{Flexible}} and {{Expressive Substrate}} for {{Computation}}},
  shorttitle = {Propagation {{Networks}}},
  author = {Radul, Alexey Andreyevich},
  date = {2009},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.150.2893},
  urldate = {2021-03-16},
  abstract = {CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep Teregowda): In this dissertation I propose a shift in the foundations of computation. Modern programming systems are not expressive enough. The traditional image of a single computer that has global effects on a large memory is too restrictive. The propagation paradigm replaces this with computing by networks of local, independent, stateless machines interconnected with stateful storage cells. In so doing, it offers great flexibility and expressive power, and has therefore been much studied, but has not yet been tamed for general-purpose computation. The novel insight that should finally permit computing with general-purpose propagation is that a cell should not be seen as storing a value, but as accumulating information about a value. Various forms of the general idea of propagation have been used with great success for various special purposes; perhaps the most immediate example is constraint propagation in constraint satisfaction systems. This success is evidence both},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/7GTARLHJ/summary.html}
}

@article{radulPropagationNetworksFlexible2009a,
  title = {Propagation {{Networks}}: A {{Flexible}} and {{Expressive Substrate}} for {{Computation}}},
  shorttitle = {Propagation {{Networks}}},
  author = {Radul, Alexey Andreyevich},
  date = {2009},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.150.2893},
  urldate = {2021-03-16},
  abstract = {CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep Teregowda): In this dissertation I propose a shift in the foundations of computation. Modern programming systems are not expressive enough. The traditional image of a single computer that has global effects on a large memory is too restrictive. The propagation paradigm replaces this with computing by networks of local, independent, stateless machines interconnected with stateful storage cells. In so doing, it offers great flexibility and expressive power, and has therefore been much studied, but has not yet been tamed for general-purpose computation. The novel insight that should finally permit computing with general-purpose propagation is that a cell should not be seen as storing a value, but as accumulating information about a value. Various forms of the general idea of propagation have been used with great success for various special purposes; perhaps the most immediate example is constraint propagation in constraint satisfaction systems. This success is evidence both},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/69LK5ACA/summary.html}
}

@article{rivasUnifiedViewMonadic2018,
  title = {A Unified View of Monadic and Applicative Non-Determinism},
  author = {Rivas, Exequiel and Jaskelioff, Mauro and Schrijvers, Tom},
  date = {2018-01-15},
  journaltitle = {Science of Computer Programming},
  volume = {152},
  pages = {70--98},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2017.09.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0167642317301958},
  urldate = {2021-10-21},
  abstract = {It is well-known that monads are monoids in the category of endofunctors, and in fact so are applicative functors. Unfortunately, monoids do not have enough structure to account for computational effects with non-determinism operators. This article recovers a unified view of computational effects with non-determinism by extending monoids to near-semirings with both additive and multiplicative structure. This enables us to generically define free constructions as well as a novel double Cayley representation that optimises both left-nested sums and left-nested products.},
  langid = {english},
  keywords = {Alternative,Free construction,Monadplus,Monoid,Near-semiring},
  file = {/Users/cperivol/Zotero/storage/W2KVB69G/Rivas et al. - 2018 - A unified view of monadic and applicative non-dete.pdf}
}

@patent{rogersMultiqueryOptimization2017,
  type = {patent},
  title = {Multi-Query Optimization},
  author = {Rogers, William Arthur and Underbrink, Joseph C. and Mcdaniel, Jason Tyler and Zirojevic, Srdan and Holler, Wesley A.},
  date = {2017-03-23},
  publisher = {{Google Patents}},
  file = {/Users/cperivol/Zotero/storage/4VPCKWHZ/Rogers et al. - 2017 - Multi-query optimization.pdf;/Users/cperivol/Zotero/storage/SNL4ZYF7/en.html}
}

@online{royDonTrashYour2000,
  title = {Don't Trash Your Intermediate Results, Cache'em},
  author = {Roy, Prasan and Ramamritham, Krithi and Seshadri, S. and Shenoy, Pradeep and Sudarshan, S.},
  date = {2000},
  eprint = {cs/0003005},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/cperivol/Zotero/storage/Q2ICX5PZ/Roy et al. - 2000 - Don't trash your intermediate results, cache'em.pdf;/Users/cperivol/Zotero/storage/2UNBCYLZ/0003005.html}
}

@inproceedings{royEfficientExtensibleAlgorithms2000,
  title = {Efficient and Extensible Algorithms for Multi Query Optimization},
  booktitle = {Proceedings of the 2000 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Roy, Prasan and Seshadri, Srinivasan and Sudarshan, S. and Bhobe, Siddhesh},
  date = {2000},
  pages = {249--260},
  file = {/Users/cperivol/Zotero/storage/W3QLEFN3/Roy et al. - 2000 - Efficient and extensible algorithms for multi quer.pdf;/Users/cperivol/Zotero/storage/GXKD5IKH/342009.html}
}

@article{roySparkcruiseHandsfreeComputation2019,
  title = {Sparkcruise: Handsfree Computation Reuse in Spark},
  shorttitle = {Sparkcruise},
  author = {Roy, Abhishek and Jindal, Alekh and Patel, Hiren and Gosalia, Ashit and Krishnan, Subru and Curino, Carlo},
  date = {2019},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {12},
  pages = {1850--1853},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/WQZ7JQVZ/Roy et al. - 2019 - Sparkcruise Handsfree computation reuse in spark.pdf;/Users/cperivol/Zotero/storage/VUF9MEDL/3352063.html}
}

@article{sagivEquivalencesRelationalExpressions1980,
  title = {Equivalences {{Among Relational Expressions}} with the {{Union}} and {{Difference Operators}}},
  author = {Sagiv, Yehoshua and Yannakakis, Mihalis},
  date = {1980-10},
  journaltitle = {J. ACM},
  volume = {27},
  number = {4},
  pages = {633--655},
  issn = {0004-5411, 1557-735X},
  doi = {10.1145/322217.322221},
  url = {https://dl.acm.org/doi/10.1145/322217.322221},
  urldate = {2021-11-15},
  abstract = {In this case replace any distinguished vanable m that column with c. (b) The distinguished symbol a ff (a) does not apply, but one or both of Ta and T2 have a in that column's summary. (c) Blank, otherwise.},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/STMXZSZJ/Sagiv and Yannakakis - 1980 - Equivalences Among Relational Expressions with the.pdf}
}

@article{sagivEquivalencesRelationalExpressions1980a,
  title = {Equivalences among Relational Expressions with the Union and Difference Operators},
  author = {Sagiv, Yehoshua and Yannakakis, Mihalis},
  date = {1980},
  journaltitle = {Journal of the ACM (JACM)},
  volume = {27},
  number = {4},
  pages = {633--655},
  publisher = {{ACM New York, NY, USA}},
  file = {/Users/cperivol/Zotero/storage/P88KI6RB/Sagiv and Yannakakis - 1980 - Equivalences among relational expressions with the.pdf}
}

@article{sahalComparativeStudyMultiquery2016,
  title = {Comparative Study of Multi-Query Optimization Techniques Using Shared Predicate-Based for Big Data},
  author = {Sahal, Radhya and Khafagy, Mohamed H. and Omara, Fatma A.},
  date = {2016},
  journaltitle = {International Journal of Grid and Distributed Computing},
  volume = {9},
  number = {5},
  pages = {229--240},
  file = {/Users/cperivol/Zotero/storage/QM5GW4MQ/Sahal et al. - 2016 - Comparative study of multi-query optimization tech.pdf}
}

@article{schaarschmidtAutomatedPolyglotPersistence2015,
  title = {Towards Automated Polyglot Persistence},
  author = {Schaarschmidt, Michael and Gessert, Felix and Ritter, Norbert},
  date = {2015},
  journaltitle = {Datenbanksysteme für Business, Technologie und Web (BTW 2015)},
  publisher = {{Gesellschaft für Informatik eV}},
  file = {/Users/cperivol/Zotero/storage/YRWD7E3R/Schaarschmidt et al. - 2015 - Towards automated polyglot persistence.pdf;/Users/cperivol/Zotero/storage/65MGR2N6/2452.html}
}

@article{sellisMultipleQueryOptimization1987,
  title = {Multiple-{{Query Optimization}}.},
  author = {Sellis, T. K.},
  date = {1987},
  url = {https://drum.lib.umd.edu/handle/1903/4543},
  urldate = {2021-11-26},
  abstract = {Some recently proposed extensions to relational database systems as well as deductive database systems, require support for processing multiple queries. For example, in a database system enhanced with inference capabilities, a simple query involving a rule with multiple definitions, may expand to more than one actual query that has to be run over the database. It is an interesting problem then, to come up with algorithms that process these queries together instead of one query at a time. The main motivation for performing such an interquery optimization lies in the fact that queries may share common data. We examine the problem of multiple-query optimization in this paper. The first major contribution of this paper is a systematic look at the problem and the presentation and analysis of algorithms that can be used for multiple query optimization. The second contribution lies in the presentation of experimental results. Our results show that using multiple-query processing algorithms may reduce execution cost considerably.},
  langid = {american},
  annotation = {00000  Accepted: 2007-05-23T09:36:49Z},
  file = {/Users/cperivol/Zotero/storage/DYNK8T5R/Sellis - 1987 - Multiple-Query Optimization..pdf;/Users/cperivol/Zotero/storage/JEUU8L5K/4543.html}
}

@article{selsamSealingPointerbasedOptimizations2020,
  title = {Sealing Pointer-Based Optimizations behind Pure Functions},
  author = {Selsam, Daniel and Hudon, Simon and de Moura, Leonardo},
  options = {useprefix=true},
  date = {2020-08-02},
  journaltitle = {Proc. ACM Program. Lang.},
  volume = {4},
  pages = {115:1--115:20},
  doi = {10.1145/3408997},
  url = {https://doi.org/10.1145/3408997},
  urldate = {2021-10-17},
  abstract = {Functional programming languages are particularly well-suited for building automated reasoning systems, since (among other reasons) a logical term is well modeled by an inductive type, traversing a term can be implemented generically as a higher-order combinator, and backtracking search is dramatically simplified by persistent datastructures. However, existing pure functional programming languages all suffer a major limitation in these domains: traversing a term requires time proportional to the tree size of the term as opposed to its graph size. This limitation would be particularly devastating when building automation for interactive theorem provers such as Lean and Coq, for which the exponential blowup of term-tree sizes has proved to be both common and difficult to prevent. All that is needed to recover the optimal scaling is the ability to perform simple operations on the memory addresses of terms, and yet allowing these operations to be used freely would clearly violate the basic premise of referential transparency. We show how to use dependent types to seal the necessary pointer-address manipulations behind pure functional interfaces while requiring only a negligible amount of additional trust. We have implemented our approach for the upcoming version (v4) of Lean, and our approach could be adopted by other languages based on dependent type theory as well.},
  issue = {ICFP},
  keywords = {functional programming,interactive theorem proving,Lean},
  file = {/Users/cperivol/Zotero/storage/D7A3SRYT/Selsam et al. - 2020 - Sealing pointer-based optimizations behind pure fu.pdf}
}

@article{shaikhhaBuildingEfficientQuery2018,
  title = {Building {{Efficient Query Engines}} in a {{High}}-{{Level Language}}},
  author = {Shaikhha, Amir and Klonatos, Yannis and Koch, Christoph},
  date = {2018-04-11},
  journaltitle = {ACM Trans. Database Syst.},
  volume = {43},
  number = {1},
  pages = {4:1--4:45},
  issn = {0362-5915},
  doi = {10.1145/3183653},
  url = {https://doi.org/10.1145/3183653},
  urldate = {2021-05-27},
  abstract = {Abstraction without regret refers to the vision of using high-level programming languages for systems development without experiencing a negative impact on performance. A database system designed according to this vision offers both increased productivity and high performance instead of sacrificing the former for the latter as is the case with existing, monolithic implementations that are hard to maintain and extend. In this article, we realize this vision in the domain of analytical query processing. We present LegoBase, a query engine written in the high-level programming language Scala. The key technique to regain efficiency is to apply generative programming: LegoBase performs source-to-source compilation and optimizes database systems code by converting the high-level Scala code to specialized, low-level C code. We show how generative programming allows to easily implement a wide spectrum of optimizations, such as introducing data partitioning or switching from a row to a column data layout, which are difficult to achieve with existing low-level query compilers that handle only queries. We demonstrate that sufficiently powerful abstractions are essential for dealing with the complexity of the optimization effort, shielding developers from compiler internals and decoupling individual optimizations from each other. We evaluate our approach with the TPC-H benchmark and show that (a) with all optimizations enabled, our architecture significantly outperforms a commercial in-memory database as well as an existing query compiler. (b) Programmers need to provide just a few hundred lines of high-level code for implementing the optimizations, instead of complicated low-level code that is required by existing query compilation approaches. (c) These optimizations may potentially come at the cost of using more system memory for improved performance. (d) The compilation overhead is low compared to the overall execution time, thus making our approach usable in practice for compiling query engines.},
  keywords = {abstraction without regret,code generation,High-level programming languages,optimizing compilers,query compilation,query processing},
  file = {/Users/cperivol/Zotero/storage/CECAMX8F/Shaikhha et al. - 2018 - Building Efficient Query Engines in a High-Level L.pdf}
}

@inproceedings{shankarQueryOptimizationMicrosoft2012,
  title = {Query Optimization in Microsoft {{SQL}} Server {{PDW}}},
  booktitle = {Proceedings of the 2012 International Conference on {{Management}} of {{Data}} - {{SIGMOD}} '12},
  author = {Shankar, Srinath and Galindo-Legaria, César and Nehme, Rimma and Aguilar-Saborit, Josep and Chung, Andrew and Elhemali, Mostafa and Halverson, Alan and Robinson, Eric and Subramanian, Mahadevan Sankara and DeWitt, David},
  date = {2012},
  pages = {767},
  publisher = {{ACM Press}},
  location = {{Scottsdale, Arizona, USA}},
  doi = {10.1145/2213836.2213953},
  url = {http://dl.acm.org/citation.cfm?doid=2213836.2213953},
  urldate = {2021-11-25},
  abstract = {In recent years, Massively Parallel Processors have increasingly been used to manage and query vast amounts of data. Dramatic performance improvements are achieved through distributed execution of queries across many nodes. Query optimization for such system is a challenging and important problem.},
  eventtitle = {The 2012 International Conference},
  isbn = {978-1-4503-1247-9},
  langid = {english},
  file = {/Users/cperivol/Zotero/storage/V4S97VYF/Shankar et al. - 2012 - Query optimization in microsoft SQL server PDW.pdf}
}

@inproceedings{shankarQueryOptimizationMicrosoft2012a,
  title = {Query Optimization in Microsoft {{SQL}} Server {{PDW}}},
  booktitle = {Proceedings of the 2012 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Shankar, Srinath and Nehme, Rimma and Aguilar-Saborit, Josep and Chung, Andrew and Elhemali, Mostafa and Halverson, Alan and Robinson, Eric and Subramanian, Mahadevan Sankara and DeWitt, David and Galindo-Legaria, César},
  date = {2012},
  pages = {767--776},
  file = {/Users/cperivol/Zotero/storage/B6U868FE/Shankar et al. - 2012 - Query optimization in microsoft SQL server PDW.pdf;/Users/cperivol/Zotero/storage/GDC6BETY/2213836.html}
}

@article{sharyginQueryCompilationPostgreSQL2017,
  title = {Query Compilation in {{PostgreSQL}} by Specialization of the {{DBMS}} Source Code},
  author = {Sharygin, E. Yu. and Buchatskiy, R. A. and Zhuykov, R. A. and Sher, A. R.},
  date = {2017-11-01},
  journaltitle = {Program Comput Soft},
  volume = {43},
  number = {6},
  pages = {353--365},
  issn = {1608-3261},
  doi = {10.1134/S0361768817060068},
  url = {https://doi.org/10.1134/S0361768817060068},
  urldate = {2021-11-27},
  abstract = {This paper describes the development of a query compiler for the PostgreSQL DBMS based on automatic code specialization methods; these methods allow one to avoid the development and support difficulties typical for classical query compilers by dividing the compiler development problem into two independent subproblems: reduction of overhead costs and implementation of algorithmic improvements. We assert that this decomposition facilitates the solution of both the subproblems: the cost reduction can be automated, while the algorithmic improvements can be implemented in the interpreter in the DBMS implementation language. This paper presents methods for online and offline specialization, considers specifics of specialization and binding-time analysis of the PostgreSQL source code, and describes the transition to a push model of execution.},
  langid = {english},
  annotation = {00000},
  file = {/Users/cperivol/Zotero/storage/BR5XNLPL/Sharygin et al. - 2017 - Query compilation in PostgreSQL by specialization .pdf}
}

@article{shumoCosetteAutomatedProver2017,
  title = {Cosette: An {{Automated Prover}} for {{SQL}}},
  shorttitle = {Cosette},
  author = {Shumo, Chu and Chenglong, Wang and Alvin, Cheung},
  date = {2017},
  journaltitle = {CIDR.[Google Scholar]}
}

@inproceedings{silvaExploitingCommonSubexpressions2012,
  title = {Exploiting Common Subexpressions for Cloud Query Processing},
  booktitle = {2012 {{IEEE}} 28th {{International Conference}} on {{Data Engineering}}},
  author = {Silva, Yasin N. and Larson, Paul-Ake and Zhou, Jingren},
  date = {2012},
  pages = {1337--1348},
  publisher = {{IEEE}}
}

@inproceedings{solimanOrcaModularQuery2014,
  title = {Orca: A Modular Query Optimizer Architecture for Big Data},
  shorttitle = {Orca},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Soliman, Mohamed A. and Antova, Lyublena and Raghavan, Venkatesh and El-Helw, Amr and Gu, Zhongxian and Shen, Entong and Caragea, George C. and Garcia-Alvarado, Carlos and Rahman, Foyzur and Petropoulos, Michalis},
  date = {2014},
  pages = {337--348},
  file = {/Users/cperivol/Zotero/storage/T857FUU7/Soliman et al. - 2014 - Orca a modular query optimizer architecture for b.pdf;/Users/cperivol/Zotero/storage/CMSIHP8A/2588555.html}
}

@inproceedings{solimanOrcaModularQuery2014a,
  title = {Orca: A Modular Query Optimizer Architecture for Big Data},
  shorttitle = {Orca},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Soliman, Mohamed A. and Antova, Lyublena and Raghavan, Venkatesh and El-Helw, Amr and Gu, Zhongxian and Shen, Entong and Caragea, George C. and Garcia-Alvarado, Carlos and Rahman, Foyzur and Petropoulos, Michalis and Waas, Florian and Narayanan, Sivaramakrishnan and Krikellas, Konstantinos and Baldwin, Rhonda},
  date = {2014-06-18},
  series = {{{SIGMOD}} '14},
  pages = {337--348},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2588555.2595637},
  url = {https://doi.org/10.1145/2588555.2595637},
  urldate = {2021-11-27},
  abstract = {The performance of analytical query processing in data management systems depends primarily on the capabilities of the system's query optimizer. Increased data volumes and heightened interest in processing complex analytical queries have prompted Pivotal to build a new query optimizer. In this paper we present the architecture of Orca, the new query optimizer for all Pivotal data management products, including Pivotal Greenplum Database and Pivotal HAWQ. Orca is a comprehensive development uniting state-of-the-art query optimization technology with own original research resulting in a modular and portable optimizer architecture. In addition to describing the overall architecture, we highlight several unique features and present performance comparisons against other systems.},
  isbn = {978-1-4503-2376-5},
  keywords = {cost model,mpp,parallel processing,query optimization},
  annotation = {00000}
}

@online{StateWillSpringerLink,
  title = {State {{Will}} Do | {{SpringerLink}}},
  url = {https://link.springer.com/chapter/10.1007/978-3-030-57761-2_10},
  urldate = {2021-09-19},
  file = {/Users/cperivol/Zotero/storage/RLCTPKDG/978-3-030-57761-2_10.html}
}

@inproceedings{stillgerLEODB2LearningOptimizer2001,
  title = {{{LEO}}-{{DB2}}'s Learning Optimizer},
  booktitle = {{{VLDB}}},
  author = {Stillger, Michael and Lohman, Guy M. and Markl, Volker and Kandil, Mokhtar},
  date = {2001},
  volume = {1},
  pages = {19--28},
  file = {/Users/cperivol/Zotero/storage/KAYPWGPD/Stillger et al. - 2001 - LEO-DB2's learning optimizer.pdf}
}

@online{sunEndtoendLearningbasedCost2019,
  title = {An End-to-End Learning-Based Cost Estimator},
  author = {Sun, Ji and Li, Guoliang},
  date = {2019},
  eprint = {1906.02560},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/cperivol/Zotero/storage/3WY36F2D/Sun and Li - 2019 - An end-to-end learning-based cost estimator.pdf;/Users/cperivol/Zotero/storage/3IZ5WTRI/1906.html}
}

@article{sussmanArtPropagator2009,
  title = {The {{Art}} of the {{Propagator}}},
  author = {Sussman, Gerald Jay and Radul, Alexey},
  date = {2009-01-26},
  url = {https://dspace.mit.edu/handle/1721.1/44215},
  urldate = {2021-10-19},
  abstract = {We develop a programming model built on the idea that the basic computational elements are autonomous machines interconnected by shared cells through which they communicate. Each machine continuously examines the cells it is interested in, and adds information to some based on deductions it can make from information from the others. This model makes it easy to smoothly combine expression-oriented and constraint-based programming; it also easily accommodates implicit incremental distributed search in ordinary programs.  This work builds on the original research of Guy Lewis Steele Jr. and was developed more recently with the help of Chris Hanson.},
  langid = {english},
  annotation = {Accepted: 2009-01-27T06:15:12Z},
  file = {/Users/cperivol/Zotero/storage/TMBSNBNL/Sussman and Radul - 2009 - The Art of the Propagator.pdf;/Users/cperivol/Zotero/storage/EI98K3F6/44215.html}
}

@book{sussmanFunctionalDifferentialGeometry2013,
  title = {Functional Differential Geometry},
  author = {Sussman, Gerald Jay and Wisdom, Jack and Farr, Will},
  date = {2013},
  publisher = {{The MIT Press}},
  location = {{Cambridge, MA}},
  isbn = {978-0-262-01934-7},
  langid = {english},
  pagetotal = {228},
  keywords = {Functional differential equations,Geometry; Differential,Mathematical physics},
  file = {/Users/cperivol/Zotero/storage/I6T8RPEL/Sussman et al. - 2013 - Functional differential geometry.pdf}
}

@inproceedings{szaboIncADSLDefinition2016,
  title = {{{IncA}}: A {{DSL}} for the Definition of Incremental Program Analyses},
  shorttitle = {{{IncA}}},
  booktitle = {Proceedings of the 31st {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Szabó, Tamás and Erdweg, Sebastian and Voelter, Markus},
  date = {2016-08-25},
  series = {{{ASE}} 2016},
  pages = {320--331},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2970276.2970298},
  url = {https://doi.org/10.1145/2970276.2970298},
  urldate = {2021-10-09},
  abstract = {Program analyses support software developers, for example, through error detection, code-quality assurance, and by enabling compiler optimizations and refactorings. To provide real-time feedback to developers within IDEs, an analysis must run efficiently even if the analyzed code base is large. To achieve this goal, we present a domain-specific language called IncA for the definition of efficient incremental program analyses that update their result as the program changes. IncA compiles analyses into graph patterns and relies on existing incremental matching algorithms. To scale IncA analyses to large programs, we describe optimizations that reduce caching and prune change propagation. Using IncA, we have developed incremental control flow and points-to analysis for C, well-formedness checks for DSLs, and 10 FindBugs checks for Java. Our evaluation demonstrates significant speedups for all analyses compared to their non-incremental counterparts.},
  isbn = {978-1-4503-3845-5},
  keywords = {Domain-specific Language,Incremental Computation,Language Workbench,Static Analysis},
  file = {/Users/cperivol/Zotero/storage/7JESSU9P/Szabó et al. - 2016 - IncA a DSL for the definition of incremental prog.pdf}
}

@inproceedings{tahboubHowArchitectQuery2018,
  title = {How to {{Architect}} a {{Query Compiler}}, {{Revisited}}},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Tahboub, Ruby Y. and Essertel, Grégory M. and Rompf, Tiark},
  date = {2018-05-27},
  series = {{{SIGMOD}} '18},
  pages = {307--322},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3183713.3196893},
  url = {https://doi.org/10.1145/3183713.3196893},
  urldate = {2021-05-27},
  abstract = {To leverage modern hardware platforms to their fullest, more and more database systems embrace compilation of query plans to native code. In the research community, there is an ongoing debate about the best way to architect such query compilers. This is perceived to be a difficult task, requiring techniques fundamentally different from traditional interpreted query execution. We aim to contribute to this discussion by drawing attention to an old but underappreciated idea known as Futamura projections, which fundamentally link interpreters and compilers. Guided by this idea, we demonstrate that efficient query compilation can actually be very simple, using techniques that are no more difficult than writing a query interpreter in a high-level language. Moreover, we demonstrate how intricate compilation patterns that were previously used to justify multiple compiler passes can be realized in one single, straightforward, generation pass. Key examples are injection of specialized index structures, data representation changes such as string dictionaries, and various kinds of code motion to reduce the amount of work on the critical path. We present LB2: a high-level query compiler developed in this style that performs on par with, and sometimes beats, the best compiled query engines on the standard TPC-H benchmark.},
  isbn = {978-1-4503-4703-7},
  keywords = {futamura projections,query compilation},
  file = {/Users/cperivol/Zotero/storage/5YLLFFHF/Tahboub et al. - 2018 - How to Architect a Query Compiler, Revisited.pdf}
}

@article{tangIntermittentQueryProcessing2019,
  title = {Intermittent Query Processing},
  author = {Tang, Dixin and Shang, Zechao and Elmore, Aaron J. and Krishnan, Sanjay and Franklin, Michael J.},
  date = {2019},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {11},
  pages = {1427--1441},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/4A9UXMXP/Tang et al. - 2019 - Intermittent query processing.pdf;/Users/cperivol/Zotero/storage/Y3JUBEQY/3342263.html}
}

@inproceedings{theodoratosDataWarehouseConfiguration1997,
  title = {Data warehouse configuration},
  booktitle = {Proceedings of the 23rd International Conference on Very Large Databases, VLDB 1997},
  author = {Theodoratos, Dimitrios and Sellis, Timos},
  date = {1997-01-01},
  pages = {126--135},
  publisher = {{Morgan Kaufmann}},
  url = {https://researchwith.njit.edu/en/publications/data-warehouse-configuration},
  urldate = {2021-11-26},
  eventtitle = {23rd International Conference on Very Large Databases, VLDB 1997},
  langid = {English (US)},
  annotation = {00000},
  file = {/Users/cperivol/Zotero/storage/V9SVBAAW/data-warehouse-configuration.html}
}

@article{tianSynergisticGraphSQL2019,
  title = {Synergistic Graph and {{SQL}} Analytics inside {{IBM Db2}}},
  author = {Tian, Yuanyuan and Sun, Wen and Tong, Sui Jun and Xu, En Liang and Pirahesh, Mir Hamid and Zhao, Wei},
  date = {2019},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {12},
  pages = {1782--1785},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/MLPJXY2E/Tian et al. - 2019 - Synergistic graph and SQL analytics inside IBM Db2.pdf;/Users/cperivol/Zotero/storage/C24DAJ7N/3352063.html}
}

@inproceedings{ullmanInformationIntegrationUsing1997,
  title = {Information Integration Using Logical Views},
  booktitle = {International {{Conference}} on {{Database Theory}}},
  author = {Ullman, Jeffrey D.},
  date = {1997},
  pages = {19--40},
  publisher = {{Springer}},
  file = {/Users/cperivol/Zotero/storage/G7GJWCWW/Ullman - 1997 - Information integration using logical views.pdf;/Users/cperivol/Zotero/storage/YI87HS8J/3-540-62222-5_34.html}
}

@book{uustaluComonadicFunctionalAttribute2005,
  title = {Comonadic Functional Attribute Evaluation},
  author = {Uustalu, Tarmo and Vene, Varmo},
  date = {2005-01-01},
  journaltitle = {Trends in Functional Programming},
  volume = {6},
  pages = {162},
  abstract = {We have previously demonstrated that dataflow computation is comonadic. Here we argue that attribute evaluation has a lot in common with dataflow computation and admits a similar analysis. We claim that this yields a new, modular way to organize both attribute evaluation programs written directly in a functional language as well as attribute grammar processors.},
  pagetotal = {145}
}

@inproceedings{vartakMistiqueSystemStore2018,
  title = {Mistique: A System to Store and Query Model Intermediates for Model Diagnosis},
  shorttitle = {Mistique},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Vartak, Manasi and F. da Trindade, Joana M. and Madden, Samuel and Zaharia, Matei},
  date = {2018},
  pages = {1285--1300},
  file = {/Users/cperivol/Zotero/storage/CRFN2YNE/Vartak et al. - 2018 - Mistique A system to store and query model interm.pdf;/Users/cperivol/Zotero/storage/FQZUHNJ4/3183713.html}
}

@inproceedings{veanesQexSymbolicSQL2010,
  title = {Qex: Symbolic {{SQL}} Query Explorer},
  shorttitle = {Qex},
  booktitle = {International {{Conference}} on {{Logic}} for {{Programming Artificial Intelligence}} and {{Reasoning}}},
  author = {Veanes, Margus and Tillmann, Nikolai and De Halleux, Jonathan},
  date = {2010},
  pages = {425--446},
  publisher = {{Springer}},
  file = {/Users/cperivol/Zotero/storage/MUS4RK5Y/Veanes et al. - 2010 - Qex Symbolic SQL query explorer.pdf;/Users/cperivol/Zotero/storage/CMP2K3HH/978-3-642-17511-4_24.html}
}

@inproceedings{veanesSymbolicQueryExploration2009,
  title = {Symbolic Query Exploration},
  booktitle = {International {{Conference}} on {{Formal Engineering Methods}}},
  author = {Veanes, Margus and Grigorenko, Pavel and De Halleux, Peli and Tillmann, Nikolai},
  date = {2009},
  pages = {49--68},
  publisher = {{Springer}},
  file = {/Users/cperivol/Zotero/storage/N5PJ5DHL/Veanes et al. - 2009 - Symbolic query exploration.pdf;/Users/cperivol/Zotero/storage/YBNI2SFH/978-3-642-10373-5_3.html}
}

@inproceedings{vegaReticleVirtualMachine2021,
  title = {Reticle: A Virtual Machine for Programming Modern {{FPGAs}}},
  shorttitle = {Reticle},
  booktitle = {Proceedings of the 42nd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Vega, Luis and McMahan, Joseph and Sampson, Adrian and Grossman, Dan and Ceze, Luis},
  date = {2021-06-19},
  series = {{{PLDI}} 2021},
  pages = {756--771},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3453483.3454075},
  url = {https://doi.org/10.1145/3453483.3454075},
  urldate = {2021-08-09},
  abstract = {Modern field-programmable gate arrays (FPGAs) have recently powered high-profile efficiency gains in systems from datacenters to embedded devices by offering ensembles of heterogeneous, reconfigurable hardware units. Programming stacks for FPGAs, however, are stuck in the past—they are based on traditional hardware languages, which were appropriate when FPGAs were simple, homogeneous fabrics of basic programmable primitives. We describe Reticle, a new low-level abstraction for FPGA programming that, unlike existing languages, explicitly represents the special-purpose units available on a particular FPGA device. Reticle has two levels: a portable intermediate language and a target-specific assembly language. We show how to use a standard instruction selection approach to lower intermediate programs to assembly programs, which can be both faster and more effective than the complex metaheuristics that existing FPGA toolchains use. We use Reticle to implement linear algebra operators and coroutines and find that Reticle compilation runs up to 100 times faster than current approaches while producing comparable or better run-time and utilization.},
  isbn = {978-1-4503-8391-2},
  keywords = {compilers,FPGAs},
  file = {/Users/cperivol/Zotero/storage/UHAX342L/Vega et al. - 2021 - Reticle a virtual machine for programming modern .pdf}
}

@inproceedings{viswanathanQueryResourceOptimization2018,
  title = {Query and Resource Optimization: Bridging the Gap},
  shorttitle = {Query and Resource Optimization},
  booktitle = {2018 {{IEEE}} 34th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Viswanathan, Lalitha and Jindal, Alekh and Karanasos, Konstantinos},
  date = {2018},
  pages = {1384--1387},
  publisher = {{IEEE}},
  file = {/Users/cperivol/Zotero/storage/IPWXKCNV/Viswanathan et al. - 2018 - Query and resource optimization Bridging the gap.pdf;/Users/cperivol/Zotero/storage/MDETESTU/8509377.html}
}

@inproceedings{voigtlanderAsymptoticImprovementComputations2008,
  title = {Asymptotic {{Improvement}} of {{Computations}} over {{Free Monads}}},
  booktitle = {Mathematics of {{Program Construction}}},
  author = {Voigtländer, Janis},
  editor = {Audebaud, Philippe and Paulin-Mohring, Christine},
  date = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {388--403},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-70594-9_20},
  abstract = {We present a low-effort program transformation to improve the efficiency of computations over free monads in Haskell. The development is calculational and carried out in a generic setting, thus applying to a variety of datatypes. An important aspect of our approach is the utilisation of type class mechanisms to make the transformation as transparent as possible, requiring no restructuring of code at all. There is also no extra support necessary from the compiler (apart from an up-to-date type checker). Despite this simplicity of use, our technique is able to achieve true asymptotic runtime improvements. We demonstrate this by examples for which the complexity is reduced from quadratic to linear.},
  isbn = {978-3-540-70594-9},
  langid = {english},
  keywords = {Functional Programming,Fusion Rule,Program Transformation,Type Checker,Type Constructor},
  file = {/Users/cperivol/Zotero/storage/HFYF856M/Voigtländer - 2008 - Asymptotic Improvement of Computations over Free M.pdf}
}

@book{vukoticNeo4jAction2015,
  title = {Neo4j in Action},
  author = {Vukotic, Aleksa and Watt, Nicki and Abedrabbo, Tareq and Fox, Dominic and Partner, Jonas},
  date = {2015},
  volume = {22},
  publisher = {{Manning Shelter Island}},
  annotation = {00000},
  file = {/Users/cperivol/Zotero/storage/478VGAJD/Vukotic et al. - 2015 - Neo4j in action.pdf}
}

@inproceedings{waasConventionalDataWarehousing2009,
  title = {Beyond {{Conventional Data Warehousing}} — {{Massively Parallel Data Processing}} with {{Greenplum Database}}},
  booktitle = {Business {{Intelligence}} for the {{Real}}-{{Time Enterprise}}},
  author = {Waas, Florian M.},
  editor = {Castellanos, Malu and Dayal, Umesh and Sellis, Timos},
  date = {2009},
  series = {Lecture {{Notes}} in {{Business Information Processing}}},
  pages = {89--96},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-03422-0_7},
  abstract = {In this presentation we discuss trends and challenges for data warehousing beyond conventional application areas. In particular, we discuss how a massively parallel system like Greenplum Database can be used for MapReduce-like data processing.},
  isbn = {978-3-642-03422-0},
  langid = {english},
  keywords = {analytics,data processing,Data Warehousing,MapReduce,petabyte-scale,User-defined Functions},
  file = {/Users/cperivol/Zotero/storage/SEAZUZJI/Waas - 2009 - Beyond Conventional Data Warehousing — Massively P.pdf}
}

@article{wangMultiqueryOptimizationMapreduce2013,
  title = {Multi-Query Optimization in Mapreduce Framework},
  author = {Wang, Guoping and Chan, Chee-Yong},
  date = {2013},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {7},
  number = {3},
  pages = {145--156},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/PY8H5BD3/Wang and Chan - 2013 - Multi-query optimization in mapreduce framework.pdf;/Users/cperivol/Zotero/storage/65Q9PBV6/2732232.html}
}

@article{wangMultiqueryOptimizationMapreduce2013a,
  title = {Multi-Query Optimization in Mapreduce Framework},
  author = {Wang, Guoping and Chan, Chee-Yong},
  date = {2013},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {7},
  number = {3},
  pages = {145--156},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/F33D4ZBU/Wang and Chan - 2013 - Multi-query optimization in mapreduce framework.pdf;/Users/cperivol/Zotero/storage/958MSY5Y/2732232.html}
}

@article{wangSpeedingSymbolicReasoning2018,
  title = {Speeding up Symbolic Reasoning for Relational Queries},
  author = {Wang, Chenglong and Cheung, Alvin and Bodík, Rastislav},
  date = {2018},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  volume = {2},
  pages = {1--25},
  publisher = {{ACM New York, NY, USA}},
  issue = {OOPSLA},
  file = {/Users/cperivol/Zotero/storage/UVJBT2N9/Wang et al. - 2018 - Speeding up symbolic reasoning for relational quer.pdf;/Users/cperivol/Zotero/storage/BWHHPDH3/3276527.html}
}

@article{wangVerifyingEquivalenceDatabasedriven2017,
  title = {Verifying Equivalence of Database-Driven Applications},
  author = {Wang, Yuepeng and Dillig, Isil and Lahiri, Shuvendu K. and Cook, William R.},
  date = {2017},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  volume = {2},
  pages = {1--29},
  publisher = {{ACM New York, NY, USA}},
  issue = {POPL},
  file = {/Users/cperivol/Zotero/storage/I8A79MQW/Wang et al. - 2017 - Verifying equivalence of database-driven applicati.pdf;/Users/cperivol/Zotero/storage/XVK37WYB/3158144.html}
}

@inproceedings{woltmannCardinalityEstimationLocal2019,
  title = {Cardinality Estimation with Local Deep Learning Models},
  booktitle = {Proceedings of the {{Second International Workshop}} on {{Exploiting Artificial Intelligence Techniques}} for {{Data Management}}},
  author = {Woltmann, Lucas and Hartmann, Claudio and Thiele, Maik and Habich, Dirk and Lehner, Wolfgang},
  date = {2019-07-05},
  series = {{{aiDM}} '19},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3329859.3329875},
  url = {https://doi.org/10.1145/3329859.3329875},
  urldate = {2021-11-28},
  abstract = {Cardinality estimation is a fundamental task in database query processing and optimization. Unfortunately, the accuracy of traditional estimation techniques is poor resulting in non-optimal query execution plans. With the recent expansion of machine learning into the field of data management, there is the general notion that data analysis, especially neural networks, can lead to better estimation accuracy. Up to now, all proposed neural network approaches for the cardinality estimation follow a global approach considering the whole database schema at once. These global models are prone to sparse data at training leading to misestimates for queries which were not represented in the sample space used for generating training queries. To overcome this issue, we introduce a novel local-oriented approach in this paper, therefore the local context is a specific sub-part of the schema. As we will show, this leads to better representation of data correlation and thus better estimation accuracy. Compared to global approaches, our novel approach achieves an improvement by two orders of magnitude in accuracy and by a factor of four in training time performance for local models.},
  isbn = {978-1-4503-6802-5},
  annotation = {00055},
  file = {/Users/cperivol/Zotero/storage/UHEWPRQ2/Woltmann et al. - 2019 - Cardinality estimation with local deep learning mo.pdf}
}

@article{wuLearningOptimizerShared2018,
  title = {Towards a Learning Optimizer for Shared Clouds},
  author = {Wu, Chenggang and Jindal, Alekh and Amizadeh, Saeed and Patel, Hiren and Le, Wangchao and Qiao, Shi and Rao, Sriram},
  date = {2018},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {3},
  pages = {210--222},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/KPTVS5MR/Wu et al. - 2018 - Towards a learning optimizer for shared clouds.pdf;/Users/cperivol/Zotero/storage/RC3IVXQ5/3291264.html}
}

@inproceedings{wuSamplingbasedQueryReoptimization2016,
  title = {Sampling-Based Query Re-Optimization},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Wu, Wentao and Naughton, Jeffrey F. and Singh, Harneet},
  date = {2016},
  pages = {1721--1736},
  file = {/Users/cperivol/Zotero/storage/9C6W7WC2/Wu et al. - 2016 - Sampling-based query re-optimization.pdf;/Users/cperivol/Zotero/storage/YEY5GVGV/2882903.html}
}

@inproceedings{xinAcceleratingHumanintheloopMachine,
  title = {Accelerating Human-in-the-Loop Machine Learning: Challenges and Opportunities (Vision Paper)},
  shorttitle = {Accelerating Human-in-the-Loop Machine Learning},
  booktitle = {Proceedings of the {{Second Workshop}} on {{Data Management}} for {{End}}-{{To}}-{{End Machine Learning}}, {{DEEM}}},
  author = {Xin, D. and Ma, L. and Liu, J. and Macke, S. and Song, S. and Parameswaran, A.},
  volume = {18}
}

@online{xinHelixHolisticOptimization2018,
  title = {Helix: Holistic Optimization for Accelerating Iterative Machine Learning},
  shorttitle = {Helix},
  author = {Xin, Doris and Macke, Stephen and Ma, Litian and Liu, Jialin and Song, Shuchen and Parameswaran, Aditya},
  date = {2018},
  eprint = {1812.05762},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/cperivol/Zotero/storage/QTZA4N7J/Xin et al. - 2018 - Helix Holistic optimization for accelerating iter.pdf;/Users/cperivol/Zotero/storage/RQ9Z8ZCK/1812.html}
}

@online{xinHowDevelopersIterate2018,
  title = {How {{Developers Iterate}} on {{Machine Learning Workflows}}–{{A Survey}} of the {{Applied Machine Learning Literature}}},
  author = {Xin, Doris and Ma, Litian and Song, Shuchen and Parameswaran, Aditya},
  date = {2018},
  eprint = {1803.10311},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/cperivol/Zotero/storage/S3F7H8JL/Xin et al. - 2018 - How Developers Iterate on Machine Learning Workflo.pdf;/Users/cperivol/Zotero/storage/3IRY5KB8/1803.html}
}

@article{yanGeneratingApplicationspecificData2019,
  title = {Generating Application-Specific Data Layouts for in-Memory Databases},
  author = {Yan, Cong and Cheung, Alvin},
  date = {2019},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {11},
  pages = {1513--1525},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/KY9IAFVD/Yan and Cheung - 2019 - Generating application-specific data layouts for i.pdf;/Users/cperivol/Zotero/storage/IIFF7P92/3342263.html}
}

@inproceedings{yangPowerstationAutomaticallyDetecting2018,
  title = {Powerstation: Automatically Detecting and Fixing Inefficiencies of Database-Backed Web Applications in Ide},
  shorttitle = {Powerstation},
  booktitle = {Proceedings of the 2018 26th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Yang, Junwen and Yan, Cong and Subramaniam, Pranav and Lu, Shan and Cheung, Alvin},
  date = {2018},
  pages = {884--887},
  file = {/Users/cperivol/Zotero/storage/7ZHCSELX/Yang et al. - 2018 - Powerstation Automatically detecting and fixing i.pdf;/Users/cperivol/Zotero/storage/VA6E5R8G/3236024.html}
}

@article{yorgeyTypeclassopedia2009,
  title = {The Typeclassopedia},
  author = {Yorgey, Brent},
  date = {2009},
  publisher = {{Citeseer}}
}

@inproceedings{yuanAutomaticViewGeneration2020,
  title = {Automatic View Generation with Deep Learning and Reinforcement Learning},
  booktitle = {2020 {{IEEE}} 36th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Yuan, Haitao and Li, Guoliang and Feng, Ling and Sun, Ji and Han, Yue},
  date = {2020},
  pages = {1501--1512},
  publisher = {{IEEE}}
}

@inproceedings{yuReinforcementLearningTreelstm2020,
  title = {Reinforcement Learning with Tree-Lstm for Join Order Selection},
  booktitle = {2020 {{IEEE}} 36th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Yu, Xiang and Li, Guoliang and Chai, Chengliang and Tang, Nan},
  date = {2020},
  pages = {1297--1308},
  publisher = {{IEEE}}
}

@inproceedings{zengIOLAPManagingUncertainty2016,
  title = {{{IOLAP}}: Managing Uncertainty for Efficient Incremental {{OLAP}}},
  shorttitle = {{{IOLAP}}},
  booktitle = {Proceedings of the 2016 International Conference on Management of Data},
  author = {Zeng, Kai and Agarwal, Sameer and Stoica, Ion},
  date = {2016},
  pages = {1347--1361},
  file = {/Users/cperivol/Zotero/storage/7C9GXEXG/2882903.html}
}

@inproceedings{zhangGViewAutomaticView2020,
  title = {G-{{View}}: Automatic {{View Selection}} in {{Graph Databases}}},
  shorttitle = {G-{{View}}},
  booktitle = {{{EDBT}} 2021: 24th {{International Conference}} on {{Extending Database Technology}}},
  author = {Zhang, Chao and Lu, Jiaheng and Guo, Qingsong and Zhang, Xinyong and Han, Xiaochun and Zhou, Minqi},
  date = {2020},
  pages = {1--12}
}

@article{zhouAutomatedVerificationQuery2019,
  title = {Automated Verification of Query Equivalence Using Satisfiability modulo Theories},
  author = {Zhou, Qi and Arulraj, Joy and Navathe, Shamkant and Harris, William and Xu, Dong},
  date = {2019},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {11},
  pages = {1276--1288},
  publisher = {{VLDB Endowment}},
  file = {/Users/cperivol/Zotero/storage/KT687I38/Zhou et al. - 2019 - Automated verification of query equivalence using .pdf;/Users/cperivol/Zotero/storage/8QWDDE6Y/3342263.html}
}

@inproceedings{zhouEfficientExploitationSimilar2007,
  title = {Efficient Exploitation of Similar Subexpressions for Query Processing},
  booktitle = {Proceedings of the 2007 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Zhou, Jingren and Larson, Per-Ake and Freytag, Johann-Christoph and Lehner, Wolfgang},
  date = {2007},
  pages = {533--544},
  file = {/Users/cperivol/Zotero/storage/9R5XNY2D/Zhou et al. - 2007 - Efficient exploitation of similar subexpressions f.pdf;/Users/cperivol/Zotero/storage/ZWTM4C86/1247480.html}
}

@inproceedings{zhouEfficientExploitationSimilar2007a,
  title = {Efficient Exploitation of Similar Subexpressions for Query Processing},
  booktitle = {Proceedings of the 2007 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Zhou, Jingren and Larson, Per-Ake and Freytag, Johann-Christoph and Lehner, Wolfgang},
  date = {2007},
  pages = {533--544},
  file = {/Users/cperivol/Zotero/storage/VWYRCUC4/Zhou et al. - 2007 - Efficient exploitation of similar subexpressions f.pdf}
}

@online{zotero-531,
  url = {https://scholar.googleusercontent.com/scholar.bib?q=info:PkkOAGFXsvMJ:scholar.google.com/&output=citation&scisdr=CgXdkB7JEL2r6iNnjtk:AAGBfm0AAAAAYaJhltja93eQ1J2zwq3BNVVtV_BTUjDv&scisig=AAGBfm0AAAAAYaJhlm09-loQtLN25gMjro4QIbHou9W9&scisf=4&ct=citation&cd=-1&hl=en},
  urldate = {2021-11-27},
  annotation = {00000},
  file = {/Users/cperivol/Zotero/storage/QFPPPRLN/scholar.html}
}



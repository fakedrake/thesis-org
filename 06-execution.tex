\dictum[Herbert Marcuse]{The concrete conditions for realizing the truth may
 vary, but the truth remains the same and theory remains its ultimate
 guardian.}

\begin{summary}
\item Tables are stored in a memory file system and records are
  stored as POD binary objects organized in 4K pages.
\item The extent of each relation involved in a plan is converted to
  C++ \cpp{struct} and each predicate is converted to a C++ callable
  class.
\item These parameterize the template-based operator implementations to allow the
  compiler to generate highly specialized code.
\item In this chapter we discuss the implementation of important
  operations and their reverses.
\end{summary}

Code generation is becoming increasingly common in RDBMSs. Used
mainly in in-memory databases, where disk IO does not dominate the
runtime, it aims to minimize the overhead of data access function
calls, to optimize the predicates and numerical expressions and to
avoid indirection in tight loops. Approaches to code generation fall
generally on a spectrum between two extremes:

\begin{itemize}
\item Transpilation of the physical plan to a low level programming
  language like C or C++ for every query
\item JIT compilation of small parts of the plan as the query
  executes.
\end{itemize}

FluiDB follows the approach of Krikellas
et. al. \cite{krikellasGeneratingCodeHolistic2010} falling far to the
former end of the spectrum. We generate very specialized,
template-heavy C++ code for every query and we use an off-the-shelf
compiler to generate highly optimized machine code.

In this chapter we will describe in detail all aspects of code
generation, the form of data the generated code manipulates and the
\emph{FluiDB C++ Library (FCL)} that supports the generated code by
implementing operators, utility functions and types.

\begin{corrected}{Contributions}
  This chapter's contribution to the state of the art revolves around
  an efficient approach to generating efficient query specific C++
  code as well as describing the underlying data structures and
  algorithms that FluiDB to leverage compiler technologies.
\end{corrected}


\section{Storage memory management}

We opted for delegating the task of memory management to the OS and
use use use tmpfs as a storage layer to our database. The tmpfs
filesystem depends on the the \emph{shmem} module (as of Linux v5.13)
for handling file operations, a resizable virtual memory filesystem
for linux. Where a typical persistent filesystem stores files in a
block device and caches pages in memory for efficiency, shmem keeps
files exclusively as pages in the page cache. The OS tries to keep all
pages in memory but when resources start running out, it writes pages
in swap.

Assuming that the pages are not in swap, normal reads for shmemfs
using \cpp{read()} are equivalent to copying pages from the page cache
to the user space, while shmem \cpp{writes()} operate directly to the
pages. We can mitigate the copying overhead of reads using \cpp{mmap}
which will remap the page to the address space of the application.
The problem with this approach, however, is that, while it allows us
to minimize copying, we still need to run system calls in tight loops,
which can be very computationally expensive. This can't be completely
mitigated unless we move the ``storage'' layer to the userspace,
re-implementing the memory management that we get for ``free'', in
terms of engineering effort, so we leave that for a future version of
FluiDB.

On the other hand, assuming a normal file system layer allows FluiDB
to easily be adapted to operate over any filesystem backed by
different storage technologies like NVM.

\section{Data layout}

Before we get into the details of the actual physical plan (in the
form of C++ code) we need to state some assumptions about the layout
of the code and the primary data.

FluiDB is a \emph{row store system} which stores tables as files, each
of which is a collection of pages, each of which is a collection of
records (rows). We use one file per table or intermediate result, and
each file is simply a sequence of records organized into pages. For
now, FluiDB does not support any kind of indexing or
compression. FluiDB can, of course, be run over any filesystem, but
non-memory based file systems diminish the benefit of code generation,
making performance IO bound.

With this in mind, there are three parts to understanding the
principles of FluiDB storage:

\begin{itemize}
\item The format in which primary data is inserted into the database.
\item The layout of the data within the database
\item The transformation from the former to the latter.
\end{itemize}

\subsection{Initial data conversion}

Starting with the initial data, for FluiDB to be adapted to a
particular dataset, it requires the primary data in CSV format and
some Haskell configuration code describing the shape of the data and
the database configuration. Our experiments so far have been revolving
around the SSB TPC-H benchmark, so the format expected is the plain
text format that dbgen \cite{perivolaropoulosFakedrakeSsbdbgen2021}
generates. This comprises of two steps: first a Haskell program parses
the CSV records into standard-layout binary objects that can be
directly cast to C/C++ standard-layout structs. These binary objects
are stored one after the other in a flat binary file. The end result
of what we want to do to be able to execute code similar to the one
presented in listing \ref{lst:flat_to_dat}.

\begin{code}
\begin{cppcode}
template<typename R, size_t batch_size=2000>
void flat_to_dat(const std::string& flat_file, const std::string& dat_file) {
  int fd;
  size_t read_bytes;
  std::array<R, batch_size> batch;
  // Open the file in binary mode
  fd = ::open(flat_file.c_str(), O_RDONLY);
  // The writer is the object used by all operator implementations
  // and it will write each record in pages. Using it here asserts
  // that the data will have the correct format.
  Writer<R> w(dat_file);
  do {
    // Read a batch of data,
    read_bytes = ::read(fd, batch.data(), sizeof(batch));
    // Use the writer object to write each record.
    for (size_t i = 0; i < read_bytes / sizeof(R); i++) {
      w.write(batch[i]);
    }
    // Keep reading until a batch is cut short.
  } while (read_bytes == sizeof(batch));
  // Wrap up.
  w.close();
  close(fd);
}
\end{cppcode}
  \caption{\label{lst:flat_to_dat}For standard FFI communication C++
    structs that do not contain fancy copy constructors, virtual
    methods, etc (standard layout) can be used directly. We take
    advantage of that to store structs directly into files as
    contiguous binary objects.}
\end{code}

For this to work we refer to the C++ standard
\cite{14:00-17:00ISOIEC14882}. The types that represent table rows
must be standard layout\footnote{In C, standard layout is commonly
known as POD (Plain Old Data). This term is deprecated as of C++20 in
favor of standard layout classes which incarnate the data layout
aspect of POD, and trivial classes which incarnate the behavioral
aspect of POD.}. According to the standard:

\begin{quote}
A class S is a standard-layout class if it:
\begin{itemize}
\item has no non-static data members of type non-standard-layout class (or
array of such types) or reference,
\item has no virtual functions and no virtual base classes
\item has the same access control for all non-static data members,
\item has no non-standard-layout base classes,
\item has at most one base class subobject of any given type,
\item has all non-static data members and bit-fields in the class and its
base classes first declared in the same class, and
\item has no element of the set M(S) of types as a base class, where for
any type X, M(X) is defined as follows. [Note: M(X) is the set of
the types of all non-base-class subobjects that may be at a zero
offset in X.]
\begin{itemize}
\item If X is a non-union class type with no (possibly inherited)
non-static data members, the set M(X) is empty.
\item If X is a non-union class type with a non-static data member of
type X\textsubscript{0} that is either of zero size or is the first non-static
data member of X (where said member may be an anonymous union),
the set M(X) consists of X\textsubscript{0} and the elements of M(X\textsubscript{0}).
\item If X is a union type, the set M(X) is the union of all M(Ui) and
the set containing all U\textsubscript{i} , where each U\textsubscript{i} is the type of the i
th non-static data member of X.
\item If X is an array type with element type X\textsubscript{e}, the set M(X) consists
of X\textsubscript{e} and the elements of M(Xe).
\item If X is a non-class, non-array type, the set M(X) is empty.
\end{itemize}
\end{itemize}
\end{quote}

The records we generate certainly conform to these requirements. An
example is the supplier row in \ref{lst:record_class}. Therefore the
object is trivially copyable (has a default copy constructor) and
occupies contiguous bytes of storage. This means that we can safely
write each record \cpp{R} as \cpp{sizeof(R)} contiguous binary data to
a file and expect to find the same value of \cpp{R} when we read
it. Based on this, we can safely copy binary record objects from
memory to disc and vice versa.

\begin{code}
\begin{cppcode}
class Record {
public:
  Record(unsigned __s__suppkey, fluidb_string<25> __s__name,
         fluidb_string<40> __s__address, fluidb_string<16> __s__city,
         fluidb_string<16> __s__nation, fluidb_string<13> __s__region,
         fluidb_string<15> __s__phone)
    : s__suppkey(__s__suppkey),
      s__name(__s__name),
      s__address(__s__address),
      s__city(__s__city),
      s__nation(__s__nation),
      s__region(__s__region),
      s__phone(__s__phone) {}
  Record() {}
  std::string show() const {
    std::stringstream o;
    o << s__suppkey << " | " << arrToString(s__name) << " | "
      << arrToString(s__address) << " | " << arrToString(s__city) << " | "
      << arrToString(s__nation) << " | " << arrToString(s__region) << " | "
      << arrToString(s__phone);
    return o.str();
  }
  bool operator==(const Record& otherRec) const {
    // compare each field ...
  }
  bool operator!=(const Record& otherRec) const {
    // compare each field ...
  }
  unsigned s__suppkey;
  fluidb_string<25> s__name;
  fluidb_string<40> s__address;
  fluidb_string<16> s__city;
  fluidb_string<16> s__nation;
  fluidb_string<13> s__region;
  fluidb_string<15> s__phone;
};
\end{cppcode}
  \caption{\label{lst:record_class}The supplier row representation in
    the generated C++ code. The \cpp{fluidb_string} type is a
    constant size array of characters.}
\end{code}

However, in the case of parsing, we need to take great care with byte
alignment which is compiler dependent. Fortunately, Clang and GCC
informally agree on the algorithm for \cpp{alignof(<cls>, <member>)} for
standard layout objects. The algorithm is presented in
\ref{lst:record_byte_padding}

\begin{code}
\begin{haskellcode}
schemaPostPaddings :: [CppType] -> Maybe [Int]
schemaPostPaddings [] = Just []
schemaPostPaddings [_] = Just [0]
schemaPostPaddings schema = do
  elemSizes <- sequenceA [cppTypeSize t | t <- schema]
  spaceAligns' <- sequenceA [cppTypeAlignment t | t <- schema]
  let (_:spaceAligns) = spaceAligns' ++ [maximum spaceAligns']
  let offsets = 0 : zipWith3 getOffset spaceAligns offsets elemSizes
  return $ zipWith (-) (zipWith (-) (tail offsets) offsets) elemSizes
  where
    getOffset nextAlig off size =
      (size + off)
      + ((nextAlig - ((size + off) `mod` nextAlig)) `mod` nextAlig)
\end{haskellcode}
  \caption{\label{lst:record_byte_padding}Algorithm to infer the
    padding of members according to the Itanium ABI.}
\end{code}

Once the flat files are generated, C++ code is generated for parsing
the file calls into the C++ function \cpp{flat_to_dat} that is
parametric to the type of the object being and uses the FCL
library to write objects, thus making sure that the data is readable
by the operators. \cpp{flat_to_dat} simply reads the input
\texttt{.flat} file as a stream of records of size
\cpp{sizeof(Record)} casting the bytes with
\cpp{reinterpret_cast<Record*>} into the record. It then use the FCL
record writing facility \cpp{Writer<R>::write} that takes care of
organizing the record into pages. Thus, the final \emph{data file} is
created that is ready for use by the generated code.

\begin{code}
\begin{cppcode}
#include <bamify.hh>
class Record {
  // ...
};
int main(int argc, char* argv[]) {
  flat_to_dat<Record>("supplier.flat","supplier.dat");
}
\end{cppcode}
  \caption{Convert a flat file to a paged data file.}
\end{code}

\subsection{Pages}

We translate the flat binary files into \texttt{.dat} files that
contain the table data in its final format. The basic block of the
file is the \cpp{Page}, and the file is simply a raw sequence of
pages. A page is constant-size data structure that contains up to
\(\left\lfloor\frac{S_{rec}}{S_{pg}} \right\rfloor\) \emph{whole}
records where \(S_{rec}\) is \cpp{sizeof(Record)} and \(S_{pg}\) is
the size of the page which typically is 4KB. All pages must contain as
many whole records as can fit except the last one.

All transactions on the storage are made at the page level: we
either read or write only entire pages. These operations are
abstracted by the \cpp{Reader} and \cpp{Writer} classes. We use one more level
of abstraction for convenience, the higher order \cpp{eachRecord}
function. To demonstrate what the interface looks like, we present the
implementation of \cpp{eachRecord} \ref{lst:each_record}.

\begin{code}
\begin{cppcode}
// Fn could be instantiated to std::function<void(const R&)> but that
// will *always* forbid f from being inlined.
template <typename R,typename Fn>
inline void eachRecord(const std::string& inpFile,Fn f) {
  Reader<R> reader;
  size_t i = 0;
  reader.open(inpFile);
  while (reader.hasNext()) {
    i++;
    f(reader.nextRecord());
  }
  reader.close();
}
\end{cppcode}
\caption{\label{lst:each_record}\cpp{eachRecord} is a utility that simply iterates over all the records in a file.}
\end{code}

As alluded to in the previous section, both the \cpp{Writer} and \cpp{Reader}
use \cpp{reinterpret_cast} to "serialize" and "deserialise" the data
respectively.

\subsection{C++ row iterators}

It is important for the planner to have full control of the storage
budget and assume that no significant memory is required for
evaluating each operator. For that reason, we require that all
operators' algorithms have constant space complexity. This may mean
major compromises for some algorithms with respect to the time
complexity like join and aggregation. Fortunately, we can mitigate
that by taking advantage of the set semantics of FluiDB relational
algebra and sorting the input tables in-place before running joins or
aggregations. Our particular implementation of in-place sorting hinges
on the \cpp{RecordMap} type that provides C++ random access iterators
to the records of a file, abstracting the page reads and writes. We
pass these iterators to \cpp{std::sort} (as demonstrated in listing
\ref{lst:record_map_sort}) which runs insertion sort for small ranges
to take advantage of the processors reorder buffer and quicksort for
longer ones. An easy optimization for the next version of FluiDB would
be to use a custom in-place sort optimized for the page size and which
respects the page boundaries.

\begin{code}
\begin{cppcode}
RecordMap<size_t> fs("/tmp/removeme.dat");
std::sort(fs.begin(), fs.end());
\end{cppcode}
  \caption{\label{lst:record_map_sort}Using a \cpp{RecordMap} to sort
    the records of a file by providing an iterator range to \cpp{std::sort}.}
\end{code}

The operation of \cpp{RecordMap} is very simple. Each iterator is
paired with the page that contains the record it points to, when an
iterator is the only on pointing inside a page and moves out of the
page, said page is written back to the file and the new page is
read. When the only iterator pointing to a page is deleted the page is
written to the file. Multiple iterators pointing to the same page do
not maintain different copies of that page and the last one to be
destroyed or to leave the page triggers the page to be written back to
the file.

\section{Physical planning}

The fundamental logic of the code generator is fairly simple: The
input of the code generator is a a list of transitions generated by
the FluiDB planner. The reader is reminded that each transition has
one of three kinds:

\begin{itemize}
\item Trigger of a t-node. The input n-nodes at the time of the trigger
are materialized and the trigger itself materializes a subset of the
output n-nodes
\item Reverse trigger of a t-node, which represents the
materialization of a subset of the input n-nodes from materialized
output n-nodes
\item The deletion of an n-node.
\end{itemize}

Transitions are meant to be executed in the order they appear in the
received sequence to preserve correctness. However, there isn't necessarily a
one-to-one correspondence between operators and transitions. Instead,
as we saw in section \ref{sec:cluster_internals}, each operation
corresponds to a cluster of connected t-nodes. The first step of the
code generator, therefore, is to group the low-level transitions
received by the planner into higher level batches that correspond to
exactly one operation. This process is driven by intermediate n-nodes,
i.e. helper n-nodes that do not correspond to materializable relations,
but are rather part of the graph to reify the valid combinations that
the planner is allowed to materialize. The constraint we are trying to
preserve while batching the low level transitions such that the
cluster level transitions do not materialize any intermediate n-nodes.

Since intermediate n-nodes are always internal to clusters and no
cluster shares a t-node with another cluster, each batch of
transitions corresponds to exactly one cluster, except for deletion
transitions which are standalone. Furthermore, each cluster
corresponds to exactly one relational operator, which we also include
in the higher order transition.

\section{Generated C++}

The C++ AST is expressed as a tree of Haskell algebraic data
types. They do not capture the entire C++ language, only the following
concepts concepts:

\begin{itemize}
\item Function declaration
\item Function application and arguments
\item Expressions
\item Code symbols
\item Assignment
\item Includes
\item Literals
\item Classes
\item Member declarations
\item Global declarations
\end{itemize}

The code generated version of each operator is parameterized by the
record types of its inputs and outputs, and highly specific code is
generated by the C++ templating system.

All operator implementations live in the FCL library which
branched out from \cite{krikellasGeneratingCodeHolistic2010}. The FCL
library makes heavy use of templates and \cpp{constexpr} to generate
very query-specific machine code for each operator.

\subsection{Code structure}

The FluiDB code generator generates a couple of different kinds of C++
components to construct the final file. In this section we will
discuss the kinds of C++ structs that FluiDB generates in order to
parameterize the FCL operators at compile time, leveraging the C++
template system.

In the following subsections when we say \emph{compile time} we refer to
the compile time of the generated code, not the compile time of FluiDB.

\subsubsection{Maybe types}

Maybe types are used to indicate optional outputs of operators. For
example an operator \(\sigma_p\) may yield \(\sigma_p A\) or
\(\sigma_{\neg p} A\) or both. On the one hand, the precise outputs that
are required are important for performance and memory budget
management, and on the other any combination of these outputs can and
should be generated through one pass over the input. Furthermore,
information about which outputs are required is known at compile time
so the generated code should be specific to the combination of outputs
that are required.

To address this, instead of passing in a simple \cpp{std::string} as a
path, we pass one of two types \cpp{Nothing} or \cpp{Just} (listing
\ref{lst:maybe_type_cpp}) that wrap the file path. Both these types
contain an \cpp{isNothing} member of \cpp{static constexpr bool} type that can be used by
\cpp{if constexpr (...)} expressions in the selection operator to help
the compiler generate highly specialized machine code.

\begin{code}
\begin{cppcode}
template <typename T>
struct Just {
  Just(T t) : value(t) {}
  T value;
  constexpr bool operator==(Just<T> const& j) const {
    return this->value == j.value;
  }
  static constexpr bool isNothing = false;
};
template <typename T=std::string>
struct Nothing {
  Nothing(T s) {}
  Nothing() {}
  static constexpr bool isNothing = true;
};

int main () {
  // At compile time the select algorithm can be constructed
  // with constexprs to only emit records to "prim_out.dat".
  Select<...> op(Just<std::string>("prim_out.dat"),
                 Nothing<std::string>, // secondary out
                "input.dat");
  op.run();
}
\end{cppcode}
\caption{\label{lst:maybe_type_cpp}The type level maybe and an example.}
\end{code}

\subsubsection{Record types}

As mentioned, FluiDB is a row store engine and handles table rows by
generating record types specific to each table, where objects of that
type are rows of the corresponding table. We went over some aspects of
the the generated record types when discussing the conversion of
primary data to FluiDB specific binary data. Record types directly or
indirectly parameterize all operators. A selection operator
\(\sigma_p\) instance must be parameterized by the record type of its
input which will be the same as the output. A projection \(\pi\) is
parameterized by the records of both the input and the output type. A
generated record type must have some structural properties to function
properly with FCL and the rest of the generated types:

\begin{itemize}
\item First and foremost, it must be standard-layout type so it can be
trivially copied from and to binary files. This means that there
should be no virtual members, the destructor must be trivial and no
static members.
\item They should be comparable by equality (\cpp{==}), inequality (\cpp{!=}) and
they should be ordered (\cpp{<}). The actual semantics of the ordering
are not important as long as there is a deterministic way of
ordering records of the same type.
\item Finally, every record type must implement an \cpp{std::string
show()} function that serializes the contents into a human readable
string.
\end{itemize}

\subsubsection{Predicate types}

Predicate functions have the type of n-ary Boolean functions \(A_1
\times ... \times A_k \to \{T,F\}\) useful to generate code for
selections and \(\theta\)-joins so in practice they are unary or
binary. They are passed as template arguments to the operators, so the
compiler has the chance to inline them to avoid the indirection. To
avoid too much code clutter in the generated code they are expected to
provide the types of the function domain \(A_1, ..., A_k\)
(demonstrated in listing \ref{lst:pred_type_cpp}) in order to minimize
the number of template parameters and keep the generated code
relatively human readable.

\begin{code}
  \begin{cppcode}
  class Predicate3421 {
    typedef Record123 Domain0;
    typedef Record32 Domain1;
    bool operator() (const Domain0& rl, const Domain1& rr) {
      return rl.__field1 < rr.__field2;
    }
  }
  \end{cppcode}
  \caption{\label{lst:pred_type_cpp}Example of generated predicate function corresponding to the predicate \(\mathit{field}_1 < \mathit{field}_2.\)}
\end{code}

\subsubsection{Record transformation types}

Record transformations, much like predicates are callables
representing pure functions, only the codomain is a record type,
instead of a Boolean. They are used by joins to combine the matching
records, by projections and aggregations to produce new records from
the inputs, by equi-joins to extract the subtuples to be checked for
equality, etc. Much like predicate functions, they can be queried for
their domain and codomain to reduce the number of template arguments
in operators (see listing \ref{ref:transform_type})

\begin{code}
\begin{cppcode}
class Transform {
  typedef Record123 Domain0;
  typedef Record32 Domain1;
  typedef Record10 Codomain;
  Codomain operator() (const Domain0& l, const Domain1& r) {
    return Record10(l.__key,l.__field1, r.__key, r.__field2);
  }
};
\end{cppcode}
  \caption{\label{ref:transform_type}A record transformation type
    defines objects with no internal state that are callable.}
\end{code}


\subsubsection{Operators}

Operators are not generated, but they are parameterized by all the
kinds of gernerated code we mentioned. They are classes that are
constructed using maybe-filenames, record transformers, record types,
and predicates. Operators must implement the \cpp{run()} method which actually
runs the internal code. For demonstration an abbreviated
version of the \(\sigma_p\) operator, which highlights the shape of an
operator class is presented in listing \ref{lst:sel_operator_cpp}.

\begin{code}
\begin{cppcode}
template <typename Predicate,
          typename PrimaryOutType,   // Maybe(std::string)
          typename SecondaryOutType  // Maybe(std::string)
          >
class Select {
  typedef typename Predicate::Domain0 Record;

public:
  Select(PrimaryOutType prim, SecondaryOutType sec, std::string in)
    : primary_file(prim), secondary_file(sec), infile(in) {
    static_assert(!PrimaryOutType::isNothing || !SecondaryOutType::isNothing,
                  "Both primary and secondary output files are Nothing.");
  }

  ~Select() {}

  void run() {
    // ...
  }

  void print_output(size_t n) {
    // Prints the first n records from each output table.
  }

private:
  PrimaryOutType primary_file;
  SecondaryOutType secondary_file;
  std::string infile;
  static Predicate predicate;
};
\end{cppcode}
  \caption{\label{lst:sel_operator_cpp}The selection operator. It is
    parameterized by the predicate and the primary and secondary
    output types. Enough information about these values is known at
    compile time such that the compiler can generate highly
    specialized code.}
\end{code}

Unfortunately, even recent versions of the C++ standard do not
include template type inference for classes and structs, therefore, to
make the generated code simpler we wrap the constructor into a
function like shown in listing \ref{lst:sel_function_cpp}. This allows the
generated code for selection to look like the one presented in listing
\ref{lst:gen_out_code}.

\begin{code}
\begin{cppcode}
// C++20 can only infer typenames (primaryout secondaryout) in
// function templates.
template<typename Predicate,
         typename PrimaryOutType,   // Maybe(std::string)
         typename SecondaryOutType  // Maybe(std::string)
         >
auto mkSelect (const PrimaryOutType prim,
               const SecondaryOutType sec,
               const std::string& in) {
  return Select<Predicate,
                PrimaryOutType,   // Maybe(std::string)
                SecondaryOutType> // Maybe(std::string)
    (prim, sec, in);
}
\end{cppcode}
\caption{\label{lst:sel_function_cpp}The C++ declaration of the select.}
\end{code}


\begin{code}
\begin{cppcode}
int main() {
  // ...
  {
    auto op = mkSelect<Predicate32>(Just("data123.dat"), Just("data53.dat"),
                                    "lineitem.dat");
    op.run();
    op.print_output();
  }
  // ...
  return 0;
}
\end{cppcode}
  \caption{\label{lst:gen_out_code}A block representing a particular
    operator.}
\end{code}


\subsection{Operator implementations}

In the following subsections we will take a tour to the implementation
of some relational operators in FCL. We do not mention every operator
supported for brevity as many are trivial.

\subsubsection{Select algorithm}

The selection algorithm is likely the simplest of the implemented ones
because FluiDB does not support indexes and makes no assumptions about
the ordering of the data. In the forward variety, it is implemented as
either a partition or a selection depending on which of the outputs it
is materializing. In its backward variety it is essentially a union
\(A \equiv \sigma_p A \cup \sigma_{\neg p} A\). As we discussed
already (listing \ref{lst:sel_function_cpp}) the FCL library expects
only a predicate class as a template parameter and three filenames as
its arguments.

It is important to remind the reader that no order is expected to be
preserved by union, which is to say that the transition \(A
\Rightarrow \{\sigma_p A, \sigma_{\neg p} A\} \Rightarrow A\) does not
preserve the order of the rows. The reader is also reminded that
FluiDB does not support \hask{NULL} values to preserve the correctness of
the reverse select operation.


\subsubsection{Projection algorithm}
\label{sec:projection_algorithm}

The implementation of the projection algorithm is parameterized by two
template arguments that extract complementary subtuples from an input
relation, affording each one with a common unique subtuple to
facilitate the inverse operation. The inverse of a projection then is
simply an equijoin between the two produced slices based in that
shared unique subtuple.

While the FCL side of the calculation is fairly straightforward, on
the Haskell side (code generation) it is slightly more complex. The
first piece of the puzzle is the representation of the
projection. When the query is initially processed so that projections
are augmented to expose a unique subtuple we actually change the
representation of projections from \hask{QProj [(e,Expr e)]} to
\hask{QProjI [e] [e] [(e,Expr e)]}. The two extra parameters typed
\hask{[e]} represent the complement of the projection (the columns
from the input not exported by the projection) and a unique subtuple
that the complement and the projection must have in common, while
\hask{[(e,Expr e)]} is the normal projection. We make this
transformation because the easiest and cheapest place to calculate the
first and second parameters (complement and correspondence) is during
the projection augmentation phase that happens immediately after
parsing. Therefore, we have all projections carrying information about the
complement at the level of the RA operator.

In particular, this process is combined with remapping of unique
subtuples into the projection. For example, during the unique column
exposure process the query \sql{select p_color from part} is found not
to be valid because \sql{p_color} is not unique for each row. For that
reason, during preprocessing we remap the \sql{p_partkey} column since
\sql{p_partkey} is the only unique column of \sql{part} this way the
constraint that every valid relation in FluiDB needs to have at least
one subtuple that is unique within that relation is satisfied and the
query becomes

\begin{sqlcode}
select p_partkey,p_color from part
\end{sqlcode}

To be able to reconstruct the input we require that at least
one unique subtuple is shared between the primary projection relation
and the complement. In the example demonstrated, this means the
\sql{p_partkey} column is duplicated in both output tables of the
operator. The particular valid projection operator that would be
generated the \hask{QProj} is presented in listing
\ref{lst:real_proj_operator}.

\begin{code}
\begin{haskellcode}
op =
  QProjI
    -- All keys except p_color
    [p_partkey,p_name,p_mfgr,p_category,p_brand1,p_type,p_size,p_container]
    -- The unique subtuple (just one column)
    [p_partkey]
    -- The the actual projection
    [(p_partkey,E0 p_partkey),(p_color,E0 p_color)]
\end{haskellcode}
  \caption{\label{lst:real_proj_operator}The projection operator
    produced from the SQL query \sql{select p_partkey,p_color from
      part}.}
\end{code}

The algorithm for reverse projection is almost a standard join. The
only difference is that in our semantics of join, the relation \(A
\Join_{a = b} B\) contains both the column \(a\) and the column \(b\),
while when joining a projection with it's complement we only create a
single copy of the shared subtuple. We will see more details about
join in the next section.

\subsubsection{The join operator}

We distinguish between two kinds of join operators: the equijoin and
the general \(\theta\)-join. The former is specific to cases where
subtuples from each table are compared and the implementation is
simply a merge join, while the latter is used for general predicates
and is essentially a pipelined \(\sigma(A \times B)\). The important
detail worth noting about join algorithm implementations in FluiDB is
that they must be able to produce antijoin relations \(A \lnsemi B\),
\(A \rnsemi B\) simultaneously with the join relation \(A \Join B\).

It is important for the join algorithm to be space efficient, and particularly
constant-space, due to the assumption made by the planner that it has full
control over memory. For general \(\theta\)-joins we use nested loop
joins. For equijoins we take advantage of the assumption that the
tables are unordered and use merge join after sorting the input tables
in-place. A sketch of the algorithm is presented in listing
\ref{lst:join_algorithm}

\begin{code}
\begin{pycode}
# This is our pythonic way of detecting which input stream finished.
class LeftFinished:
    pass
class RightFinished:
    pass


def lnext(it):
    """If iteration finishes throw LeftFinished"""
    try:
        return next(x)
    except:
        raise LeftFinished

def rnext(it):
    """If iteration finishes throw RightFinished"""
    try:
        return next(x)
    except:
        raise RightFinished

@template(extractl,extractr,combine)
def merge_equijoin(in_l,in_r,anti_l,out,anti_r):
    # In place sort each of the inputs
    sort(in_l,key=extractl)
    sort(out_l,key=extractr)
    # Define iterators and current values for each of the inputs.
    it_l = iter(in_l)
    val_l = next(it_l)
    it_r = iter(in_r)
    val_r = next(it_r)

    try:
        # The actual merge join algorithm
        while True:
            # Gather blocks of equal records from left and right and write
            # out their product.
            if extractl(val_l) == extractr(val_r):
                # Gather a sequence of equal-key values from the left
                val_ls = []
                tmp = val_l

                while extractl(val_l) == extractl(tmp):
                    val_ls.append(val_l)
                    val_l = lnext(it_l,None)

                # Gather a sequence of equal-key values from the right
                tmp = val_r
                val_rs = []
                while extractr(val_r) == extractr(tmp):
                    val_rs.append(val_r)
                    val_r = rnext(it_r)

                # Write out their product.
                for l,r in product(val_ls,val_rs):
                    out.write(combine(l,r))

            # Push the non-equal records to the antijoins
            while extractl(val_l) < extractr(val_r):
                anti_l.write(val_r)
                val_r = rnext(it_r)
            while extractl(val_r) < extractr(val_l):
                anti_r.write(val_l)
                val_l = lnext(it_l)
    except RightFinished:
        # if we ran out of records from the right operand
        # Push the remaining if the left operand to left antijoin
        for l in it_l:
            left_anti.write(r)
    except LeftFinished:
        # if we ran out of records from the left operand
        # Push the remaining if the right operand to right antijoin
        for r in it_r:
            right_anti.write(r)
\end{pycode}
\caption{\label{lst:join_algorithm}The equi-join algorithm first sorts
  in place the inputs w.r.t. equal subtuples and then merges
  them.}
\end{code}


The type of the join operator is defined in terms of a template such
that it can be specialized at compile time (see listing
\ref{lst:join_decl}). The template arguments are the following:

\begin{itemize}
\item Three file paths are provided, \cpp{OutFile}, \cpp{LeftAnti} and
  \cpp{RightAnti} are each either of type \cpp{Nothing} indicating to
  the compiler not to generate any code that relates to the particular
  output or of type \cpp{Just<std::string>} indicating to the compiler
  the opposite.
\item The subtuple extraction functions \cpp{LeftExtract} and
  \cpp{RightExtract}. These are record transformation functions that
  extract the subtuple that needs to
\end{itemize}

\begin{code}
\begin{cppcode}
template <typename LeftExtract, typename RightExtract, typename Combine,
          typename OutFile,    // Maybe(std::string)
          typename LeftAnti,   // Maybe(std::string)
          typename RightAnti>  // Maybe(std::string)
class Join;
\end{cppcode}
\caption{\label{lst:join_decl}Class declaration of the join operator}
\end{code}

The reverse join operator is composed of two pipelined steps:
\begin{itemize}
\item A projection and deduplication based on the unique subtuple of
the input on the join output to get a left or right semijoin.
\item A union of the semijoin with the antijoin to get the input table.
\end{itemize}

As with the selection operator, the correctness of this reverse
operation is predicated on the fact that FluiDB has no notion of
\sql{NULL} values.

\subsubsection{Aggregation and sort algorithms}

The main challenge to implementing aggregations in the context of
FluiDB is maintaining constant space during its evaluation, as the
planner tries to use as much of the memory budget as possible to
maintain intermediate results. Therefore, we opted against using an
auxiliary hash map to group records. Like we did to implement
joins we perform aggregation in two steps: first we sort the records
in place based on grouping columns and then aggregate in a single
pass.

\section{Conclusion}

In this chapter we discussed some physical attributes of how FluiDB
evaluates the final plans it constructs via code generation. We
discussed the storage layout and the fundamental ideas the code
generator was built around to generate highly specialized
code. Finally, we discussed some specific algorithms used by the FCL
library that is called by the generated code to implement the actual
operators.

Among this we discussed some caveats that relate to the FluiDB's
approach to code generation, namely, that highly specific code
generated can be expensive to compile from scratch and that the shmem
in-memory filesystem provided by Linux is a cheap solution in terms of
engineering effort but would be better replaced with a solution that
does not involve system calls.

\dictum[Κ. Βαρναλης]{I am not the sow of chance, the forger of new life}

\begin{summary}
\item Relational databases are question answering systems that deal
  with information organized in tables or relations.
\item Query optimization and planning revolves around finding an
  efficient alorithm for answering a query.
\item In-mamory databases keep all the data in main memory so they are
  closer to the processer
\item many in-memory databases employ some variant of code generation
  to execute the query plans.
\item Intermediate result recycling is the practice of reusing
  computation between queries.
\end{summary}

FluiDB is a system that generally focuses on relational query,
optimization and planning. This chapter aims to give the reader some
idea about where FluiDB fits in the design space and the historical
context in which it was developed.  First, we outline a very high
level overview of the \emph{query language and operators} involved in
database management, as well as the the overall architecture of such
systems. Following that we will focus on the \emph{query planning}
subsystems of relational query databases and some traditional
approaches to query evaluation. Afterwards, we will focus particularly
on \emph{in-memory relational databases} and the trade-offs that
govern their design. Finally, we look into systems that utilize
\emph{intermediate result recycling} and how they solve the problem of
automatically selecting and maintaining intermediate results for
workloads.

\section{Relational databases}

Databases are more than just a method of accessing data. In their
essence they are machines for question answering. The typical database
works in a perpetual loop of reading queries and coming up with
answers based on a set of data points. There are two important aspects
that every database needs to define in order to delineate its
operational semantics:

\begin{itemize}
\item The language in which the queries are expressed in
\item The representation of data in terms of which the queries are
  expressed and the results are presented.
\end{itemize}

The oldest, most studied, and most common model is the relational model
which defines queries in terms of \emph{relational algebra} and organizes
data in \emph{tables} or relations.

A relation is typically an unordered set of tuples
\(\{d_1,d_2,...,d_k\}\) where each element \(d_i\) represents an
attribute. The core relational algebra is an extension of the algebra
of sets (that defines operators of set union \(\cup\), intersection
\(\cap\), product \(\times\), and difference \(-\)) that includes the
operators of joins \(\Join\), projection \(\pi\) and selection
\(\sigma\). Upon this foundation relational databases typically define
extra operators to increase the expressive power of the language like
aggregations \(\gamma\), semijoins \(\lsemi\), sorting, limiting, etc.

A typical relational database processes operates in a cascading
fashion. It initially receives a query in a textual form. While
relational algebra is the language that underpins the operation of a
relational database, it is rarely the language in which users interact
with it. Instead, most relational databases expect queries written in a
variant of SQL, a query language that is parsed into a tree of
relational algebra operators.

This tree is processed by the query optimizer which rewrites it into a
representation that is efficient to be executed by taking advantage
of the mathematical properties of RA and often gathering statistics
about the underlying data itself. This process leads to the formation
of a relational algebra expression called the \emph{logical plan}. It
is a high level description of a sequence of operations required to
produce a result. A logical plan only includes the denotational
semantics of these operations, making no assumptions about their
implementation.

Each RA operator is typically implemented by several different
algorithms, each being efficient and even possible in some situations
but not in others. For example, a join can be implemented by nested
loops or with merge join among others.  While the former algorithm is
general and can implement any join, it is very inefficient. On the
other hand a merge join is much more efficient but can only implement
joins of the form \(\Join_{a=b}\) (equijoins or natural
joins). Furthermore, a \emph{logical plan} typically contains only
implied information about the scheduling of the operators, for
example, the logical plan \((\pi A) \Join (\sigma B)\) implies that
the join can not begin being evaluated before the projection and
selection but the latter can be evaluated in any order, or even
simultaneously via pipelining. All these details about the execution
of the query are resolved by the \emph{physical planner}. The physical
planner accepts a logical plan and emits an unambiguous algorithm that
will produce the result of the query, called the \emph{physical plan}.


The final step is actually executing the physical plan which is
handled by the \emph{query execution engine}. The execution engine
simply evaluates the physical plan on top of the data, making use of
the correct algorithms, auxiliary structures like indexes managing
memory, handling page-level caching, etc.

% TODO: RDBMS diagram

\section{Query optimization and planning}

Query optimization and planning generally refers to the processes that
take place between reading a query and executing a physical plan. The
concerns of query optimization are the correctness of the final result and
the efficiency of the plan generated, usually in terms of time, but
also in terms of space.

Finding an optimal plan is in the general case NP-complete
\cite{ullmanInformationIntegrationUsing1997}, but query optimizers can
do a good job at finding good plans using heuristics. The selection
and organization of these heuristics as well as query cost estimation
are the main problems that make a query optimizer nontrivial. In
particular one could separate the the tasks of a query optimizer
into three broad categories:

\begin{itemize}
\item Query rewriting
\item Query plan enumeration
\item Size and cost estimation (cost model)
\end{itemize}


At a very high level, the architecture of a query optimizer is
demonstrated in figure \ref{fig:optimizer_arch} and is broadly
comprised of the following components:

\begin{figure}[p]
\centering
\includegraphics[width=\textwidth]{./imgs/optimizer_architecture.pdf}
\caption{\label{fig:optimizer_arch}Common architecture of a query
  optimizer.}
\end{figure}

\begin{itemize}

\item The \emph{parser} which receives the query in textual form and
  produces a logical plan in the form of an abstract syntax tree.
\item The \emph{deterministic optimizer} or \emph{logical planner}
  that rewrites the logical plan applying optimizations that are based
  solely on the general properties of the relational algebra.
\item The \emph{physical planner} that transforms a logical plan into
  a physical plan that can be unambiguously executed by the execution
  engine to produce a result. The physical planner typically has at
  least some information about the underlying data that the plan will
  operate on like estimations about the statistics of the values or
  the cardinalities of the relations.
\item The information about the data being manipulated by the plan is
  inferred by the \emph{cost estimator}. It uses a cost model to
  predict the cost of plans and the cardinality of relations by taking
  into account the provenance of relations as well as physical
  properties of the data like the presence of indexes, the data layout,
  etc.
\end{itemize}

These subsystems are presented here as separate for simplicity and
because in many database systems they are clearly delineated, but it is
also common that they blur into each other. For example, in some RDBMSes
the physical and logical planner are merged into one
\cite{graefeCascadesFrameworkQuery1995,shankarQueryOptimizationMicrosoft2012,solimanOrcaModularQuery2014}. In
fact, it is increasingly common for systems to intersperse query
planning with query execution, adapting the optimization strategy
\cite{graefeDynamicQueryEvaluation1989} to concrete information about
the intermediate results evaluated rather than purely relying on
estimations and predictions
\cite{dingPlanStitchHarnessing2018,chaudhuriPayasyougoFrameworkQuery2008,wuSamplingbasedQueryReoptimization2016,herodotouXplusSqltuningawareQuery2010}. The
degree to which these subsystems are separate is a major concern in
the design space of query optimizers.

Another important aspect to be considered, and which is indeed
important in the design of FluiDB, is the number of queries considered
at a time during optimization and the way in which they are
considered. The optimizer usually considers one query at a time and
maintains little or no state between executions. Although it has found
little adoption in main stream databases, multiquery optimization has
been researched expensively
\cite{michiardiCachebasedMultiqueryOptimization2021,wangMultiqueryOptimizationMapreduce2013,royEfficientExtensibleAlgorithms2000,rogersMultiqueryOptimization2017}. What
is more common in recent years is recycling intermediate
results. RDBMSes that incorporate this technique materialize and cache
intermediate relations, reusing them when they appear as sub-queries
in later queries
\cite{perezHistoryawareQueryOptimization2014,nagelRecyclingPipelinedQuery2013,ivanovaArchitectureRecyclingIntermediates2010}.

The final query processing concern that is relevant to the design of
FluiDB regards the traversal and pruning of the search space. As
mentioned, query optimization is generally NP-complete, so the viable
options DBMS designers are left with are randomized algorithms, ML
approaches, and heuristics. Virtually all systems implement heuristics
entirely or to some degree, while recently more and more also
incorporate randomized algorithms
\cite{chandeGeneticOptimizationJoin2011} and machine learning
\cite{liMachineLearningDatabases2021,marcusNeoLearnedQuery2019}.

\subsection{Logical and physical query optimization}

The \emph{logical planner} transforms a syntax tree in the form of
relational algebraic expressions, where the operators only contain
information about the semantic meaning of operations and none relating
to the algorithms that will eventually be executed on the input
data. It is for all intents and purposes a rewrite engine for
relational algebra. Typical transformations performed by the logical
query optimizer are

\begin{itemize}
\item Predicate normalization to conjunctive normal form, e.g. \((a_0 = 1
  \land a_1 = b_1) \lor b_2 = 3 \hookrightarrow (a_0 = 1 \lor b_2 = 3)
  \land (a_1 = b_1 \lor b_2 = 3)\)
\item Predicate push-down e.g. \(A \Join_{a_0 = 1 \land p} B
  \hookrightarrow (\sigma_{a_0 = 1} A) \Join_p B\).
\item Cartesian product to joins \(\sigma_p ( A \times B )
  \hookrightarrow A \Join_p B\)
\item Searching the join ordering space.
\end{itemize}

The \emph{physical query planner}, on the other hand, will specialize
the logical operators deciding on the particular algorithm that should
be used. A physical query planner therefore requires low level
information about the query that relates to the indexes available,
possible ordering of the data, materialized views, etc.

Either of the planners needs to enumerate the plans under
consideration while traversing the search space. There are two major
approaches to this:

\begin{itemize}
\item The \emph{top-down} approach, where the planner establishes the
  top level operator and branches searching the children, backtracking
  as necessary. This was the approach of the descendants of the
  \emph{volcano optimizer} \cite{graefeVolcanoOptimizerGenerator1993a}
  where they implement a "search engine" that uses a branch and bound
  approach to optimization with caching.
\item The \emph{bottom-up} approach where the planner builds and
  connects fragments of a plan accumulating those fragments into a
  larger plan in a traditional dynamic programming approach.
  \cite{raasveldtDuckdbEmbeddableAnalytical2019,kemperHyPerHybridOLTP2011}
\end{itemize}

An important optimizer, on which most modern optimizers are still
being based, is the \emph{cascades optimizer}
\cite{graefeCascadesFrameworkQuery1995}. In short, cascades keeps
track of \emph{groups} of equivalent query expressions and uses those
as the fundamental atom that it manipulates. For example instead of
keeping track of \(A \Join (B \Join C)\) and \((A \Join B) \Join C\)
separately they would be part of the same query group. It then uses a
global hash table (the "memo" structure) to match the best plan that
corresponds to each group.

Many database systems use optimizers similar to cascades (like the
Microsoft SQL server, Postgres,
MemSQL\cite{chenMemSQLQueryOptimizer2016}, and Greenplum -- now Orca
\cite{solimanOrcaModularQuery2014a} to name a few). Notable among the
descendants of cascades is Apache Calcite
\cite{begoliApacheCalciteFoundational2018}, a framework for
implementing query planning used by a number of commercial and
research databases \cite{nunesalonsoBuildingPolyglotData2020}.

\subsection{Cost estimation}

Probably the hardest aspect of the planner design is the cost
estimation algorithm, which is required by the optimizers, especially
the physical planner. These are required in order to select plans and
to navigate the search space. A good cost model can help basic
planners make decent plans and a bad one can cause sophisticated
planners make horrible plans \cite{leisHowGoodAre2015}. As cost
estimation, we generally refer to a number of different related
procedures that are broadly the \emph{estimation of the cost} of an
arbitrary plan and include the prediction of the \emph{cardinality} of
not yet materialized relations.

It seems that the most important challenges involved in the design of
a cost model relate to the fact that we are fundamentally operating
with scarce and highly uncertain information. Especially relating to
cardinality estimation, uncertainty and bad predictions propagate and
make make it extremely hard for the planners to make correct
decisions. Consider, for example a join. Some joins are similar to
Cartesian products, producing large output tables, and some joins are
more similar to lookups producing only a few rows. A cost model that
confuses the kind of join will make very bad predictions w.r.t.\ the
cost of any relational algebra expression that uses the said join,
causeing an optimizer to select an expensive plan or avoid a cheap
one.

Cardinality estimation most commonly takes the form of selectivity
estimation, i.e.\ what percentage of tuples from the input make it to
the output of a selection or a join. This is sensitive to the
statistical properties of the values and the selection/join
predicates.

The most common approach is to maintain pre-computed statistics
related to the primary tables. This yields better plans than keeping
no statistics at all, but they are very hard to maintain and their
effectiveness becomes very limited for complex queries. Another
approach employed is to delay the decisions of the optimizer and
essentially merge the planning and plan execution process. This does
not completely eliminate the problem, but it allows the system to have
more up to date and precise data that would allow it to correct course
early in the event of exceptionally bad estimations. One family of
techniques that is becoming popular and was initially used in IBM DB2
\cite{stillgerLEODB2LearningOptimizer2001}, is caching the statistics
of previously computed relations and using that data to make better
predictions in the future. More recently, this takes the form of
employing machine learning to learn from past cardinalities
\cite{ortizEmpiricalAnalysisDeep2019}. This approach is still rarely
used due to the engineering cost of keeping the collected information
up to date.

To give the reader a more well-rounded intuition of the state of cost
estimation besides cardinality estimation, it is worth mentioning that,
to estimate the cost of an operation, Postgres uses magic configurable
variables to weigh IO with CPU evaluation time, while DB2 runs
micro-benchmarks on the production system to make these estimations.

Cost estimation is beyond the current scope of FluiDB and we only
implement the most naive cardinality estimation, but it is worth
mentioning how more mature systems approach the issue.

\section{In-memory relational databases}

In-memory databases are databases where all data lives in main
memory. The design of in-memory databases is different from the design
of a disk-backed database in a number of respects. To name a few:

\begin{itemize}
\item Page buffers have little use and caching of data in general has very
different goals. While in disk-backed databases caches are mainly used to
avoid disk IO, in in-memory databases they focus on reuse of computation.
\item Concurrency control is much simpler as storage synchronization
concerns are almost entirely eliminated.
\item In disk-backed databases, only a small percentage of time is spend on
actual computation \cite{harizopoulosOLTPLookingGlass2018}. Much of
the non-compute latency is directly linked to the persistent
storage.
\end{itemize}

On the other hand, there are concerns that are specific to the
\emph{lack of backing database storage}. To name a few:

\begin{itemize}
\item Should the system rely on record IDs like in a persistent database
or can it use direct pointers to records?
\item Error prone software can bring the data to an irrecoverable state.
\item Query execution algorithms have fundamentally different
characteristics. While in persistent databases IO dominates the
runtime the bottlenecks for an in-memory database are much more
complex and they can include things like locking, cache misses,
predicate evaluation, and data movements.
\item As main memory is a much less abundant resource than persistent
storage, in-memory databases are often distributed, making network a
major concern.
\end{itemize}

One increasingly common technique to address many of these issues,
which is also used by FluiDB, is \emph{code generation}. Since
workloads for in-memory databases are typically CPU bound, there are
major gains in performance to be had by specializing the code being
executed. A typical interpereter based plan execution engine makes
heavy use of virtual function calls and conditionals inside tight
loops, which kills performance on virtually all modern
architectures. The value proposition of code generation is to inline
or hard-code the virtual functions and erase the conditionals at
runtime to reduce the number of operations and make better use of
hardware optimization.

We identify 4 different approaches in the literature to solving this
problem:

\subsection{Transpilation}
\label{sec:transpilation}

Transpilation of a physical plan to a systems language like C or C++
and then fed to an off-the-shelf compiler
\cite{krikellasGeneratingCodeHolistic2010} is expensive but it
generates highly efficient code more easily debuggable execution
plans. The most notable complete database systems that used this
technique were Microsoft Hekaton that generates C code from SQL
queries and older versions of MemSQL. For FluiDB, we reuse a lot of
techniques introduced in HIQUE
\cite{krikellasGeneratingCodeHolistic2010} to translate physical plans
to template-heavy C++ code, making the assumption that query
compilation will be much faster than the query runtime.

\subsection{Third party JIT compilers}

JIT compilation has received a lot of attention in the compiler
community, especially in the context of the JVM. A number of database
systems have taken advantage of this to speed up query execution. Most
notable of these are SPARK \cite{armbrustSparkSQLRelational2015}, which
generates Scala AST which is then converted to JVM byte code, and
Neo4j that directly generates bytecode out of the queries.

Systems like Peloton \cite{menonRelaxedOperatorFusion2017} and
recently Postgres \cite{sharyginQueryCompilationPostgreSQL2017}
compile query plans to LLVM IR code that is then passed to the LLVM
compiler to generate high performance machine code. Notable among the
systems that use this approach is HyPer
\cite{neumannEvolutionCompilingQueryEngine} which addresses the
problem of query compilation overhead with an adaptive execution
approach: they built an IR interpreter that starts running the query
while the compiler does proper optimization and compilation of the
LLVM IR program being interpreted. Cheap queries are thus completed
reasonably fast, while in the case of complex queries the interpreted
program is seamlessly replaced by the compiled one once the LLVM
compiler finishes generating machine code.

\subsection{Direct machine code generation}

Some databases do not reuse any compiler or JITing VM, but rather
directly generate highly specific machine code out of the physical
plan. The first system to attempt that, like most techniques used
today, was System-R, which originally compiled SQL statements directly
to machine code by stitching together code fragments from a "fragment
library" \cite{chamberlinHistoryEvaluationSystem1981} but it was
quickly deprecated due to the large engineering effort
required. Oracle also includes similar to some of their databases and
MemSQL express their plans in a custom language called MPL (MemSQL
Plan Language) for which they have a custom compiler that translates
it directly to machine code.

\subsection{Custom execution engines}

The final category is per database code generation rather than per-query
code generation. Volcano/EXODUS
\cite{graefeVolcanoOptimizerGenerator1993a} and more recently SageDB
\cite{kraskaSageDBLearnedDatabase} generate an optimizer and execution
engine that is specific to the database schema but not to the queries.

\section{Intermediate result recycling}

A materialized view is a relation defined by a query that is
persistently stored. A view that is not stored is said to be
virtual. \emph{View selection} is the process of selecting an
appropriate set of materialized views to improve the performance of a
workload \cite{mamiSurveyViewSelection2012}.  Automated materialized
view selection or intermediate view recycling has been on the radar of
the database research community for a while now. A few approaches to
this problem have to do with AND/OR directed acyclic graphs
\cite{guptaSelectionViewsMaterialize1997}, modeling the problem as a
state optimization \cite{theodoratosDataWarehouseConfiguration1997},
and lattices to represent data cube operations (i.e. multiple
aggregations over the same relation) \cite{ImplementingDataCubes}.

A related problem is \emph{multiquery optimization (MQO)}
\cite{theodoratosDataWarehouseConfiguration1997} that attempts to plan
multiple queries simultaneously. An efficient solution using AND/OR
DAGs was proposed by Roy in
\cite{royEfficientExtensibleAlgorithms2000} where they insert queries
and their sub-queries in a graph and attempt to evaluate a plan by
traversing that graph from multiple sources in a fashion similar to
volcano optimization. Building on that Kathuria
et. al. \cite{kathuriaEfficientProvableMultiquery2017} present an
approximation algorithm that runs in time quadratic to the number of
common subexpressions and provides theoretical guarantees on the
quality of the solution obtained.

Researchers have further looked at opportunistically reusing
intermediate results that would be materialized. Both in traditional
databases
\cite{ivanovaArchitectureRecyclingIntermediates2010,nagelRecyclingPipelinedQuery2013}
and on non-database xontexts like MapReduce
\cite{elghandourReStoreReusingResults2012a}, and there have been
attempts to unify the planner with the materialized view selection
engine \cite{perezHistoryawareQueryOptimization2014a}. Notable in this
field is Nectar \cite{gundaNectarAutomaticManagement2010} which is
also not an RDBMS which automatically compresses rarely used data into
programs that generate that data. Nectar focuses on sharing
computation and data as much as possible.

Work such as MRShare \cite{nykielMRShareSharingMultiple2010} tries to
bridge the gap between intermediate result recycling and MQO by
automatically grouping queries in a workload in such a way that
computation can be maximally shared.
